{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T18:54:26.419854Z",
     "start_time": "2025-08-13T18:54:26.416361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "import os\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "# RelBench\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "\n",
    "# TabPFN\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "\n",
    "# Device selection with MPS preference\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "id": "b04c45a19b5430c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T18:54:26.437912Z",
     "start_time": "2025-08-13T18:54:26.431458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = time.perf_counter()\n",
    "    yield lambda: time.perf_counter() - start\n",
    "\n",
    "\n",
    "def classification_metrics(y_true, y_pred, y_prob=None) -> Dict[str, float]:\n",
    "    out = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            out[\"roc_auc\"] = roc_auc_score(y_true, y_prob)\n",
    "        except Exception:\n",
    "            out[\"roc_auc\"] = np.nan\n",
    "    else:\n",
    "        out[\"roc_auc\"] = np.nan\n",
    "    return out\n",
    "\n",
    "\n",
    "def regression_metrics(y_true, y_pred) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        \"mse\": mean_squared_error(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "def to_pandas(table) -> pd.DataFrame:\n",
    "    if hasattr(table, \"to_pandas\"):\n",
    "        return table.to_pandas()\n",
    "    if hasattr(table, \"df\"):\n",
    "        return table.df if isinstance(table.df, pd.DataFrame) else pd.DataFrame(table.df)\n",
    "    return pd.DataFrame(table)\n",
    "\n",
    "\n",
    "def coerce_datetime(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    if col in df.columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def first_existing(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def merge_asof_by_group(left_idx: pd.DataFrame,\n",
    "                        right_feat: pd.DataFrame,\n",
    "                        key_col: str,\n",
    "                        date_col: str = \"date\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A leak-safe asof join: for each group by key_col, join the last known\n",
    "    feature snapshot at or before 'date' (shift should be applied in the feature builder).\n",
    "    \"\"\"\n",
    "    if right_feat.empty:\n",
    "        return pd.DataFrame(index=left_idx.index)\n",
    "\n",
    "    parts = []\n",
    "    for k, g_left in left_idx.groupby(key_col, sort=False):\n",
    "        g_right = right_feat[right_feat[key_col] == k]\n",
    "        if g_right.empty:\n",
    "            parts.append(pd.DataFrame(index=g_left.index))\n",
    "            continue\n",
    "        merged = pd.merge_asof(\n",
    "            g_left.sort_values(date_col),\n",
    "            g_right.sort_values(date_col),\n",
    "            on=date_col, direction=\"backward\"\n",
    "        )\n",
    "        merged.index = g_left.sort_values(date_col).index  # restore original row order\n",
    "        # drop keys/date from the right side (they duplicate)\n",
    "        drop_cols = [c for c in [key_col, date_col] if c in merged.columns]\n",
    "        merged = merged.drop(columns=[c for c in drop_cols if c in merged.columns], errors=\"ignore\")\n",
    "        parts.append(merged.sort_index())\n",
    "\n",
    "    return pd.concat(parts, axis=0).sort_index()\n"
   ],
   "id": "9e0b598fd9c1d009",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T18:54:26.448861Z",
     "start_time": "2025-08-13T18:54:26.445020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "dataset = get_dataset(\"rel-avito\")\n",
    "db = dataset.get_db()\n",
    "\n",
    "def load_all_tables(db) -> Dict[str, pd.DataFrame]:\n",
    "    names = db.table_dict\n",
    "    out = {}\n",
    "    for n in names:\n",
    "        out[n] = to_pandas(db.table_dict[n])\n",
    "    return out\n",
    "\n",
    "tables = load_all_tables(db)\n",
    "list(tables.keys())"
   ],
   "id": "5302bc3993511b05",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AdsInfo',\n",
       " 'Category',\n",
       " 'Location',\n",
       " 'PhoneRequestsStream',\n",
       " 'SearchInfo',\n",
       " 'SearchStream',\n",
       " 'UserInfo',\n",
       " 'VisitStream']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T18:54:26.714794Z",
     "start_time": "2025-08-13T18:54:26.465521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Normalize date columns for relevant tables (Avito schema)\n",
    "# We will standardize to 'date' column names inside our engineered frames.\n",
    "date_mappings = {\n",
    "    \"SearchInfo\": [\"SearchDate\"],\n",
    "    \"SearchStream\": [\"SearchDate\"],\n",
    "    \"VisitStream\": [\"ViewDate\"],\n",
    "    \"PhoneRequestsStream\": [\"PhoneRequestDate\"]\n",
    "}\n",
    "\n",
    "for tname, df in tables.items():\n",
    "    # coerce known date columns if exist\n",
    "    if tname in date_mappings:\n",
    "        for c in date_mappings[tname]:\n",
    "            if c in df.columns:\n",
    "                df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    # also coerce any column containing 'date' substring (robustness)\n",
    "    for c in df.columns:\n",
    "        if \"date\" in c.lower():\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n"
   ],
   "id": "2779de11f7096822",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T18:54:26.726339Z",
     "start_time": "2025-08-13T18:54:26.723960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "TASK_SPECS = {\n",
    "    \"user-visits\":    {\"kind\": \"clf\",  \"join\": \"user\"},\n",
    "    \"user-clicks\":    {\"kind\": \"clf\",  \"join\": \"user\"},\n",
    "    \"ad-ctr\":         {\"kind\": \"reg\",  \"join\": \"ad\"},\n",
    "    \"user-ad-visit\":  {\"kind\": \"link\", \"join\": \"user-ad\"}  # probabilities â†’ MAP@k via task.evaluate\n",
    "}\n",
    "\n",
    "TASKS = list(TASK_SPECS.keys())\n",
    "TASKS\n"
   ],
   "id": "1d50da024f2ae835",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user-visits', 'user-clicks', 'ad-ctr', 'user-ad-visit']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T18:54:26.742250Z",
     "start_time": "2025-08-13T18:54:26.738173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def run_single_table_experiment(task_name: str):\n",
    "    spec = TASK_SPECS[task_name]\n",
    "    task = get_task(\"rel-avito\", task_name)\n",
    "\n",
    "    X_train, y_train = task.get_X_y(\"train\")\n",
    "    X_val,   y_val   = task.get_X_y(\"val\")\n",
    "    X_test,  y_test  = task.get_X_y(\"test\")\n",
    "\n",
    "    # Choose model\n",
    "    if spec[\"kind\"] == \"reg\":\n",
    "        model = TabPFNRegressor(device=DEVICE)\n",
    "        is_reg = True\n",
    "    else:\n",
    "        model = TabPFNClassifier(device=DEVICE)\n",
    "        is_reg = False\n",
    "\n",
    "    # Fit\n",
    "    with elapsed_timer() as t:\n",
    "        model.fit(X_train, y_train)\n",
    "    fit_time = t()\n",
    "\n",
    "    # Predict val/test\n",
    "    with elapsed_timer() as t:\n",
    "        y_val_pred = model.predict(X_val)\n",
    "    pred_time_val = t()\n",
    "\n",
    "    with elapsed_timer() as t:\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    pred_time_test = t()\n",
    "\n",
    "    # Probabilities for clf/link\n",
    "    prob_val = prob_test = None\n",
    "    if not is_reg:\n",
    "        try:\n",
    "            proba_val  = model.predict_proba(X_val)\n",
    "            proba_test = model.predict_proba(X_test)\n",
    "            # binary -> keep positive class prob\n",
    "            if proba_val.ndim == 2 and proba_val.shape[1] == 2:\n",
    "                prob_val = proba_val[:, 1]\n",
    "                prob_test = proba_test[:, 1]\n",
    "            else:\n",
    "                # multiclass or already 1D\n",
    "                prob_val = proba_val\n",
    "                prob_test = proba_test\n",
    "        except Exception:\n",
    "            prob_val = prob_test = None\n",
    "\n",
    "    # Secondary metrics\n",
    "    if is_reg:\n",
    "        val_sec  = {**regression_metrics(y_val, y_val_pred)}\n",
    "        test_sec = {**regression_metrics(y_test, y_test_pred)}\n",
    "    else:\n",
    "        val_sec  = {**classification_metrics(y_val, y_val_pred, prob_val)}\n",
    "        test_sec = {**classification_metrics(y_test, y_test_pred, prob_test)}\n",
    "\n",
    "    # Primary metric via RelBench\n",
    "    if spec[\"kind\"] in [\"clf\", \"link\"]:\n",
    "        # RelBench expects probabilities/scores\n",
    "        val_primary  = task.evaluate(prob_val if prob_val is not None else y_val_pred, \"val\")\n",
    "        test_primary = task.evaluate(prob_test if prob_test is not None else y_test_pred)\n",
    "    else:\n",
    "        # regression expects numeric predictions\n",
    "        val_primary  = task.evaluate(y_val_pred, \"val\")\n",
    "        test_primary = task.evaluate(y_test_pred)\n",
    "\n",
    "    res = {\n",
    "        \"val\":  {**val_sec,  \"fit_time\": fit_time, \"predict_time\": pred_time_val,  \"primary_metric_relbench\": val_primary},\n",
    "        \"test\": {**test_sec, \"fit_time\": fit_time, \"predict_time\": pred_time_test, \"primary_metric_relbench\": test_primary},\n",
    "    }\n",
    "    return res\n"
   ],
   "id": "adce041b5fa0f4e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T18:54:26.761851Z",
     "start_time": "2025-08-13T18:54:26.752134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def build_user_timeseries(tables: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns per-user historical features keyed by ['UserID','date'].\n",
    "    Features are cumulative/expanding and SHIFTED by 1 to avoid look-ahead leakage.\n",
    "    \"\"\"\n",
    "    # --- Source 1: VisitStream (views)\n",
    "    vs = tables.get(\"VisitStream\", pd.DataFrame()).copy()\n",
    "    if not vs.empty:\n",
    "        vs = vs.dropna(subset=[\"UserID\"])\n",
    "        date_col = first_existing(vs, [\"ViewDate\", \"date\", \"Date\", \"viewDate\"])\n",
    "        if date_col is None:\n",
    "            vs = pd.DataFrame(columns=[\"UserID\", \"date\"])\n",
    "        else:\n",
    "            vs = coerce_datetime(vs, date_col)\n",
    "            vs = vs.rename(columns={date_col: \"date\"})\n",
    "            vs = vs[[\"UserID\", \"AdID\", \"date\"]].dropna(subset=[\"date\"])\n",
    "    else:\n",
    "        vs = pd.DataFrame(columns=[\"UserID\", \"AdID\", \"date\"])\n",
    "\n",
    "    # --- Source 2: SearchStream (clicks)\n",
    "    ss = tables.get(\"SearchStream\", pd.DataFrame()).copy()\n",
    "    if not ss.empty:\n",
    "        ss = ss.dropna(subset=[\"UserID\"])\n",
    "        date_col = first_existing(ss, [\"SearchDate\", \"date\", \"Date\"])\n",
    "        if date_col is None:\n",
    "            ss = pd.DataFrame(columns=[\"UserID\", \"AdID\", \"IsClick\", \"date\"])\n",
    "        else:\n",
    "            ss = coerce_datetime(ss, date_col)\n",
    "            ss = ss.rename(columns={date_col: \"date\"})\n",
    "            if \"IsClick\" not in ss.columns:\n",
    "                ss[\"IsClick\"] = np.nan\n",
    "            ss = ss[[\"UserID\", \"AdID\", \"IsClick\", \"date\"]].dropna(subset=[\"date\"])\n",
    "    else:\n",
    "        ss = pd.DataFrame(columns=[\"UserID\", \"AdID\", \"IsClick\", \"date\"])\n",
    "\n",
    "    # --- Source 3: PhoneRequestsStream (optional activity proxy)\n",
    "    prs = tables.get(\"PhoneRequestsStream\", pd.DataFrame()).copy()\n",
    "    if not prs.empty and \"UserID\" in prs.columns:\n",
    "        date_col = first_existing(prs, [\"PhoneRequestDate\", \"date\", \"Date\"])\n",
    "        if date_col is not None:\n",
    "            prs = coerce_datetime(prs, date_col)\n",
    "            prs = prs.rename(columns={date_col: \"date\"})\n",
    "            prs = prs[[\"UserID\", \"AdID\", \"date\"]].dropna(subset=[\"date\"])\n",
    "        else:\n",
    "            prs = pd.DataFrame(columns=[\"UserID\", \"AdID\", \"date\"])\n",
    "    else:\n",
    "        prs = pd.DataFrame(columns=[\"UserID\", \"AdID\", \"date\"])\n",
    "\n",
    "    # Build a combined event log per user (per-day granularity)\n",
    "    # Views\n",
    "    vs[\"view_cnt\"] = 1\n",
    "    vs_agg = vs.groupby([\"UserID\", \"date\"]).agg(view_cnt=(\"view_cnt\", \"sum\"),\n",
    "                                                ad_visited_n=(\"AdID\", \"nunique\")).reset_index()\n",
    "    # Clicks\n",
    "    if \"IsClick\" in ss.columns:\n",
    "        ss[\"click_cnt\"] = (ss[\"IsClick\"] == 1).astype(int)\n",
    "    else:\n",
    "        ss[\"click_cnt\"] = 0\n",
    "    ss_agg = ss.groupby([\"UserID\", \"date\"]).agg(click_cnt=(\"click_cnt\", \"sum\"),\n",
    "                                                ad_clicked_n=(\"AdID\", \"nunique\")).reset_index()\n",
    "    # Phone reqs\n",
    "    prs[\"phone_cnt\"] = 1\n",
    "    prs_agg = prs.groupby([\"UserID\", \"date\"]).agg(phone_cnt=(\"phone_cnt\", \"sum\"),\n",
    "                                                 ad_phone_n=(\"AdID\", \"nunique\")).reset_index()\n",
    "\n",
    "    # Merge daily aggregates\n",
    "    daily = (vs_agg.merge(ss_agg, on=[\"UserID\", \"date\"], how=\"outer\")\n",
    "                  .merge(prs_agg, on=[\"UserID\", \"date\"], how=\"outer\"))\n",
    "    daily = daily.sort_values([\"UserID\", \"date\"]).fillna(0)\n",
    "\n",
    "    # Expanding historical features, shifted by 1\n",
    "    def expanding_shifted(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        out = pd.DataFrame(index=g.index)\n",
    "        for c in [\"view_cnt\", \"ad_visited_n\", \"click_cnt\", \"ad_clicked_n\", \"phone_cnt\", \"ad_phone_n\"]:\n",
    "            if c in g:\n",
    "                out[f\"usr_cum_{c}\"] = g[c].cumsum().shift(1)\n",
    "                out[f\"usr_avg_{c}\"] = g[c].expanding().mean().shift(1)\n",
    "        # Activity rates\n",
    "        if \"click_cnt\" in g and \"view_cnt\" in g:\n",
    "            rate = (g[\"click_cnt\"].replace(0, np.nan) / g[\"view_cnt\"].replace(0, np.nan))\n",
    "            out[\"usr_rate_click_per_view\"] = rate.expanding().mean().shift(1)\n",
    "        return out\n",
    "\n",
    "    feats = daily.groupby(\"UserID\", group_keys=False).apply(expanding_shifted)\n",
    "    feats = pd.concat([daily[[\"UserID\", \"date\"]], feats], axis=1)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def build_ad_timeseries(tables: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns per-ad historical features keyed by ['AdID','date'].\n",
    "    \"\"\"\n",
    "    # Views\n",
    "    vs = tables.get(\"VisitStream\", pd.DataFrame()).copy()\n",
    "    if not vs.empty and \"AdID\" in vs.columns:\n",
    "        date_col = first_existing(vs, [\"ViewDate\", \"date\", \"Date\"])\n",
    "        if date_col is not None:\n",
    "            vs = coerce_datetime(vs, date_col).rename(columns={date_col: \"date\"})\n",
    "            vs = vs[[\"AdID\", \"UserID\", \"date\"]].dropna(subset=[\"date\"])\n",
    "        else:\n",
    "            vs = pd.DataFrame(columns=[\"AdID\", \"UserID\", \"date\"])\n",
    "    else:\n",
    "        vs = pd.DataFrame(columns=[\"AdID\", \"UserID\", \"date\"])\n",
    "\n",
    "    # Clicks (SearchStream)\n",
    "    ss = tables.get(\"SearchStream\", pd.DataFrame()).copy()\n",
    "    if not ss.empty and \"AdID\" in ss.columns:\n",
    "        date_col = first_existing(ss, [\"SearchDate\", \"date\", \"Date\"])\n",
    "        if date_col is not None:\n",
    "            ss = coerce_datetime(ss, date_col).rename(columns={date_col: \"date\"})\n",
    "            if \"IsClick\" not in ss.columns:\n",
    "                ss[\"IsClick\"] = 0\n",
    "            ss = ss[[\"AdID\", \"UserID\", \"IsClick\", \"date\"]].dropna(subset=[\"date\"])\n",
    "        else:\n",
    "            ss = pd.DataFrame(columns=[\"AdID\", \"UserID\", \"IsClick\", \"date\"])\n",
    "    else:\n",
    "        ss = pd.DataFrame(columns=[\"AdID\", \"UserID\", \"IsClick\", \"date\"])\n",
    "\n",
    "    # Aggregate per day per Ad\n",
    "    vs[\"view_cnt\"] = 1\n",
    "    vs_agg = vs.groupby([\"AdID\", \"date\"]).agg(view_cnt=(\"view_cnt\", \"sum\"),\n",
    "                                              user_view_n=(\"UserID\", \"nunique\")).reset_index()\n",
    "\n",
    "    ss[\"click_cnt\"] = (ss[\"IsClick\"] == 1).astype(int)\n",
    "    ss_agg = ss.groupby([\"AdID\", \"date\"]).agg(click_cnt=(\"click_cnt\", \"sum\"),\n",
    "                                              user_click_n=(\"UserID\", \"nunique\")).reset_index()\n",
    "\n",
    "    daily = vs_agg.merge(ss_agg, on=[\"AdID\", \"date\"], how=\"outer\").sort_values([\"AdID\", \"date\"]).fillna(0)\n",
    "\n",
    "    # Expanding and CTR\n",
    "    def expanding_shifted(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        out = pd.DataFrame(index=g.index)\n",
    "        for c in [\"view_cnt\", \"user_view_n\", \"click_cnt\", \"user_click_n\"]:\n",
    "            out[f\"ad_cum_{c}\"] = g[c].cumsum().shift(1)\n",
    "            out[f\"ad_avg_{c}\"] = g[c].expanding().mean().shift(1)\n",
    "        # Historical CTR\n",
    "        cum_click = g[\"click_cnt\"].cumsum().shift(1)\n",
    "        cum_view  = g[\"view_cnt\"].cumsum().shift(1)\n",
    "        out[\"ad_hist_ctr\"] = (cum_click / cum_view).replace([np.inf, -np.inf], np.nan)\n",
    "        return out\n",
    "\n",
    "    feats = daily.groupby(\"AdID\", group_keys=False).apply(expanding_shifted)\n",
    "    feats = pd.concat([daily[[\"AdID\", \"date\"]], feats], axis=1)\n",
    "\n",
    "    # Attach static ad attributes (no time leakage)\n",
    "    ads = tables.get(\"AdsInfo\", pd.DataFrame()).copy()\n",
    "    if not ads.empty and \"AdID\" in ads.columns:\n",
    "        ads_static = ads.drop_duplicates(\"AdID\")\n",
    "        # Optional: enrich with Category / Location\n",
    "        cat = tables.get(\"Category\", pd.DataFrame()).copy()\n",
    "        loc = tables.get(\"Location\", pd.DataFrame()).copy()\n",
    "        if not cat.empty and \"CategoryID\" in ads_static.columns:\n",
    "            ads_static = ads_static.merge(cat.add_prefix(\"cat_\"), left_on=\"CategoryID\", right_on=\"cat_CategoryID\", how=\"left\")\n",
    "        if not loc.empty and \"LocationID\" in ads_static.columns:\n",
    "            ads_static = ads_static.merge(loc.add_prefix(\"loc_\"), left_on=\"LocationID\", right_on=\"loc_LocationID\", how=\"left\")\n",
    "        feats = feats.merge(ads_static, on=\"AdID\", how=\"left\")\n",
    "    return feats\n"
   ],
   "id": "74659d08ece0042d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T18:54:26.894373Z",
     "start_time": "2025-08-13T18:54:26.770463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Build once and cache\n",
    "user_feats = build_user_timeseries(tables)\n",
    "ad_feats   = build_ad_timeseries(tables)\n",
    "\n",
    "user_feats.head(3), ad_feats.head(3)\n"
   ],
   "id": "8b43eba2a1c5b4de",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['UserID']",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_11690/1997542822.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# %%\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m# Build once and cache\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0muser_feats\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuild_user_timeseries\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mad_feats\u001B[0m   \u001B[0;34m=\u001B[0m \u001B[0mbuild_ad_timeseries\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0muser_feats\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mad_feats\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_11690/3590133507.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(tables)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[0;31m# --- Source 2: SearchStream (clicks)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[0mss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtables\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"SearchStream\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m         \u001B[0mss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropna\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msubset\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"UserID\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m         \u001B[0mdate_col\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfirst_existing\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"SearchDate\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"date\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Date\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdate_col\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0mss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"UserID\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"AdID\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"IsClick\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"date\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/TabPFN/lib/python3.10/site-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001B[0m\n\u001B[1;32m   6673\u001B[0m             \u001B[0max\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_axis\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0magg_axis\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6674\u001B[0m             \u001B[0mindices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_indexer_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msubset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6675\u001B[0m             \u001B[0mcheck\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindices\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6676\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mcheck\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0many\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 6677\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msubset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcheck\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   6678\u001B[0m             \u001B[0magg_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0magg_axis\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6679\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6680\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mthresh\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_default\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: ['UserID']"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%\n",
    "def index_to_df(idx: np.ndarray, mode: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert task index array to a DataFrame with standardized column names.\n",
    "    mode:\n",
    "      - 'user'     -> expects (UserID, date)\n",
    "      - 'ad'       -> expects (AdID, date)\n",
    "      - 'user-ad'  -> expects (UserID, AdID, date)\n",
    "    \"\"\"\n",
    "    arr = np.array(idx)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr.reshape(-1, 1)\n",
    "    if mode == \"user\":\n",
    "        assert arr.shape[1] == 2, f\"Expected 2 columns in index for 'user', got {arr.shape[1]}\"\n",
    "        df = pd.DataFrame(arr, columns=[\"UserID\", \"date\"])\n",
    "    elif mode == \"ad\":\n",
    "        assert arr.shape[1] == 2, f\"Expected 2 columns in index for 'ad', got {arr.shape[1]}\"\n",
    "        df = pd.DataFrame(arr, columns=[\"AdID\", \"date\"])\n",
    "    elif mode == \"user-ad\":\n",
    "        assert arr.shape[1] == 3, f\"Expected 3 columns in index for 'user-ad', got {arr.shape[1]}\"\n",
    "        df = pd.DataFrame(arr, columns=[\"UserID\", \"AdID\", \"date\"])\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def enrich_split(X: pd.DataFrame, idx: np.ndarray, join_mode: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given original task features X and its index, attach engineered features.\n",
    "    \"\"\"\n",
    "    idx_df = index_to_df(idx, join_mode)\n",
    "    base = X.reset_index(drop=True).copy()\n",
    "\n",
    "    if join_mode == \"user\":\n",
    "        # join user features by (UserID, date) asof within user\n",
    "        left = idx_df[[\"UserID\", \"date\"]].copy()\n",
    "        uf = merge_asof_by_group(left, user_feats, key_col=\"UserID\", date_col=\"date\")\n",
    "        out = pd.concat([base, uf.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    elif join_mode == \"ad\":\n",
    "        left = idx_df[[\"AdID\", \"date\"]].copy()\n",
    "        af = merge_asof_by_group(left, ad_feats, key_col=\"AdID\", date_col=\"date\")\n",
    "        out = pd.concat([base, af.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    elif join_mode == \"user-ad\":\n",
    "        # Merge user and ad streams separately, then concat\n",
    "        left_u = idx_df[[\"UserID\", \"date\"]].copy()\n",
    "        left_a = idx_df[[\"AdID\", \"date\"]].copy()\n",
    "        uf = merge_asof_by_group(left_u, user_feats, key_col=\"UserID\", date_col=\"date\")\n",
    "        af = merge_asof_by_group(left_a, ad_feats,   key_col=\"AdID\",   date_col=\"date\")\n",
    "        # Prefix to avoid collisions\n",
    "        uf = uf.add_prefix(\"u_\")\n",
    "        af = af.add_prefix(\"a_\")\n",
    "        out = pd.concat([base, uf.reset_index(drop=True), af.reset_index(drop=True)], axis=1)\n",
    "    else:\n",
    "        raise ValueError(join_mode)\n",
    "\n",
    "    return out\n"
   ],
   "id": "51a5ef8ce5c4bf19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%\n",
    "def run_merged_table_experiment(task_name: str):\n",
    "    spec = TASK_SPECS[task_name]\n",
    "    task = get_task(\"rel-avito\", task_name)\n",
    "\n",
    "    # Original features and labels\n",
    "    X_train, y_train = task.get_X_y(\"train\")\n",
    "    X_val,   y_val   = task.get_X_y(\"val\")\n",
    "    X_test,  y_test  = task.get_X_y(\"test\")\n",
    "\n",
    "    # Task indices (for safe alignment and time cutoffs)\n",
    "    idx_train = task.get_index(\"train\")\n",
    "    idx_val   = task.get_index(\"val\")\n",
    "    idx_test  = task.get_index(\"test\")\n",
    "\n",
    "    # Enrich with leak-safe histories (user/ad/user-ad)\n",
    "    X_train_en = enrich_split(X_train, idx_train, spec[\"join\"])\n",
    "    X_val_en   = enrich_split(X_val,   idx_val,   spec[\"join\"])\n",
    "    X_test_en  = enrich_split(X_test,  idx_test,  spec[\"join\"])\n",
    "\n",
    "    # Choose model\n",
    "    if spec[\"kind\"] == \"reg\":\n",
    "        model = TabPFNRegressor(device=DEVICE)\n",
    "        is_reg = True\n",
    "    else:\n",
    "        model = TabPFNClassifier(device=DEVICE)\n",
    "        is_reg = False\n",
    "\n",
    "    # Train\n",
    "    with elapsed_timer() as t:\n",
    "        model.fit(X_train_en, y_train)\n",
    "    fit_time = t()\n",
    "\n",
    "    # Predict val/test\n",
    "    with elapsed_timer() as t:\n",
    "        y_val_pred = model.predict(X_val_en)\n",
    "    pred_time_val = t()\n",
    "\n",
    "    with elapsed_timer() as t:\n",
    "        y_test_pred = model.predict(X_test_en)\n",
    "    pred_time_test = t()\n",
    "\n",
    "    # Probabilities for clf/link\n",
    "    prob_val = prob_test = None\n",
    "    if not is_reg:\n",
    "        try:\n",
    "            proba_val  = model.predict_proba(X_val_en)\n",
    "            proba_test = model.predict_proba(X_test_en)\n",
    "            if proba_val.ndim == 2 and proba_val.shape[1] == 2:\n",
    "                prob_val  = proba_val[:, 1]\n",
    "                prob_test = proba_test[:, 1]\n",
    "            else:\n",
    "                prob_val, prob_test = proba_val, proba_test\n",
    "        except Exception:\n",
    "            prob_val = prob_test = None\n",
    "\n",
    "    # Secondary metrics\n",
    "    if is_reg:\n",
    "        val_sec  = {**regression_metrics(y_val, y_val_pred)}\n",
    "        test_sec = {**regression_metrics(y_test, y_test_pred)}\n",
    "    else:\n",
    "        val_sec  = {**classification_metrics(y_val, y_val_pred, prob_val)}\n",
    "        test_sec = {**classification_metrics(y_test, y_test_pred, prob_test)}\n",
    "\n",
    "    # Primary via RelBench\n",
    "    if spec[\"kind\"] in [\"clf\", \"link\"]:\n",
    "        val_primary  = task.evaluate(prob_val if prob_val is not None else y_val_pred, \"val\")\n",
    "        test_primary = task.evaluate(prob_test if prob_test is not None else y_test_pred)\n",
    "    else:\n",
    "        val_primary  = task.evaluate(y_val_pred, \"val\")\n",
    "        test_primary = task.evaluate(y_test_pred)\n",
    "\n",
    "    res = {\n",
    "        \"val\":  {**val_sec,  \"fit_time\": fit_time, \"predict_time\": pred_time_val,  \"primary_metric_relbench\": val_primary},\n",
    "        \"test\": {**test_sec, \"fit_time\": fit_time, \"predict_time\": pred_time_test, \"primary_metric_relbench\": test_primary},\n",
    "    }\n",
    "    return res\n"
   ],
   "id": "aea7854cd70dc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%\n",
    "all_rows = []\n",
    "\n",
    "for task_name in TASKS:\n",
    "    print(f\"\\n=== {task_name} | Single-Table ===\")\n",
    "    single = run_single_table_experiment(task_name)\n",
    "    for split, metrics in single.items():\n",
    "        all_rows.append({\"task\": task_name, \"setting\": \"single\", \"split\": split, **metrics})\n",
    "\n",
    "    print(f\"=== {task_name} | Merged-Table ===\")\n",
    "    merged = run_merged_table_experiment(task_name)\n",
    "    for split, metrics in merged.items():\n",
    "        all_rows.append({\"task\": task_name, \"setting\": \"merged\", \"split\": split, **metrics})\n",
    "\n",
    "results_df = pd.DataFrame(all_rows).sort_values([\"task\", \"setting\", \"split\"]).reset_index(drop=True)\n",
    "results_df\n"
   ],
   "id": "10771f71f0f2b242"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%\n",
    "def plot_metric(metric: str, title: Optional[str] = None):\n",
    "    sub = results_df[(results_df[\"split\"] == \"test\") & results_df[metric].notna()]\n",
    "    if sub.empty:\n",
    "        print(f\"No data to plot for {metric}\")\n",
    "        return\n",
    "    pivot = sub.pivot(index=\"task\", columns=\"setting\", values=metric)\n",
    "    ax = pivot.plot(kind=\"bar\", figsize=(9, 4))\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(title or metric)\n",
    "    ax.grid(True, axis=\"y\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Classification/Link relevant (AUROC approximated via classification_metrics; primary in 'primary_metric_relbench')\n",
    "plot_metric(\"roc_auc\", \"Test AUROC (classification/link)\")\n",
    "plot_metric(\"f1_macro\", \"Test F1 Macro (classification/link)\")\n",
    "\n",
    "# Regression\n",
    "plot_metric(\"mae\", \"Test MAE (regression)\")\n",
    "plot_metric(\"mse\", \"Test MSE (regression)\")\n",
    "\n",
    "# Timing\n",
    "plot_metric(\"fit_time\", \"Fit Time (s)\")\n",
    "plot_metric(\"predict_time\", \"Predict Time (s)\")\n"
   ],
   "id": "51d155a8f19d87a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
