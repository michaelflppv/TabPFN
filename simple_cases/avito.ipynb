{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T07:41:12.449963Z",
     "start_time": "2025-08-14T07:41:12.445487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "import os\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "# RelBench\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "\n",
    "# TabPFN\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "\n",
    "# Device selection with MPS preference\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "id": "b04c45a19b5430c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T07:41:12.474874Z",
     "start_time": "2025-08-14T07:41:12.469069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = time.perf_counter()\n",
    "    yield lambda: time.perf_counter() - start\n",
    "\n",
    "\n",
    "def classification_metrics(y_true, y_pred, y_prob=None) -> Dict[str, float]:\n",
    "    out = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            out[\"roc_auc\"] = roc_auc_score(y_true, y_prob)\n",
    "        except Exception:\n",
    "            out[\"roc_auc\"] = np.nan\n",
    "    else:\n",
    "        out[\"roc_auc\"] = np.nan\n",
    "    return out\n",
    "\n",
    "\n",
    "def regression_metrics(y_true, y_pred) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        \"mse\": mean_squared_error(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "def to_pandas(table) -> pd.DataFrame:\n",
    "    if hasattr(table, \"to_pandas\"):\n",
    "        return table.to_pandas()\n",
    "    if hasattr(table, \"df\"):\n",
    "        return table.df if isinstance(table.df, pd.DataFrame) else pd.DataFrame(table.df)\n",
    "    return pd.DataFrame(table)\n",
    "\n",
    "\n",
    "def coerce_datetime(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    if col in df.columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def first_existing(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def merge_asof_by_group(left_idx: pd.DataFrame,\n",
    "                        right_feat: pd.DataFrame,\n",
    "                        key_col: str,\n",
    "                        date_col: str = \"date\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A leak-safe asof join: for each group by key_col, join the last known\n",
    "    feature snapshot at or before 'date' (shift should be applied in the feature builder).\n",
    "    \"\"\"\n",
    "    if right_feat.empty:\n",
    "        return pd.DataFrame(index=left_idx.index)\n",
    "\n",
    "    parts = []\n",
    "    for k, g_left in left_idx.groupby(key_col, sort=False):\n",
    "        g_right = right_feat[right_feat[key_col] == k]\n",
    "        if g_right.empty:\n",
    "            parts.append(pd.DataFrame(index=g_left.index))\n",
    "            continue\n",
    "        merged = pd.merge_asof(\n",
    "            g_left.sort_values(date_col),\n",
    "            g_right.sort_values(date_col),\n",
    "            on=date_col, direction=\"backward\"\n",
    "        )\n",
    "        merged.index = g_left.sort_values(date_col).index  # restore original row order\n",
    "        # drop keys/date from the right side (they duplicate)\n",
    "        drop_cols = [c for c in [key_col, date_col] if c in merged.columns]\n",
    "        merged = merged.drop(columns=[c for c in drop_cols if c in merged.columns], errors=\"ignore\")\n",
    "        parts.append(merged.sort_index())\n",
    "\n",
    "    return pd.concat(parts, axis=0).sort_index()\n"
   ],
   "id": "9e0b598fd9c1d009",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T07:41:12.680573Z",
     "start_time": "2025-08-14T07:41:12.487065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "dataset = get_dataset(\"rel-avito\")\n",
    "db = dataset.get_db()\n",
    "\n",
    "def load_all_tables(db) -> Dict[str, pd.DataFrame]:\n",
    "    names = db.table_dict\n",
    "    out = {}\n",
    "    for n in names:\n",
    "        out[n] = to_pandas(db.table_dict[n])\n",
    "        # Limit each table to 1000 samples\n",
    "        if len(out[n]) > 1000:\n",
    "            out[n] = out[n].sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "tables = load_all_tables(db)\n",
    "list(tables.keys())"
   ],
   "id": "5302bc3993511b05",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Location',\n",
       " 'PhoneRequestsStream',\n",
       " 'SearchInfo',\n",
       " 'VisitStream',\n",
       " 'Category',\n",
       " 'SearchStream',\n",
       " 'UserInfo',\n",
       " 'AdsInfo']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T07:41:12.689577Z",
     "start_time": "2025-08-14T07:41:12.685158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Normalize date columns for relevant tables (Avito schema)\n",
    "# We will standardize to 'date' column names inside our engineered frames.\n",
    "date_mappings = {\n",
    "    \"SearchInfo\": [\"SearchDate\"],\n",
    "    \"SearchStream\": [\"SearchDate\"],\n",
    "    \"VisitStream\": [\"ViewDate\"],\n",
    "    \"PhoneRequestsStream\": [\"PhoneRequestDate\"]\n",
    "}\n",
    "\n",
    "for tname, df in tables.items():\n",
    "    # coerce known date columns if exist\n",
    "    if tname in date_mappings:\n",
    "        for c in date_mappings[tname]:\n",
    "            if c in df.columns:\n",
    "                df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    # also coerce any column containing 'date' substring (robustness)\n",
    "    for c in df.columns:\n",
    "        if \"date\" in c.lower():\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n"
   ],
   "id": "2779de11f7096822",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T07:41:12.708482Z",
     "start_time": "2025-08-14T07:41:12.706177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "TASK_SPECS = {\n",
    "    \"user-visits\":    {\"kind\": \"clf\",  \"join\": \"user\"},\n",
    "    \"user-clicks\":    {\"kind\": \"clf\",  \"join\": \"user\"},\n",
    "    \"ad-ctr\":         {\"kind\": \"reg\",  \"join\": \"ad\"},\n",
    "    \"user-ad-visit\":  {\"kind\": \"link\", \"join\": \"user-ad\"}  # probabilities â†’ MAP@k via task.evaluate\n",
    "}\n",
    "\n",
    "TASKS = list(TASK_SPECS.keys())\n",
    "TASKS\n"
   ],
   "id": "1d50da024f2ae835",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user-visits', 'user-clicks', 'ad-ctr', 'user-ad-visit']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T07:41:12.724178Z",
     "start_time": "2025-08-14T07:41:12.719471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def run_single_table_experiment(task_name: str):\n",
    "    spec = TASK_SPECS[task_name]\n",
    "    task = get_task(\"rel-avito\", task_name)\n",
    "\n",
    "    # Create a training, validation, and test splits\n",
    "    train_table = task.get_table(\"train\")\n",
    "    val_table = task.get_table(\"val\")\n",
    "    test_table = task.get_table(\"test\", mask_input_cols=False)\n",
    "\n",
    "    # Input features need to be numeric, otherwise encoded\n",
    "    df = train_table.df\n",
    "    if len(df) > 1000:\n",
    "        df = df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "    X_train = df.drop(columns=[task.target_col]).select_dtypes(include=[np.number])\n",
    "    y_train = df[task.target_col]\n",
    "\n",
    "    df = val_table.df\n",
    "    if len(df) > 1000:\n",
    "        df = df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "    X_val = df.drop(columns=[task.target_col]).select_dtypes(include=[np.number])\n",
    "    y_val = df[task.target_col]\n",
    "\n",
    "    df = test_table.df\n",
    "    if len(df) > 1000:\n",
    "        df = df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "    X_test = df.drop(columns=[task.target_col]).select_dtypes(include=[np.number])\n",
    "    y_test = df[task.target_col]\n",
    "\n",
    "    # Choose model\n",
    "    if spec[\"kind\"] == \"reg\":\n",
    "        model = TabPFNRegressor(device=DEVICE)\n",
    "        is_reg = True\n",
    "    else:\n",
    "        model = TabPFNClassifier(device=DEVICE)\n",
    "        is_reg = False\n",
    "\n",
    "    # Fit\n",
    "    with elapsed_timer() as t:\n",
    "        model.fit(X_train, y_train)\n",
    "    fit_time = t()\n",
    "\n",
    "    # Predict val/test\n",
    "    with elapsed_timer() as t:\n",
    "        y_val_pred = model.predict(X_val)\n",
    "    pred_time_val = t()\n",
    "\n",
    "    with elapsed_timer() as t:\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    pred_time_test = t()\n",
    "\n",
    "    # Probabilities for clf/link\n",
    "    prob_val = prob_test = None\n",
    "    if not is_reg:\n",
    "        try:\n",
    "            proba_val  = model.predict_proba(X_val)\n",
    "            proba_test = model.predict_proba(X_test)\n",
    "            # binary -> keep positive class prob\n",
    "            if proba_val.ndim == 2 and proba_val.shape[1] == 2:\n",
    "                prob_val = proba_val[:, 1]\n",
    "                prob_test = proba_test[:, 1]\n",
    "            else:\n",
    "                # multiclass or already 1D\n",
    "                prob_val = proba_val\n",
    "                prob_test = proba_test\n",
    "        except Exception:\n",
    "            prob_val = prob_test = None\n",
    "\n",
    "    # Secondary metrics\n",
    "    if is_reg:\n",
    "        val_sec  = {**regression_metrics(y_val, y_val_pred)}\n",
    "        test_sec = {**regression_metrics(y_test, y_test_pred)}\n",
    "    else:\n",
    "        val_sec  = {**classification_metrics(y_val, y_val_pred, prob_val)}\n",
    "        test_sec = {**classification_metrics(y_test, y_test_pred, prob_test)}\n",
    "\n",
    "    # Primary metric via RelBench\n",
    "    if spec[\"kind\"] in [\"clf\", \"link\"]:\n",
    "        # RelBench expects probabilities/scores\n",
    "        val_primary  = task.evaluate(prob_val if prob_val is not None else y_val_pred, \"val\")\n",
    "        test_primary = task.evaluate(prob_test if prob_test is not None else y_test_pred)\n",
    "    else:\n",
    "        # regression expects numeric predictions\n",
    "        val_primary  = task.evaluate(y_val_pred, \"val\")\n",
    "        test_primary = task.evaluate(y_test_pred)\n",
    "\n",
    "    res = {\n",
    "        \"val\":  {**val_sec,  \"fit_time\": fit_time, \"predict_time\": pred_time_val,  \"primary_metric_relbench\": val_primary},\n",
    "        \"test\": {**test_sec, \"fit_time\": fit_time, \"predict_time\": pred_time_test, \"primary_metric_relbench\": test_primary},\n",
    "    }\n",
    "    return res\n"
   ],
   "id": "adce041b5fa0f4e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T07:41:12.745071Z",
     "start_time": "2025-08-14T07:41:12.733771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def build_user_timeseries(tables: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns per-user historical features keyed by ['UserID','date'].\n",
    "    Features are cumulative/expanding and SHIFTED by 1 to avoid look-ahead leakage.\n",
    "    \"\"\"\n",
    "    # --- Source 1: VisitStream (views)\n",
    "    vs = tables.get(\"VisitStream\", pd.DataFrame()).copy()\n",
    "    if not vs.empty:\n",
    "        vs = vs.dropna(subset=[\"UserID\"])\n",
    "        date_col = first_existing(vs, [\"ViewDate\", \"date\", \"Date\", \"viewDate\"])\n",
    "        if date_col is None:\n",
    "            vs = pd.DataFrame(columns=[\"UserID\", \"AdID\", \"date\"])\n",
    "        else:\n",
    "            vs = coerce_datetime(vs, date_col)\n",
    "            vs = vs.rename(columns={date_col: \"date\"})\n",
    "            required_vs_cols = [\"UserID\", \"AdID\", \"date\"]\n",
    "            existing_vs_cols = [c for c in required_vs_cols if c in vs.columns]\n",
    "            vs = vs[existing_vs_cols].dropna(subset=[\"date\"]) if \"date\" in existing_vs_cols else pd.DataFrame(columns=required_vs_cols)\n",
    "    else:\n",
    "        vs = pd.DataFrame(columns=[\"UserID\", \"AdID\", \"date\"])\n",
    "\n",
    "    # --- Source 2: SearchStream (clicks)\n",
    "    ss = tables.get(\"SearchStream\", pd.DataFrame()).copy()\n",
    "    if not ss.empty:\n",
    "        if \"UserID\" in ss.columns:\n",
    "            ss = ss.dropna(subset=[\"UserID\"])\n",
    "        date_col = first_existing(ss, [\"SearchDate\", \"date\", \"Date\"])\n",
    "        if date_col is None:\n",
    "            ss = pd.DataFrame(columns=[\"UserID\", \"AdID\", \"IsClick\", \"date\"])\n",
    "        else:\n",
    "            ss = coerce_datetime(ss, date_col)\n",
    "            ss = ss.rename(columns={date_col: \"date\"})\n",
    "            if \"IsClick\" not in ss.columns:\n",
    "                ss[\"IsClick\"] = np.nan\n",
    "            required_ss_cols = [\"UserID\", \"AdID\", \"IsClick\", \"date\"]\n",
    "            existing_ss_cols = [c for c in required_ss_cols if c in ss.columns]\n",
    "            ss = ss[existing_ss_cols].dropna(subset=[\"date\"]) if \"date\" in existing_ss_cols else pd.DataFrame(columns=required_ss_cols)\n",
    "    else:\n",
    "        ss = pd.DataFrame(columns=[\"UserID\", \"AdID\", \"IsClick\", \"date\"])\n",
    "\n",
    "    # --- Source 3: PhoneRequestsStream (optional activity proxy)\n",
    "    prs = tables.get(\"PhoneRequestsStream\", pd.DataFrame()).copy()\n",
    "    if not prs.empty and \"UserID\" in prs.columns:\n",
    "        date_col = first_existing(prs, [\"PhoneRequestDate\", \"date\", \"Date\"])\n",
    "        if date_col is not None:\n",
    "            prs = coerce_datetime(prs, date_col)\n",
    "            prs = prs.rename(columns={date_col: \"date\"})\n",
    "            required_prs_cols = [\"UserID\", \"AdID\", \"date\"]\n",
    "            existing_prs_cols = [c for c in required_prs_cols if c in prs.columns]\n",
    "            prs = prs[existing_prs_cols].dropna(subset=[\"date\"]) if \"date\" in existing_prs_cols else pd.DataFrame(columns=required_prs_cols)\n",
    "        else:\n",
    "            prs = pd.DataFrame(columns=[\"UserID\", \"AdID\", \"date\"])\n",
    "    else:\n",
    "        prs = pd.DataFrame(columns=[\"UserID\", \"AdID\", \"date\"])\n",
    "\n",
    "    # Build a combined event log per user (per-day granularity)\n",
    "    # Views\n",
    "    if all(c in vs.columns for c in [\"UserID\", \"date\", \"AdID\"]):\n",
    "        vs[\"view_cnt\"] = 1\n",
    "        vs_agg = vs.groupby([\"UserID\", \"date\"]).agg(view_cnt=(\"view_cnt\", \"sum\"),\n",
    "                                                    ad_visited_n=(\"AdID\", \"nunique\")).reset_index()\n",
    "    else:\n",
    "        vs_agg = pd.DataFrame(columns=[\"UserID\", \"date\", \"view_cnt\", \"ad_visited_n\"])\n",
    "    # Clicks\n",
    "    if all(c in ss.columns for c in [\"UserID\", \"date\", \"AdID\"]):\n",
    "        if \"IsClick\" in ss.columns:\n",
    "            ss[\"click_cnt\"] = (ss[\"IsClick\"] == 1).astype(int)\n",
    "        else:\n",
    "            ss[\"click_cnt\"] = 0\n",
    "        ss_agg = ss.groupby([\"UserID\", \"date\"]).agg(click_cnt=(\"click_cnt\", \"sum\"),\n",
    "                                                    ad_clicked_n=(\"AdID\", \"nunique\")).reset_index()\n",
    "    else:\n",
    "        ss_agg = pd.DataFrame(columns=[\"UserID\", \"date\", \"click_cnt\", \"ad_clicked_n\"])\n",
    "    # Phone reqs\n",
    "    if all(c in prs.columns for c in [\"UserID\", \"date\", \"AdID\"]):\n",
    "        prs[\"phone_cnt\"] = 1\n",
    "        prs_agg = prs.groupby([\"UserID\", \"date\"]).agg(phone_cnt=(\"phone_cnt\", \"sum\"),\n",
    "                                                      ad_phone_n=(\"AdID\", \"nunique\")).reset_index()\n",
    "    else:\n",
    "        prs_agg = pd.DataFrame(columns=[\"UserID\", \"date\", \"phone_cnt\", \"ad_phone_n\"])\n",
    "\n",
    "    # Merge daily aggregates\n",
    "    daily = (vs_agg.merge(ss_agg, on=[\"UserID\", \"date\"], how=\"outer\")\n",
    "                  .merge(prs_agg, on=[\"UserID\", \"date\"], how=\"outer\"))\n",
    "    daily = daily.sort_values([\"UserID\", \"date\"]).fillna(0)\n",
    "\n",
    "    # Expanding historical features, shifted by 1\n",
    "    def expanding_shifted(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        out = pd.DataFrame(index=g.index)\n",
    "        for c in [\"view_cnt\", \"ad_visited_n\", \"click_cnt\", \"ad_clicked_n\", \"phone_cnt\", \"ad_phone_n\"]:\n",
    "            if c in g:\n",
    "                out[f\"usr_cum_{c}\"] = g[c].cumsum().shift(1)\n",
    "                out[f\"usr_avg_{c}\"] = g[c].expanding().mean().shift(1)\n",
    "        # Activity rates\n",
    "        if \"click_cnt\" in g and \"view_cnt\" in g:\n",
    "            rate = (g[\"click_cnt\"].replace(0, np.nan) / g[\"view_cnt\"].replace(0, np.nan))\n",
    "            out[\"usr_rate_click_per_view\"] = rate.expanding().mean().shift(1)\n",
    "        return out\n",
    "\n",
    "    feats = daily.groupby(\"UserID\", group_keys=False).apply(expanding_shifted)\n",
    "    feats = pd.concat([daily[[\"UserID\", \"date\"]], feats], axis=1)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def build_ad_timeseries(tables: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns per-ad historical features keyed by ['AdID','date'].\n",
    "    \"\"\"\n",
    "    # Views\n",
    "    vs = tables.get(\"VisitStream\", pd.DataFrame()).copy()\n",
    "    if not vs.empty and \"AdID\" in vs.columns:\n",
    "        date_col = first_existing(vs, [\"ViewDate\", \"date\", \"Date\"])\n",
    "        if date_col is not None:\n",
    "            vs = coerce_datetime(vs, date_col).rename(columns={date_col: \"date\"})\n",
    "            required_vs_cols = [\"AdID\", \"UserID\", \"date\"]\n",
    "            existing_vs_cols = [c for c in required_vs_cols if c in vs.columns]\n",
    "            vs = vs[existing_vs_cols].dropna(subset=[\"date\"]) if \"date\" in existing_vs_cols else pd.DataFrame(columns=required_vs_cols)\n",
    "        else:\n",
    "            vs = pd.DataFrame(columns=[\"AdID\", \"UserID\", \"date\"])\n",
    "    else:\n",
    "        vs = pd.DataFrame(columns=[\"AdID\", \"UserID\", \"date\"])\n",
    "\n",
    "    # Clicks (SearchStream)\n",
    "    ss = tables.get(\"SearchStream\", pd.DataFrame()).copy()\n",
    "    if not ss.empty and \"AdID\" in ss.columns:\n",
    "        date_col = first_existing(ss, [\"SearchDate\", \"date\", \"Date\"])\n",
    "        if date_col is not None:\n",
    "            ss = coerce_datetime(ss, date_col).rename(columns={date_col: \"date\"})\n",
    "            if \"IsClick\" not in ss.columns:\n",
    "                ss[\"IsClick\"] = 0\n",
    "            required_ss_cols = [\"AdID\", \"UserID\", \"IsClick\", \"date\"]\n",
    "            existing_ss_cols = [c for c in required_ss_cols if c in ss.columns]\n",
    "            ss = ss[existing_ss_cols].dropna(subset=[\"date\"]) if \"date\" in existing_ss_cols else pd.DataFrame(columns=required_ss_cols)\n",
    "        else:\n",
    "            ss = pd.DataFrame(columns=[\"AdID\", \"UserID\", \"IsClick\", \"date\"])\n",
    "    else:\n",
    "        ss = pd.DataFrame(columns=[\"AdID\", \"UserID\", \"IsClick\", \"date\"])\n",
    "\n",
    "    # Aggregate per day per Ad\n",
    "    if all(c in vs.columns for c in [\"AdID\", \"date\", \"UserID\"]):\n",
    "        vs[\"view_cnt\"] = 1\n",
    "        vs_agg = vs.groupby([\"AdID\", \"date\"]).agg(view_cnt=(\"view_cnt\", \"sum\"),\n",
    "                                                  user_view_n=(\"UserID\", \"nunique\")).reset_index()\n",
    "    else:\n",
    "        vs_agg = pd.DataFrame(columns=[\"AdID\", \"date\", \"view_cnt\", \"user_view_n\"])\n",
    "\n",
    "    if all(c in ss.columns for c in [\"AdID\", \"date\", \"UserID\", \"IsClick\"]):\n",
    "        ss[\"click_cnt\"] = (ss[\"IsClick\"] == 1).astype(int)\n",
    "        ss_agg = ss.groupby([\"AdID\", \"date\"]).agg(click_cnt=(\"click_cnt\", \"sum\"),\n",
    "                                                  user_click_n=(\"UserID\", \"nunique\")).reset_index()\n",
    "    else:\n",
    "        ss_agg = pd.DataFrame(columns=[\"AdID\", \"date\", \"click_cnt\", \"user_click_n\"])\n",
    "\n",
    "    daily = vs_agg.merge(ss_agg, on=[\"AdID\", \"date\"], how=\"outer\").sort_values([\"AdID\", \"date\"]).fillna(0)\n",
    "\n",
    "    # Expanding and CTR\n",
    "    def expanding_shifted(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        out = pd.DataFrame(index=g.index)\n",
    "        for c in [\"view_cnt\", \"user_view_n\", \"click_cnt\", \"user_click_n\"]:\n",
    "            if c in g:\n",
    "                out[f\"ad_cum_{c}\"] = g[c].cumsum().shift(1)\n",
    "                out[f\"ad_avg_{c}\"] = g[c].expanding().mean().shift(1)\n",
    "        # Historical CTR\n",
    "        if \"click_cnt\" in g and \"view_cnt\" in g:\n",
    "            cum_click = g[\"click_cnt\"].cumsum().shift(1)\n",
    "            cum_view  = g[\"view_cnt\"].cumsum().shift(1)\n",
    "            out[\"ad_hist_ctr\"] = (cum_click / cum_view).replace([np.inf, -np.inf], np.nan)\n",
    "        return out\n",
    "\n",
    "    feats = daily.groupby(\"AdID\", group_keys=False).apply(expanding_shifted)\n",
    "    feats = pd.concat([daily[[\"AdID\", \"date\"]], feats], axis=1)\n",
    "\n",
    "    # Attach static ad attributes (no time leakage)\n",
    "    ads = tables.get(\"AdsInfo\", pd.DataFrame()).copy()\n",
    "    if not ads.empty and \"AdID\" in ads.columns:\n",
    "        ads_static = ads.drop_duplicates(\"AdID\")\n",
    "        # Optional: enrich with Category / Location\n",
    "        cat = tables.get(\"Category\", pd.DataFrame()).copy()\n",
    "        loc = tables.get(\"Location\", pd.DataFrame()).copy()\n",
    "        if not cat.empty and \"CategoryID\" in ads_static.columns:\n",
    "            ads_static = ads_static.merge(cat.add_prefix(\"cat_\"), left_on=\"CategoryID\", right_on=\"cat_CategoryID\", how=\"left\")\n",
    "        if not loc.empty and \"LocationID\" in ads_static.columns:\n",
    "            ads_static = ads_static.merge(loc.add_prefix(\"loc_\"), left_on=\"LocationID\", right_on=\"loc_LocationID\", how=\"left\")\n",
    "        feats = feats.merge(ads_static, on=\"AdID\", how=\"left\")\n",
    "    return feats\n"
   ],
   "id": "74659d08ece0042d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T07:41:16.367561Z",
     "start_time": "2025-08-14T07:41:12.755204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute cumulative user and ad features and shift them to avoid leakage.\n",
    "# This will be used to enrich task indices later.\n",
    "user_feats = build_user_timeseries(tables)\n",
    "ad_feats   = build_ad_timeseries(tables)\n",
    "\n",
    "user_feats.head(3), ad_feats.head(3)\n"
   ],
   "id": "8b43eba2a1c5b4de",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_14476/1342376210.py:86: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  daily = daily.sort_values([\"UserID\", \"date\"]).fillna(0)\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_14476/1342376210.py:101: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  feats = daily.groupby(\"UserID\", group_keys=False).apply(expanding_shifted)\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_14476/1342376210.py:155: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  daily = vs_agg.merge(ss_agg, on=[\"AdID\", \"date\"], how=\"outer\").sort_values([\"AdID\", \"date\"]).fillna(0)\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_14476/1342376210.py:171: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  feats = daily.groupby(\"AdID\", group_keys=False).apply(expanding_shifted)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   UserID                date  usr_cum_view_cnt  usr_avg_view_cnt  \\\n",
       " 0      24 2015-05-13 22:53:35               NaN               NaN   \n",
       " 1      31 2015-05-02 21:13:01               NaN               NaN   \n",
       " 2      50 2015-04-29 14:17:19               NaN               NaN   \n",
       " \n",
       "    usr_cum_ad_visited_n  usr_avg_ad_visited_n  usr_cum_click_cnt  \\\n",
       " 0                   NaN                   NaN                NaN   \n",
       " 1                   NaN                   NaN                NaN   \n",
       " 2                   NaN                   NaN                NaN   \n",
       " \n",
       "    usr_avg_click_cnt  usr_cum_ad_clicked_n  usr_avg_ad_clicked_n  \\\n",
       " 0                NaN                   NaN                   NaN   \n",
       " 1                NaN                   NaN                   NaN   \n",
       " 2                NaN                   NaN                   NaN   \n",
       " \n",
       "    usr_cum_phone_cnt  usr_avg_phone_cnt  usr_cum_ad_phone_n  \\\n",
       " 0                NaN                NaN                 NaN   \n",
       " 1                NaN                NaN                 NaN   \n",
       " 2                NaN                NaN                 NaN   \n",
       " \n",
       "    usr_avg_ad_phone_n  usr_rate_click_per_view  \n",
       " 0                 NaN                      NaN  \n",
       " 1                 NaN                      NaN  \n",
       " 2                 NaN                      NaN  ,\n",
       "     AdID                date  ad_cum_view_cnt  ad_avg_view_cnt  \\\n",
       " 0   5328 2015-05-07 13:40:58              NaN              NaN   \n",
       " 1   9228 2015-05-09 09:27:32              NaN              NaN   \n",
       " 2  24560 2015-05-01 01:57:02              NaN              NaN   \n",
       " \n",
       "    ad_cum_user_view_n  ad_avg_user_view_n  ad_cum_click_cnt  ad_avg_click_cnt  \\\n",
       " 0                 NaN                 NaN               NaN               NaN   \n",
       " 1                 NaN                 NaN               NaN               NaN   \n",
       " 2                 NaN                 NaN               NaN               NaN   \n",
       " \n",
       "    ad_cum_user_click_n  ad_avg_user_click_n  ...  Title  IsContext  \\\n",
       " 0                  NaN                  NaN  ...    NaN        NaN   \n",
       " 1                  NaN                  NaN  ...    NaN        NaN   \n",
       " 2                  NaN                  NaN  ...    NaN        NaN   \n",
       " \n",
       "    cat_CategoryID  cat_Level cat_ParentCategoryID  cat_SubcategoryID  \\\n",
       " 0            <NA>        NaN                  NaN                NaN   \n",
       " 1            <NA>        NaN                  NaN                NaN   \n",
       " 2            <NA>        NaN                  NaN                NaN   \n",
       " \n",
       "    loc_LocationID  loc_Level  loc_RegionID  loc_CityID  \n",
       " 0            <NA>        NaN           NaN         NaN  \n",
       " 1            <NA>        NaN           NaN         NaN  \n",
       " 2            <NA>        NaN           NaN         NaN  \n",
       " \n",
       " [3 rows x 24 columns])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T07:41:16.385985Z",
     "start_time": "2025-08-14T07:41:16.381866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def index_to_df(idx: np.ndarray, mode: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert task index array to a DataFrame with standardized column names.\n",
    "    mode:\n",
    "      - 'user'     -> expects (UserID, date)\n",
    "      - 'ad'       -> expects (AdID, date)\n",
    "      - 'user-ad'  -> expects (UserID, AdID, date)\n",
    "    \"\"\"\n",
    "    arr = np.array(idx)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr.reshape(-1, 1)\n",
    "    if mode == \"user\":\n",
    "        assert arr.shape[1] == 2, f\"Expected 2 columns in index for 'user', got {arr.shape[1]}\"\n",
    "        df = pd.DataFrame(arr, columns=[\"UserID\", \"date\"])\n",
    "    elif mode == \"ad\":\n",
    "        assert arr.shape[1] == 2, f\"Expected 2 columns in index for 'ad', got {arr.shape[1]}\"\n",
    "        df = pd.DataFrame(arr, columns=[\"AdID\", \"date\"])\n",
    "    elif mode == \"user-ad\":\n",
    "        assert arr.shape[1] == 3, f\"Expected 3 columns in index for 'user-ad', got {arr.shape[1]}\"\n",
    "        df = pd.DataFrame(arr, columns=[\"UserID\", \"AdID\", \"date\"])\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def enrich_split(X: pd.DataFrame, idx: np.ndarray, join_mode: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given original task features X and its index, attach engineered features.\n",
    "    \"\"\"\n",
    "    idx_df = index_to_df(idx, join_mode)\n",
    "    base = X.reset_index(drop=True).copy()\n",
    "\n",
    "    if join_mode == \"user\":\n",
    "        # join user features by (UserID, date) asof within user\n",
    "        left = idx_df[[\"UserID\", \"date\"]].copy()\n",
    "        uf = merge_asof_by_group(left, user_feats, key_col=\"UserID\", date_col=\"date\")\n",
    "        out = pd.concat([base, uf.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    elif join_mode == \"ad\":\n",
    "        left = idx_df[[\"AdID\", \"date\"]].copy()\n",
    "        af = merge_asof_by_group(left, ad_feats, key_col=\"AdID\", date_col=\"date\")\n",
    "        out = pd.concat([base, af.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    elif join_mode == \"user-ad\":\n",
    "        # Merge user and ad streams separately, then concat\n",
    "        left_u = idx_df[[\"UserID\", \"date\"]].copy()\n",
    "        left_a = idx_df[[\"AdID\", \"date\"]].copy()\n",
    "        uf = merge_asof_by_group(left_u, user_feats, key_col=\"UserID\", date_col=\"date\")\n",
    "        af = merge_asof_by_group(left_a, ad_feats,   key_col=\"AdID\",   date_col=\"date\")\n",
    "        # Prefix to avoid collisions\n",
    "        uf = uf.add_prefix(\"u_\")\n",
    "        af = af.add_prefix(\"a_\")\n",
    "        out = pd.concat([base, uf.reset_index(drop=True), af.reset_index(drop=True)], axis=1)\n",
    "    else:\n",
    "        raise ValueError(join_mode)\n",
    "\n",
    "    return out\n"
   ],
   "id": "51a5ef8ce5c4bf19",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T07:41:16.393592Z",
     "start_time": "2025-08-14T07:41:16.388301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def run_merged_table_experiment(task_name: str):\n",
    "    spec = TASK_SPECS[task_name]\n",
    "    task = get_task(\"rel-avito\", task_name)\n",
    "\n",
    "    train_table = task.get_table(\"train\")\n",
    "    val_table = task.get_table(\"val\")\n",
    "    test_table = task.get_table(\"test\", mask_input_cols=False)\n",
    "\n",
    "    df_train = train_table.df\n",
    "    if len(df_train) > 1000:\n",
    "        df_train = df_train.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "    df_val = val_table.df\n",
    "    if len(df_val) > 1000:\n",
    "        df_val = df_val.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "    df_test = test_table.df\n",
    "    if len(df_test) > 1000:\n",
    "        df_test = df_test.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Determine index columns based on join mode\n",
    "    if spec[\"join\"] == \"user\":\n",
    "        idx_train = list(zip(df_train[\"UserID\"], df_train[\"date\"]))\n",
    "        idx_val   = list(zip(df_val[\"UserID\"], df_val[\"date\"]))\n",
    "        idx_test  = list(zip(df_test[\"UserID\"], df_test[\"date\"]))\n",
    "    elif spec[\"join\"] == \"ad\":\n",
    "        idx_train = list(zip(df_train[\"AdID\"], df_train[\"date\"]))\n",
    "        idx_val   = list(zip(df_val[\"AdID\"], df_val[\"date\"]))\n",
    "        idx_test  = list(zip(df_test[\"AdID\"], df_test[\"date\"]))\n",
    "    elif spec[\"join\"] == \"user-ad\":\n",
    "        idx_train = list(zip(df_train[\"UserID\"], df_train[\"AdID\"], df_train[\"date\"]))\n",
    "        idx_val   = list(zip(df_val[\"UserID\"], df_val[\"AdID\"], df_val[\"date\"]))\n",
    "        idx_test  = list(zip(df_test[\"UserID\"], df_test[\"AdID\"], df_test[\"date\"]))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown join mode: {spec['join']}\")\n",
    "\n",
    "    X_train = df_train.drop(columns=[task.target_col]).select_dtypes(include=[np.number])\n",
    "    y_train = df_train[task.target_col]\n",
    "\n",
    "    X_val = df_val.drop(columns=[task.target_col]).select_dtypes(include=[np.number])\n",
    "    y_val = df_val[task.target_col]\n",
    "\n",
    "    X_test = df_test.drop(columns=[task.target_col]).select_dtypes(include=[np.number])\n",
    "    y_test = df_test[task.target_col]\n",
    "\n",
    "    # Choose model\n",
    "    if spec[\"kind\"] == \"reg\":\n",
    "        model = TabPFNRegressor(device=DEVICE)\n",
    "        is_reg = True\n",
    "    else:\n",
    "        model = TabPFNClassifier(device=DEVICE)\n",
    "        is_reg = False\n",
    "\n",
    "    # Train\n",
    "    with elapsed_timer() as t:\n",
    "        model.fit(X_train, y_train)\n",
    "    fit_time = t()\n",
    "\n",
    "    # Predict val/test\n",
    "    with elapsed_timer() as t:\n",
    "        y_val_pred = model.predict(X_val)\n",
    "    pred_time_val = t()\n",
    "\n",
    "    with elapsed_timer() as t:\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    pred_time_test = t()\n",
    "\n",
    "    # Probabilities for clf/link\n",
    "    prob_val = prob_test = None\n",
    "    if not is_reg:\n",
    "        try:\n",
    "            proba_val  = model.predict_proba(X_val)\n",
    "            proba_test = model.predict_proba(X_test)\n",
    "            if proba_val.ndim == 2 and proba_val.shape[1] == 2:\n",
    "                prob_val  = proba_val[:, 1]\n",
    "                prob_test = proba_test[:, 1]\n",
    "            else:\n",
    "                prob_val, prob_test = proba_val, proba_test\n",
    "        except Exception:\n",
    "            prob_val = prob_test = None\n",
    "\n",
    "    # Secondary metrics\n",
    "    if is_reg:\n",
    "        val_sec  = {**regression_metrics(y_val, y_val_pred)}\n",
    "        test_sec = {**regression_metrics(y_test, y_test_pred)}\n",
    "    else:\n",
    "        val_sec  = {**classification_metrics(y_val, y_val_pred, prob_val)}\n",
    "        test_sec = {**classification_metrics(y_test, y_test_pred, prob_test)}\n",
    "\n",
    "    # Primary via RelBench\n",
    "    if spec[\"kind\"] in [\"clf\", \"link\"]:\n",
    "        val_primary  = task.evaluate(prob_val if prob_val is not None else y_val_pred, \"val\")\n",
    "        test_primary = task.evaluate(prob_test if prob_test is not None else y_test_pred)\n",
    "    else:\n",
    "        val_primary  = task.evaluate(y_val_pred, \"val\")\n",
    "        test_primary = task.evaluate(y_test_pred)\n",
    "\n",
    "    res = {\n",
    "        \"val\":  {**val_sec,  \"fit_time\": fit_time, \"predict_time\": pred_time_val,  \"primary_metric_relbench\": val_primary},\n",
    "        \"test\": {**test_sec, \"fit_time\": fit_time, \"predict_time\": pred_time_test, \"primary_metric_relbench\": test_primary},\n",
    "    }\n",
    "    return res\n"
   ],
   "id": "aea7854cd70dc0",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T07:45:25.925997Z",
     "start_time": "2025-08-14T07:41:16.410639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Directory to save tables and results\n",
    "OUTPUT_DIR = \"avito_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for task_name in TASKS:\n",
    "    print(f\"\\n=== {task_name} | Single-Table ===\")\n",
    "    single = run_single_table_experiment(task_name)\n",
    "    for split, metrics in single.items():\n",
    "        # Print and save single-table split\n",
    "        print(f\"Task: {task_name}, Setting: single, Split: {split}\")\n",
    "        # Save the corresponding table\n",
    "        table = get_task(\"rel-avito\", task_name).get_table(split).df\n",
    "        table_path = os.path.join(OUTPUT_DIR, f\"{task_name}_single_{split}.csv\")\n",
    "        print(table.head())\n",
    "        table.to_csv(table_path, index=False)\n",
    "        all_rows.append({\"task\": task_name, \"setting\": \"single\", \"split\": split, **metrics})\n",
    "\n",
    "    print(f\"=== {task_name} | Merged-Table ===\")\n",
    "    merged = run_merged_table_experiment(task_name)\n",
    "    for split, metrics in merged.items():\n",
    "        # Print and save merged-table split\n",
    "        print(f\"Task: {task_name}, Setting: merged, Split: {split}\")\n",
    "        # Save the corresponding table\n",
    "        table = get_task(\"rel-avito\", task_name).get_table(split).df\n",
    "        table_path = os.path.join(OUTPUT_DIR, f\"{task_name}_merged_{split}.csv\")\n",
    "        print(table.head())\n",
    "        table.to_csv(table_path, index=False)\n",
    "        all_rows.append({\"task\": task_name, \"setting\": \"merged\", \"split\": split, **metrics})\n",
    "\n",
    "results_df = pd.DataFrame(all_rows).sort_values([\"task\", \"setting\", \"split\"]).reset_index(drop=True)\n",
    "results_csv_path = os.path.join(OUTPUT_DIR, \"all_results.csv\")\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "results_df\n"
   ],
   "id": "10771f71f0f2b242",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== user-visits | Single-Table ===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'df'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m task_name \u001B[38;5;129;01min\u001B[39;00m TASKS:\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m=== \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtask_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Single-Table ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m     single \u001B[38;5;241m=\u001B[39m \u001B[43mrun_single_table_experiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m split, metrics \u001B[38;5;129;01min\u001B[39;00m single\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m     12\u001B[0m         \u001B[38;5;66;03m# Print and save single-table split\u001B[39;00m\n\u001B[1;32m     13\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTask: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtask_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Setting: single, Split: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msplit\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[17], line 80\u001B[0m, in \u001B[0;36mrun_single_table_experiment\u001B[0;34m(task_name)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;66;03m# Primary metric via RelBench\u001B[39;00m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spec[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkind\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlink\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;66;03m# RelBench expects probabilities/scores\u001B[39;00m\n\u001B[0;32m---> 80\u001B[0m     val_primary  \u001B[38;5;241m=\u001B[39m \u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprob_val\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mprob_val\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43my_val_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mval\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m     test_primary \u001B[38;5;241m=\u001B[39m task\u001B[38;5;241m.\u001B[39mevaluate(prob_test \u001B[38;5;28;01mif\u001B[39;00m prob_test \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m y_test_pred)\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;66;03m# regression expects numeric predictions\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/TabPFN/lib/python3.10/site-packages/relbench/base/task_entity.py:56\u001B[0m, in \u001B[0;36mEntityTask.evaluate\u001B[0;34m(self, pred, target_table, metrics)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m target_table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     target_table \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_table(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m, mask_input_cols\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 56\u001B[0m target \u001B[38;5;241m=\u001B[39m \u001B[43mtarget_table\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdf\u001B[49m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_col]\u001B[38;5;241m.\u001B[39mto_numpy()\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pred) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(target):\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe length of pred and target must be the same (got \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(pred)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(target)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, respectively).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     61\u001B[0m     )\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'df'"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%\n",
    "def plot_metric(metric: str, title: Optional[str] = None):\n",
    "    sub = results_df[(results_df[\"split\"] == \"test\") & results_df[metric].notna()]\n",
    "    if sub.empty:\n",
    "        print(f\"No data to plot for {metric}\")\n",
    "        return\n",
    "    pivot = sub.pivot(index=\"task\", columns=\"setting\", values=metric)\n",
    "    ax = pivot.plot(kind=\"bar\", figsize=(9, 4))\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(title or metric)\n",
    "    ax.grid(True, axis=\"y\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Classification/Link relevant (AUROC approximated via classification_metrics; primary in 'primary_metric_relbench')\n",
    "plot_metric(\"roc_auc\", \"Test AUROC (classification/link)\")\n",
    "plot_metric(\"f1_macro\", \"Test F1 Macro (classification/link)\")\n",
    "\n",
    "# Regression\n",
    "plot_metric(\"mae\", \"Test MAE (regression)\")\n",
    "plot_metric(\"mse\", \"Test MSE (regression)\")\n",
    "\n",
    "# Timing\n",
    "plot_metric(\"fit_time\", \"Fit Time (s)\")\n",
    "plot_metric(\"predict_time\", \"Predict Time (s)\")\n"
   ],
   "id": "51d155a8f19d87a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
