{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T11:22:55.473176Z",
     "start_time": "2025-08-13T11:22:53.764118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Imports & device\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, Any, Optional\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "\n",
    "def get_device() -> str:\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    return \"cpu\"\n",
    "\n",
    "DEVICE = get_device()\n",
    "DEVICE\n"
   ],
   "id": "3be3333acfaf0187",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T11:22:55.484697Z",
     "start_time": "2025-08-13T11:22:55.481029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2) Utilities to extract pandas DataFrames from RelBench tables\n",
    "def table_to_pandas(table) -> pd.DataFrame:\n",
    "    # RelBench Table typically exposes .to_pandas(); fallbacks included\n",
    "    if hasattr(table, \"to_pandas\"):\n",
    "        return table.to_pandas()\n",
    "    if hasattr(table, \"df\"):\n",
    "        return table.df.copy()\n",
    "    if hasattr(table, \"data\"):\n",
    "        return pd.DataFrame(table.data)\n",
    "    raise RuntimeError(\"Unsupported RelBench Table object; cannot materialize to pandas.\")\n",
    "\n",
    "def get_db_table_names(db) -> list:\n",
    "    if hasattr(db, \"list_tables\"):\n",
    "        return list(db.list_tables())\n",
    "    if hasattr(db, \"tables\"):\n",
    "        return list(db.tables.keys())\n",
    "    if hasattr(db, \"get_table_names\"):\n",
    "        return list(db.get_table_names())\n",
    "    if hasattr(db, \"table_dict\"):\n",
    "        return list(db.table_dict.keys())\n",
    "    # Check for specifically named attributes like 'results', 'drivers', etc. that are common in F1 dataset\n",
    "    known_f1_tables = [\"drivers\", \"races\", \"results\", \"circuits\", \"standings\", \"constructors\",\n",
    "                     \"constructor_results\", \"constructor_standings\", \"qualifying\"]\n",
    "    if all(hasattr(db, table) for table in known_f1_tables[:4]):  # Check at least a few core tables exist\n",
    "        return known_f1_tables\n",
    "\n",
    "    raise RuntimeError(\"Unsupported RelBench Database object; cannot list tables. \"\n",
    "                      \"Please provide a custom table_names list to build_merged_view.\")\n",
    "\n",
    "def get_db_table_df(db, name: str) -> pd.DataFrame:\n",
    "    if hasattr(db, \"get_table\"):\n",
    "        t = db.get_table(name)\n",
    "        return table_to_pandas(t)\n",
    "    # common fallback attributes\n",
    "    if hasattr(db, \"tables\") and name in db.tables:\n",
    "        return table_to_pandas(db.tables[name])\n",
    "    if hasattr(db, \"table_dict\") and name in db.table_dict:\n",
    "        return table_to_pandas(db.table_dict[name])\n",
    "    if hasattr(db, name):\n",
    "        # Try to get the table as a direct attribute\n",
    "        return table_to_pandas(getattr(db, name))\n",
    "    raise KeyError(f\"Table '{name}' not found in Database.\")\n"
   ],
   "id": "8ba8df03217b3118",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T11:22:55.491039Z",
     "start_time": "2025-08-13T11:22:55.487728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3) Light preprocessing for TabPFN\n",
    "def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Convert datetime-like columns to numeric ordinals\n",
    "    for col in df.columns:\n",
    "        if np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            df[col] = pd.to_datetime(df[col]).map(pd.Timestamp.toordinal).astype(\"int64\")\n",
    "        # Some datasets encode timestamps as strings\n",
    "        elif df[col].dtype == \"object\":\n",
    "            # try parse; if fails, keep as string (TabPFN can handle categoricals)\n",
    "            try:\n",
    "                # Use more explicit parsing options to avoid warnings\n",
    "                if any(df[col].str.contains('-', na=False)):\n",
    "                    # Likely ISO format\n",
    "                    parsed = pd.to_datetime(df[col], format='%Y-%m-%d', errors=\"coerce\")\n",
    "                    if parsed.isna().mean() > 0.5:  # If most values failed, try more flexible format\n",
    "                        parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
    "                else:\n",
    "                    # Try common formats\n",
    "                    parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
    "                \n",
    "                # Only convert if parsing was successful\n",
    "                if parsed.isna().mean() < 0.5:  # More than half of values parsed successfully\n",
    "                    df[col] = parsed.map(pd.Timestamp.toordinal).astype(\"float64\")\n",
    "                    # Fill NaN values with median to avoid downstream issues\n",
    "                    df[col] = df[col].fillna(df[col].median())\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def split_X_y(df: pd.DataFrame, target_name: Optional[str] = None) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    if target_name is None:\n",
    "        # common target name candidates in RelBench tasks\n",
    "        for cand in [\"target\", \"label\", \"y\", \"dnf\", \"top3\", \"position\"]:\n",
    "            if cand in df.columns:\n",
    "                target_name = cand\n",
    "                break\n",
    "    if target_name is None or target_name not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"Could not infer target column. Columns: {list(df.columns)[:10]}...\"\n",
    "        )\n",
    "    y = df[target_name]\n",
    "    X = df.drop(columns=[target_name])\n",
    "    return X, y\n"
   ],
   "id": "63c430211efae6c1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T11:22:55.585333Z",
     "start_time": "2025-08-13T11:22:55.580758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4) Build SINGLE vs MERGED feature views\n",
    "\n",
    "def build_single_view(table) -> pd.DataFrame:\n",
    "    df = table_to_pandas(table)\n",
    "    # Limit to max 10000 samples\n",
    "    if len(df) > 10000:\n",
    "        print(f\"Limiting dataframe from {len(df)} to 10000 samples\")\n",
    "        df = df.sample(10000, random_state=42)\n",
    "    return preprocess_df(df)\n",
    "\n",
    "def discover_fk_columns(df: pd.DataFrame) -> list:\n",
    "    # Heuristic: columns ending with 'Id' that aren't the row's own index\n",
    "    return [c for c in df.columns if c.lower().endswith(\"id\")]\n",
    "\n",
    "def pick_right_table_name(db_table_names: list, fk_col: str) -> Optional[str]:\n",
    "    # Exact match (e.g., driverId -> 'drivers', circuitId -> 'circuits')\n",
    "    base = fk_col[:-2]  # drop 'Id'\n",
    "    candidates = [\n",
    "        base.lower(),\n",
    "        base.lower() + \"s\",  # plural\n",
    "        base.lower() + \"es\", # rare pluralization\n",
    "        base.lower().replace(\"y\", \"ies\") + \"\"  # e.g., 'qualify' -> 'qualifies' (heuristic)\n",
    "    ]\n",
    "    for name in db_table_names:\n",
    "        lname = name.lower()\n",
    "        if lname in candidates:\n",
    "            return name\n",
    "    # sometimes the fk column already matches the table primary key (e.g., raceId -> races)\n",
    "    for name in db_table_names:\n",
    "        if fk_col.lower().startswith(name.lower()[:-1]) and name.lower().endswith(\"s\"):\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "def primary_key_name_for_table(table_name: str) -> str:\n",
    "    # F1 schema uses '<table>Id' primary keys (plural to singular)\n",
    "    # E.g., 'drivers' -> 'driverId', 'races' -> 'raceId', 'results' -> 'resultId'\n",
    "    if table_name.lower().endswith(\"ies\"):   # constructor_standings -> not this pattern\n",
    "        singular = table_name[:-3] + \"y\"\n",
    "    elif table_name.lower().endswith(\"s\"):\n",
    "        singular = table_name[:-1]\n",
    "    else:\n",
    "        singular = table_name\n",
    "    return f\"{singular}Id\"\n",
    "\n",
    "def build_merged_view(base_table, db) -> pd.DataFrame:\n",
    "    base = table_to_pandas(base_table)\n",
    "\n",
    "    try:\n",
    "        db_names = get_db_table_names(db)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Warning: {e}. Continuing with base table only.\")\n",
    "        return preprocess_df(base.copy())\n",
    "\n",
    "    merged = base.copy()\n",
    "    fk_cols = discover_fk_columns(merged)\n",
    "\n",
    "    for fk in fk_cols:\n",
    "        right_name = pick_right_table_name(db_names, fk)\n",
    "        if right_name is None:\n",
    "            continue\n",
    "        try:\n",
    "            right_df = get_db_table_df(db, right_name)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        pk = primary_key_name_for_table(right_name)\n",
    "        if pk not in right_df.columns or fk not in merged.columns:\n",
    "            continue\n",
    "\n",
    "        # Perform left join and drop duplicate key from right\n",
    "        right_cols = [c for c in right_df.columns]\n",
    "        suffix = f\"__{right_name}\"\n",
    "        merged = merged.merge(\n",
    "            right_df,\n",
    "            how=\"left\",\n",
    "            left_on=fk,\n",
    "            right_on=pk,\n",
    "            suffixes=(\"\", suffix),\n",
    "        )\n",
    "        # Drop the right pk to avoid duplicates\n",
    "        if pk in merged.columns:\n",
    "            # Preserve the left fk; drop the right pk if it is duplicated\n",
    "            cols_to_drop = [c for c in merged.columns if c == pk and c != fk]\n",
    "            if cols_to_drop:\n",
    "                merged = merged.drop(columns=cols_to_drop)\n",
    "\n",
    "    merged = preprocess_df(merged)\n",
    "    return merged\n"
   ],
   "id": "6cea263847bcdf24",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T11:22:55.652206Z",
     "start_time": "2025-08-13T11:22:55.648165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation & timing utilities for ML models\n",
    "# Add a standardization function to help avoid numerical issues\n",
    "def standardize_features(X_train, X_test=None):\n",
    "    \"\"\"Standardize features to help prevent numerical issues.\n",
    "\n",
    "    Returns standardized X_train and optionally X_test, along with scaler\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "    # Use RobustScaler which is less affected by outliers\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train),\n",
    "        columns=X_train.columns,\n",
    "        index=X_train.index\n",
    "    )\n",
    "\n",
    "    if X_test is not None:\n",
    "        X_test_scaled = pd.DataFrame(\n",
    "            scaler.transform(X_test),\n",
    "            columns=X_test.columns,\n",
    "            index=X_test.index\n",
    "        )\n",
    "        return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "    return X_train_scaled, scaler\n",
    "\n",
    "def fit_predict_with_timing(estimator, X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Fit an estimator and time both fitting and prediction operations.\n",
    "    \n",
    "    Returns:\n",
    "    - predictions (point predictions)\n",
    "    - probabilities (for classification only, otherwise None)\n",
    "    - fit_time_s (in seconds)\n",
    "    - predict_time_s (in seconds)\n",
    "    \"\"\"\n",
    "    # Handle NaN values that might cause warnings\n",
    "    X_train = X_train.fillna(X_train.median())\n",
    "    X_test = X_test.fillna(X_train.median())\n",
    "    \n",
    "    # Apply standardization to help prevent numerical issues\n",
    "    X_train_scaled, X_test_scaled, _ = standardize_features(X_train, X_test)\n",
    "    \n",
    "    # Fit and time\n",
    "    t0 = time.time()\n",
    "    estimator.fit(X_train_scaled, y_train)\n",
    "    fit_time_s = time.time() - t0\n",
    "\n",
    "    # Predict and time\n",
    "    t0 = time.time()\n",
    "    if hasattr(estimator, \"predict_proba\"):\n",
    "        proba = estimator.predict_proba(X_test_scaled)\n",
    "        preds = np.argmax(proba, axis=1)\n",
    "        # Map integer predictions back to original classes\n",
    "        if hasattr(estimator, \"classes_\"):\n",
    "            preds = estimator.classes_[preds]\n",
    "    else:\n",
    "        proba = None\n",
    "        preds = estimator.predict(X_test_scaled)\n",
    "    predict_time_s = time.time() - t0\n",
    "\n",
    "    return preds, proba, fit_time_s, predict_time_s\n",
    "\n",
    "def eval_classification(y_true, y_pred, y_proba=None) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate classification predictions with common metrics.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    metrics[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    try:\n",
    "        metrics[\"f1_macro\"] = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    except:\n",
    "        # Handle edge cases where f1 fails (e.g., single class in test set)\n",
    "        metrics[\"f1_macro\"] = np.nan\n",
    "\n",
    "    # For binary classification, add AUC if probabilities provided\n",
    "    if y_proba is not None and len(np.unique(y_true)) <= 2:\n",
    "        try:\n",
    "            # Get probability of positive class\n",
    "            if y_proba.shape[1] == 2:\n",
    "                y_score = y_proba[:, 1]  # Use second column for positive class\n",
    "                metrics[\"roc_auc\"] = roc_auc_score(y_true, y_score)\n",
    "        except:\n",
    "            # Handle edge cases where ROC AUC fails (e.g., single class in test set)\n",
    "            metrics[\"roc_auc\"] = np.nan\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def eval_regression(y_true, y_pred) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate regression predictions with common metrics.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    metrics[\"mae\"] = mean_absolute_error(y_true, y_pred)\n",
    "    metrics[\"mse\"] = mean_squared_error(y_true, y_pred)\n",
    "    # R2 score (coefficient of determination)\n",
    "    # metrics[\"r2\"] = r2_score(y_true, y_pred)\n",
    "    return metrics\n",
    "\n"
   ],
   "id": "429f25813401f900",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-13T11:22:55.670180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5) Run single vs merged view experiments\n",
    "\n",
    "task_name = \"driver-dnf\"\n",
    "db = get_dataset(\"rel-f1\").get_db()  # FIX: Use \"rel-f1\" instead of \"f1\"\n",
    "task = get_task(\"rel-f1\", task_name)  # FIX: Use \"rel-f1\" instead of just task_name\n",
    "\n",
    "# Target column inferred from task metadata\n",
    "target_name = None\n",
    "if hasattr(task, 'target'):\n",
    "    target_name = task.target\n",
    "elif hasattr(task, 'target_col'):\n",
    "    target_name = task.target_col\n",
    "problem_type = \"classification\"  # Default to classification for driver-dnf\n",
    "\n",
    "# Tables for splits\n",
    "train_table = task.get_table(\"train\")\n",
    "val_table   = task.get_table(\"val\")\n",
    "test_table  = task.get_table(\"test\", mask_input_cols=False)\n",
    "\n",
    "# SINGLE view\n",
    "df_tr_single = build_single_view(train_table)\n",
    "df_va_single = build_single_view(val_table)\n",
    "df_te_single = build_single_view(test_table)\n",
    "\n",
    "Xtr_s, ytr_s = split_X_y(df_tr_single, target_name)\n",
    "Xva_s, yva_s = split_X_y(df_va_single, target_name)\n",
    "Xte_s, yte_s = split_X_y(df_te_single, target_name)\n",
    "\n",
    "# MERGED view\n",
    "df_tr_merged = build_merged_view(train_table, db)\n",
    "df_va_merged = build_merged_view(val_table, db)\n",
    "df_te_merged = build_merged_view(test_table, db)\n",
    "\n",
    "Xtr_m, ytr_m = split_X_y(df_tr_merged, target_name)\n",
    "Xva_m, yva_m = split_X_y(df_va_merged, target_name)\n",
    "Xte_m, yte_m = split_X_y(df_te_merged, target_name)\n",
    "\n",
    "# Choose estimator\n",
    "if problem_type == \"classification\":\n",
    "    est_single = TabPFNClassifier(device=DEVICE)\n",
    "    est_merged = TabPFNClassifier(device=DEVICE)\n",
    "elif problem_type == \"regression\":\n",
    "    est_single = TabPFNRegressor(device=DEVICE)\n",
    "    est_merged = TabPFNRegressor(device=DEVICE)\n",
    "else:\n",
    "    raise ValueError(\"problem_type must be 'classification' or 'regression'.\")\n",
    "\n",
    "# Fit on train, evaluate on test\n",
    "yhat_s, proba_s, fit_s, pred_s = fit_predict_with_timing(est_single, Xtr_s, ytr_s, Xte_s)\n",
    "yhat_m, proba_m, fit_m, pred_m = fit_predict_with_timing(est_merged, Xtr_m, ytr_m, Xte_m)\n",
    "\n",
    "# Metrics\n",
    "if problem_type == \"classification\":\n",
    "    metrics_single = eval_classification(yte_s, yhat_s, proba_s)\n",
    "    metrics_merged = eval_classification(yte_m, yhat_m, proba_m)\n",
    "else:\n",
    "    metrics_single = eval_regression(yte_s, yhat_s)\n",
    "    metrics_merged = eval_regression(yte_m, yhat_m)\n",
    "\n",
    "# Add timing & metadata\n",
    "metrics_single.update({\"fit_time_s\": fit_s, \"predict_time_s\": pred_s, \"view\": \"single\", \"task\": task_name})\n",
    "metrics_merged.update({\"fit_time_s\": fit_m, \"predict_time_s\": pred_m, \"view\": \"merged\", \"task\": task_name})\n",
    "\n",
    "results_single = pd.DataFrame([metrics_single])\n",
    "results_merged = pd.DataFrame([metrics_merged])\n",
    "\n",
    "results_single, results_merged\n"
   ],
   "id": "21f1a0649e9f1c52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /Users/michaelflppv/Library/Caches/relbench/rel-f1/db...\n",
      "Done in 0.04 seconds.\n",
      "Limiting dataframe from 11411 to 10000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/var/folders/yl/k2ptpp6s2ms3m1kvq65cr07m0000gn/T/ipykernel_7072/2416068232.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(df[col], infer_datetime_format=True, errors=\"coerce\")\n",
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 6) Aggregate results for all tasks\n",
    "def run_task(task_name: str, problem_type: str) -> pd.DataFrame:\n",
    "    db = get_dataset(\"rel-f1\").get_db()\n",
    "    task = get_task(\"rel-f1\", task_name)\n",
    "\n",
    "    # Target column inferred from task metadata\n",
    "    if hasattr(task, 'target'):\n",
    "        target_name = task.target\n",
    "    elif hasattr(task, 'target_col'):\n",
    "        target_name = task.target_col\n",
    "    else:\n",
    "        # If we can't find the target attribute, infer it from the test table\n",
    "        test_table = task.get_table(\"test\", mask_input_cols=False)\n",
    "        test_df = table_to_pandas(test_table)\n",
    "        # Look for typical target column names\n",
    "        possible_targets = ['target', 'label', 'y', 'did_not_finish', 'top3', 'position']\n",
    "        for col in possible_targets:\n",
    "            if col in test_df.columns:\n",
    "                target_name = col\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"Could not determine target column for task {task_name}\")\n",
    "    # Tables for splits\n",
    "    train_table = task.get_table(\"train\")\n",
    "    val_table   = task.get_table(\"val\")\n",
    "    test_table  = task.get_table(\"test\", mask_input_cols=False)\n",
    "\n",
    "    # Set maximum number of samples to 10000 for each table\n",
    "    MAX_SAMPLES = 10000\n",
    "\n",
    "    # SINGLE view\n",
    "    df_tr_single = build_single_view(train_table)\n",
    "    if len(df_tr_single) > MAX_SAMPLES:\n",
    "        df_tr_single = df_tr_single.sample(MAX_SAMPLES, random_state=42)\n",
    "\n",
    "    df_va_single = build_single_view(val_table)\n",
    "    if len(df_va_single) > MAX_SAMPLES:\n",
    "        df_va_single = df_va_single.sample(MAX_SAMPLES, random_state=42)\n",
    "\n",
    "    df_te_single = build_single_view(test_table)\n",
    "    if len(df_te_single) > MAX_SAMPLES:\n",
    "        df_te_single = df_te_single.sample(MAX_SAMPLES, random_state=42)\n",
    "\n",
    "    Xtr_s, ytr_s = split_X_y(df_tr_single, target_name)\n",
    "    Xva_s, yva_s = split_X_y(df_va_single, target_name)\n",
    "    Xte_s, yte_s = split_X_y(df_te_single, target_name)\n",
    "\n",
    "    # MERGED view\n",
    "    df_tr_merged = build_merged_view(train_table, db)\n",
    "    if len(df_tr_merged) > MAX_SAMPLES:\n",
    "        df_tr_merged = df_tr_merged.sample(MAX_SAMPLES, random_state=42)\n",
    "\n",
    "    df_va_merged = build_merged_view(val_table, db)\n",
    "    if len(df_va_merged) > MAX_SAMPLES:\n",
    "        df_va_merged = df_va_merged.sample(MAX_SAMPLES, random_state=42)\n",
    "\n",
    "    df_te_merged = build_merged_view(test_table, db)\n",
    "    if len(df_te_merged) > MAX_SAMPLES:\n",
    "        df_te_merged = df_te_merged.sample(MAX_SAMPLES, random_state=42)\n",
    "\n",
    "    Xtr_m, ytr_m = split_X_y(df_tr_merged, target_name)\n",
    "    Xva_m, yva_m = split_X_y(df_va_merged, target_name)\n",
    "    Xte_m, yte_m = split_X_y(df_te_merged, target_name)\n",
    "\n",
    "    # Choose estimator\n",
    "    if problem_type == \"classification\":\n",
    "        est_single = TabPFNClassifier(device=DEVICE)\n",
    "        est_merged = TabPFNClassifier(device=DEVICE)\n",
    "    elif problem_type == \"regression\":\n",
    "        est_single = TabPFNRegressor(device=DEVICE)\n",
    "        est_merged = TabPFNRegressor(device=DEVICE)\n",
    "    else:\n",
    "        raise ValueError(\"problem_type must be 'classification' or 'regression'.\")\n",
    "\n",
    "    # Fit on train, evaluate on test\n",
    "    yhat_s, proba_s, fit_s, pred_s = fit_predict_with_timing(est_single, Xtr_s, ytr_s, Xte_s)\n",
    "    yhat_m, proba_m, fit_m, pred_m = fit_predict_with_timing(est_merged, Xtr_m, ytr_m, Xte_m)\n",
    "\n",
    "    # Metrics\n",
    "    if problem_type == \"classification\":\n",
    "        metrics_single = eval_classification(yte_s, yhat_s, proba_s)\n",
    "        metrics_merged = eval_classification(yte_m, yhat_m, proba_m)\n",
    "    else:\n",
    "        metrics_single = eval_regression(yte_s, yhat_s)\n",
    "        metrics_merged = eval_regression(yte_m, yhat_m)\n",
    "\n",
    "    # Add timing & metadata\n",
    "    metrics_single.update({\"fit_time_s\": fit_s, \"predict_time_s\": pred_s, \"view\": \"single\", \"task\": task_name})\n",
    "    metrics_merged.update({\"fit_time_s\": fit_m, \"predict_time_s\": pred_m, \"view\": \"merged\", \"task\": task_name})\n",
    "\n",
    "    return pd.DataFrame([metrics_single, metrics_merged])\n",
    "\n",
    "\n",
    "results_all = []\n",
    "# Driver DNF (classification)\n",
    "results_all.append(run_task(\"driver-dnf\", problem_type=\"classification\"))\n",
    "# Driver Top-3 (classification)\n",
    "results_all.append(run_task(\"driver-top3\", problem_type=\"classification\"))\n",
    "# Driver Position (regression)\n",
    "results_all.append(run_task(\"driver-position\", problem_type=\"regression\"))\n",
    "\n",
    "results = pd.concat(results_all, ignore_index=True)\n",
    "results\n"
   ],
   "id": "9f87d89b57e89aa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 7) Pretty summary per task\n",
    "def tidy(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [\"task\", \"view\", \"accuracy\", \"roc_auc\", \"f1_macro\", \"mae\", \"mse\", \"fit_time_s\", \"predict_time_s\"]\n",
    "    return df[[c for c in cols if c in df.columns]].sort_values([\"task\", \"view\"])\n",
    "\n",
    "summary = tidy(results)\n",
    "summary\n"
   ],
   "id": "a60319048044d572"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 8) Optional: quick comparison deltas (merged - single) per task\n",
    "def compare_merged_vs_single(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for task in df[\"task\"].unique():\n",
    "        sub = df[df.task == task].set_index(\"view\")\n",
    "        if {\"single\", \"merged\"} <= set(sub.index):\n",
    "            a = sub.loc[\"single\"]\n",
    "            b = sub.loc[\"merged\"]\n",
    "            delta = (b - a).to_dict()\n",
    "            delta[\"task\"] = task\n",
    "            rows.append(delta)\n",
    "    out = pd.DataFrame(rows)\n",
    "    return out[[c for c in [\"task\", \"accuracy\", \"roc_auc\", \"f1_macro\", \"mae\", \"mse\", \"fit_time_s\", \"predict_time_s\"] if c in out.columns]]\n",
    "\n",
    "delta = compare_merged_vs_single(summary)\n",
    "delta\n"
   ],
   "id": "dae602792585fef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 9) (Optional) Save results to CSV\n",
    "summary_path = \"tabpfn_rel_f1_summary.csv\"\n",
    "delta_path = \"tabpfn_rel_f1_delta.csv\"\n",
    "summary.to_csv(summary_path, index=False)\n",
    "delta.to_csv(delta_path, index=False)\n",
    "summary_path, delta_path\n"
   ],
   "id": "41fc34b96592b7cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
