{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **TabPFN Relational Benchmark**\n",
    "This notebook benchmarks the performance of TabPFN models on datasets from RelBench in two scenarios:\n",
    "1. **Single Table** – Using only the target entity table.\n",
    "2. **Merged Table** – Using a naively denormalized table obtained by joining related tables.\n",
    "\n",
    "It automates dataset loading, preprocessing (including date feature engineering), vectorization, model training, prediction, and evaluation for all compatible tasks within a chosen RelBench dataset. The results allow comparing model performance between single-table and merged-table configurations.\n"
   ],
   "id": "7ab5a9bf5466de0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "id": "ac93e97c56d4b905"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:32:03.128350Z",
     "start_time": "2025-08-19T11:31:57.129262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import time\n",
    "import inspect\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- PyTorch and PyTorch Geometric ---\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "\n",
    "# --- Skrub and Sentence Transformers ---\n",
    "from skrub import TableVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- RelBench ---\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task, get_task_names\n",
    "from relbench.base import TaskType\n",
    "import relbench.metrics\n",
    "from relbench.modeling.utils import get_stype_proposal, to_unix_time\n",
    "from relbench.modeling.graph import make_pkey_fkey_graph\n",
    "\n",
    "# --- TabPFN ---\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n"
   ],
   "id": "57624fe02c838842",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set Global Configuration",
   "id": "7154198e21fe2a01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:32:03.167268Z",
     "start_time": "2025-08-19T11:32:03.134552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Device preference\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Define global dataset variable (any available dataset from relbench.datasets)\n",
    "# \"rel-f1\" is the default, but can be overridden by setting DATASET variable\n",
    "DATASET = \"rel-stack\"\n",
    "\n",
    "# Global configuration variables with defaults\n",
    "SEED   = globals().get(\"SEED\", 42)\n",
    "N_ESTIMATORS = globals().get(\"N_ESTIMATORS\", 16) # number of TabPFN estimators\n",
    "TABPFN_MAX = globals().get(\"TABPFN_MAX\", 1000)  # hard ceiling for TabPFN"
   ],
   "id": "c5959e09ecd4c20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Notebook Configuration and Dataset Selection\n",
    "\n",
    "\n",
    "Sets the dataset name (`DATASET`) and download flag (`DOWNLOAD`), then discovers all available tasks for the selected dataset using RelBench’s APIs. Filters tasks to only those compatible with TabPFN (classification and regression).\n"
   ],
   "id": "a86eaa89573fdf63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:32:03.397028Z",
     "start_time": "2025-08-19T11:32:03.366282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reuse existing config if present, otherwise set defaults\n",
    "DATASET = globals().get(\"DATASET\", \"rel-f1\")\n",
    "DOWNLOAD = globals().get(\"DOWNLOAD\", True)\n",
    "\n",
    "# Discover tasks and keep only entity-level cls/reg tasks TabPFN can handle\n",
    "def _is_tabpfn_friendly(task):\n",
    "    return task.task_type in (\n",
    "        TaskType.BINARY_CLASSIFICATION,\n",
    "        TaskType.MULTICLASS_CLASSIFICATION,\n",
    "        TaskType.MULTILABEL_CLASSIFICATION,\n",
    "        TaskType.REGRESSION,\n",
    "    )\n",
    "\n",
    "_all = get_task_names(DATASET)  # shown in tutorials\n",
    "TASKS = []\n",
    "for tname in _all:\n",
    "    try:\n",
    "        t = get_task(DATASET, tname, download=DOWNLOAD)\n",
    "        if _is_tabpfn_friendly(t):\n",
    "            TASKS.append(tname)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {tname}: {e!s}\")\n",
    "\n",
    "print(f\"{DATASET}: {len(TASKS)} TabPFN-friendly tasks -> {TASKS}\")\n"
   ],
   "id": "d94075cf88bf6806",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel-stack: 3 TabPFN-friendly tasks -> ['user-engagement', 'post-votes', 'user-badge']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Patch RelBench Metrics (Optional)",
   "id": "affc15acb6a0f8aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:32:03.412926Z",
     "start_time": "2025-08-19T11:32:03.410686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Patch relbench.metrics.skm.mean_squared_error to local mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "relbench.metrics.skm.mean_squared_error = mean_squared_error\n",
    "\n",
    "def patched_rmse(true, pred):\n",
    "    if \"squared\" in inspect.signature(mean_squared_error).parameters:\n",
    "        return mean_squared_error(true, pred, squared=False)\n",
    "    else:\n",
    "        return np.sqrt(mean_squared_error(true, pred))\n",
    "\n",
    "relbench.metrics.rmse = patched_rmse"
   ],
   "id": "46fcd0d47afb30c1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Text Embedding Configuration",
   "id": "5629815fd3214b7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:32:03.434427Z",
     "start_time": "2025-08-19T11:32:03.427663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a text embedding class using GloVe embeddings from Sentence Transformers.\n",
    "class GloveTextEmbedding:\n",
    "    def __init__(self, device: Optional[torch.device] = None):\n",
    "        self.model = SentenceTransformer(\n",
    "            \"sentence-transformers/average_word_embeddings_glove.6B.300d\",\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def __call__(self, sentences: List[str]) -> Tensor:\n",
    "        return torch.from_numpy(self.model.encode(sentences))"
   ],
   "id": "78e9b5ef45024fb3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Functions for Dataset Loading and Table Processing\n",
   "id": "2d748a1a5dea1d28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Date Feature Engineering\n",
    "\n",
    "Processes all tables in the dataset to detect and parse date columns, replacing missing values and generating engineered date-related features (e.g., year, month, weekday, cyclical encodings).\n"
   ],
   "id": "76f494f170e324f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:32:10.706704Z",
     "start_time": "2025-08-19T11:32:03.445997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset and its database\n",
    "dataset = get_dataset(DATASET)\n",
    "db = dataset.get_db()\n",
    "\n",
    "# Helper function to convert any table-like object to a pandas DataFrame\n",
    "def to_pandas(table):\n",
    "    if hasattr(table, \"to_pandas\"):\n",
    "        return table.to_pandas()\n",
    "    if hasattr(table, \"df\"):\n",
    "        return table.df\n",
    "    raise ValueError(\"Unknown table type\")\n",
    "\n",
    "# Convert all tables to pandas DataFrames\n",
    "tables = {name: to_pandas(tbl) for name, tbl in db.table_dict.items()}\n",
    "\n",
    "# Date feature engineering\n",
    "# This will modify the tables in-place, adding new date-related features.\n",
    "for name, df in tables.items():\n",
    "    # Drop duplicate columns and force a copy\n",
    "    df = df.loc[:, ~df.columns.duplicated()].copy()\n",
    "    date_cols = [col for col in df.columns if \"date\" in col.lower()]\n",
    "    if not date_cols:\n",
    "        tables[name] = df\n",
    "        continue\n",
    "\n",
    "    # Compute cleaned dates and all features in memory\n",
    "    dt_clean = {}\n",
    "    feats = {}\n",
    "    for col in date_cols:\n",
    "        dt = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "        dt_filled = dt.fillna(dt.min())\n",
    "        dt_clean[col] = dt_filled.dt.tz_localize(None)\n",
    "        feats.update({\n",
    "            f\"{col}_year\":            dt_filled.dt.year,\n",
    "            f\"{col}_month\":           dt_filled.dt.month,\n",
    "            f\"{col}_day\":             dt_filled.dt.day,\n",
    "            f\"{col}_weekday\":         dt_filled.dt.weekday,\n",
    "            f\"{col}_quarter\":         dt_filled.dt.quarter,\n",
    "            f\"{col}_is_month_start\":  dt_filled.dt.is_month_start.astype(int),\n",
    "            f\"{col}_is_month_end\":    dt_filled.dt.is_month_end.astype(int),\n",
    "            f\"{col}_is_weekend\":      (dt_filled.dt.weekday >= 5).astype(int),\n",
    "            f\"{col}_month_sin\":       np.sin(2 * np.pi * dt_filled.dt.month / 12),\n",
    "            f\"{col}_month_cos\":       np.cos(2 * np.pi * dt_filled.dt.month / 12),\n",
    "            f\"{col}_weekday_sin\":     np.sin(2 * np.pi * dt_filled.dt.weekday / 7),\n",
    "            f\"{col}_weekday_cos\":     np.cos(2 * np.pi * dt_filled.dt.weekday / 7),\n",
    "            f\"{col}_elapsed_days\":    (dt_filled - dt_filled.min()).dt.days,\n",
    "        })\n",
    "\n",
    "    # Drop original date columns and concatenate new ones in a single op\n",
    "    df = df.drop(columns=date_cols)\n",
    "    new_cols_df = pd.DataFrame({**dt_clean, **feats}, index=df.index)\n",
    "    df = pd.concat([df, new_cols_df], axis=1)\n",
    "\n",
    "    tables[name] = df\n",
    "\n",
    "# Update the database with modified DataFrames\n",
    "for name, df in tables.items():\n",
    "    db.table_dict[name].df = df\n",
    "\n",
    "# Patch infer_series_stype to handle \"truth value of a Series\" errors without recursion\n",
    "import torch_frame.utils.infer_stype as ts\n",
    "\n",
    "# Wrap only once by marking the safe wrapper\n",
    "if not getattr(ts.infer_series_stype, \"__safe_wrapped__\", False):\n",
    "    original_infer = ts.infer_series_stype\n",
    "\n",
    "    def safe_infer_series_stype(ser):\n",
    "        try:\n",
    "            return original_infer(ser)\n",
    "        except ValueError as e:\n",
    "            if \"truth value of a Series\" in str(e):\n",
    "                return original_infer(ser.dropna())\n",
    "            raise\n",
    "\n",
    "    safe_infer_series_stype.__safe_wrapped__ = True\n",
    "    ts.infer_series_stype = safe_infer_series_stype"
   ],
   "id": "d37c8ff4b032ae52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /Users/michaelflppv/Library/Caches/relbench/rel-stack/db...\n",
      "Done in 2.44 seconds.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Flatten One-Hop Foreign Key Relationships\n",
    "\n",
    "This function takes a heterogeneous data structure (tensor frames) and a database object, then flattens the one-hop foreign key relationships starting from a specified target table. It performs left joins to denormalize the data into a single pandas DataFrame.\n",
    "- Takes a heterogeneous data object (`hetero_data`), a database object (`db`), a target table name, and a list of cutoff times.\n",
    "- Copies the target table as a pandas DataFrame to use as the base for denormalization.\n",
    "- Iterates over each row in the base table and its corresponding cutoff time.\n",
    "- For each row, checks all edge types in the heterogeneous data where the source is the target table.\n",
    "- For each such edge, finds neighbor rows (from the destination table) that are connected and, if time is present, only those before the cutoff time.\n",
    "- If no neighbors are found, appends a row with NaN values for the neighbor columns.\n",
    "- If neighbors exist, fetches their features, creates a DataFrame, and merges it with the base row (repeating the base row as needed).\n",
    "- Collects all resulting DataFrames and concatenates them into a single denormalized DataFrame, which is returned."
   ],
   "id": "8a3ef84da239d251"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:32:10.734905Z",
     "start_time": "2025-08-19T11:32:10.730631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flatten_one_hop(hetero_data, db, target_table, cutoff_times):\n",
    "    # base table as pandas\n",
    "    base_df = db.table_dict[target_table].df.copy()\n",
    "    out_frames = []\n",
    "\n",
    "    # iterate over each row and its cutoff\n",
    "    for src_idx, cutoff in enumerate(cutoff_times):\n",
    "        # build a base row for the current source index\n",
    "        base_row = base_df.iloc[[src_idx]].reset_index(drop=True)\n",
    "\n",
    "        # examine each edge type (src → dst). If src is the target table,\n",
    "        # fetch neighbors (dst) and their features before the cutoff time.\n",
    "        for (src, rel, dst), edge_store in hetero_data.edge_items():\n",
    "            if src != target_table:\n",
    "                continue\n",
    "\n",
    "            # get edge_index and optional time array\n",
    "            row_idx, col_idx = edge_store.edge_index\n",
    "            mask = (row_idx == src_idx)\n",
    "            if hasattr(edge_store, \"time\"):\n",
    "                mask &= (edge_store.time <= cutoff)\n",
    "\n",
    "            nbr_ids = col_idx[mask]\n",
    "            if nbr_ids.numel() == 0:\n",
    "                # no neighbor before cutoff: emit base row with NaN columns\n",
    "                # build a placeholder df with correct columns\n",
    "                nbr_cols = [f\"{dst}__{k}\" for k in hetero_data[dst].keys()]\n",
    "                placeholder = pd.DataFrame([{c: pd.NA for c in nbr_cols}])\n",
    "                out_frames.append(pd.concat([base_row, placeholder], axis=1))\n",
    "                continue\n",
    "\n",
    "            # fetch neighbor node features\n",
    "            nbr_store = hetero_data[dst]\n",
    "            nbr_dict = {}\n",
    "            for feat_name, tensor in nbr_store.items():\n",
    "                if isinstance(tensor, torch.Tensor) and tensor.dim() == 1:\n",
    "                    nbr_dict[f\"{dst}__{feat_name}\"] = tensor[nbr_ids].tolist()\n",
    "            nbr_df = pd.DataFrame(nbr_dict)\n",
    "\n",
    "            # repeat base_row for each neighbor and concat\n",
    "            repeated = pd.concat([base_row] * len(nbr_df), ignore_index=True)\n",
    "            merged = pd.concat([repeated.reset_index(drop=True),\n",
    "                                nbr_df.reset_index(drop=True)], axis=1)\n",
    "            out_frames.append(merged)\n",
    "\n",
    "    return pd.concat(out_frames, ignore_index=True)"
   ],
   "id": "de77c03733f542aa",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fetch Dataset Splits\n",
    "\n",
    "Utility functions to load a task’s splits (`train`, `val`, `test`), convert them to pandas DataFrames, and extract features (`X`) and targets (`y`). Includes functions to build:\n",
    "- **Single-table frames** directly from the target entity table.\n",
    "- **Merged-table frames** by performing a one-hop foreign key → primary key join to denormalize data.\n"
   ],
   "id": "1a35d633effdd263"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:32:10.754177Z",
     "start_time": "2025-08-19T11:32:10.749223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fetches the task and its splits from the dataset, returning a task object and a dictionary of DataFrames for each split.\n",
    "def fetch_splits(dataset_name: str, task_name: str, download: bool = True):\n",
    "    task = get_task(dataset_name, task_name, download=download)\n",
    "    # keep original columns (tutorial shows mask_input_cols flag)\n",
    "    splits = {\n",
    "        split: task.get_table(split, mask_input_cols=False)\n",
    "        for split in (\"train\", \"val\", \"test\")\n",
    "    }\n",
    "    return task, splits\n",
    "\n",
    "# Converts a DataFrame to feature matrix (X) and target vector (y) based on the specified target column.\n",
    "def to_Xy(df: pd.DataFrame, target_col: str):\n",
    "    y = df[target_col].to_numpy()\n",
    "    X = df.drop(columns=[target_col])\n",
    "    return X, y\n",
    "\n",
    "# Infers the primary key column from a table object and its DataFrame. It checks for common attribute names first, then falls back to finding the first unique column.\n",
    "def _infer_pk(table_obj, df: pd.DataFrame):\n",
    "    # best-effort: check common attribute names first, then infer by uniqueness\n",
    "    for attr in (\"primary_key_col\", \"pkey\", \"pk\", \"primary_key\", \"id_col\"):\n",
    "        if hasattr(table_obj, attr):\n",
    "            cand = getattr(table_obj, attr)\n",
    "            if isinstance(cand, str) and cand in df.columns:\n",
    "                return cand\n",
    "    # fallback: take the first unique column if exists\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            if df[c].is_unique:\n",
    "                return c\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# Builds frames for each split of a task, either using the original target table (single-table mode) or by denormalizing the data (merged-table mode). Returns a dictionary with split names as keys and tuples of (X, y, original_df) as values.\n",
    "def build_single_table_frames(task, splits):\n",
    "    frames = {}\n",
    "    for split, table in splits.items():\n",
    "        df = table.df.copy()\n",
    "        # IMPORTANT: do not touch column masking/order; just drop the target to form X\n",
    "        X, y = to_Xy(df, task.target_col)\n",
    "        frames[split] = (X, y, df)  # keep original df for evaluation alignment\n",
    "    return frames\n",
    "\n",
    "# Builds frames for each split of a dataset in merged-table mode by performing a one-hop foreign key → primary key join. It flattens the data structure, extracts features and targets, and returns a dictionary with split names as keys and tuples of (X, y, merged_df) as values.\n",
    "def build_merged_table_frames(hetero_data, db, task, splits):\n",
    "    out = {}\n",
    "    tbl = task.entity_table\n",
    "    tcol = db.table_dict[tbl].time_col\n",
    "\n",
    "    for name, table in splits.items():\n",
    "        # get pandas DataFrame from Table object\n",
    "        df = table.df if hasattr(table, \"df\") else to_pandas(table)\n",
    "        # compute cutoff timestamps\n",
    "        cutoff = to_unix_time(df[tcol]) if tcol else [None] * len(df)\n",
    "        # flatten via one-hop join\n",
    "        merged = flatten_one_hop(hetero_data, db, tbl, cutoff)\n",
    "        # split into features/target\n",
    "        X, y = to_Xy(merged, task.target_col)\n",
    "        out[name] = (X, y, merged)\n",
    "    return out\n"
   ],
   "id": "1cfbaca7c6d3b12c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vectorization Wrapper (Version-Safe)\n",
    "\n",
    "Initializes a `TableVectorizer` with only supported arguments for the installed `skrub` or `dirty_cat` version, ensuring compatibility. Transforms `train`, `val`, and `test` splits into numerical feature matrices, converting them to dense format if necessary.\n"
   ],
   "id": "f7cf59b1861625f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Helper Functions for Vectorization",
   "id": "ff99875cd0db904d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:32:10.770782Z",
     "start_time": "2025-08-19T11:32:10.767773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This function creates a `TableVectorizer` instance with version-specific arguments.\n",
    "def _make_table_vectorizer():\n",
    "    sig = inspect.signature(TableVectorizer.__init__)\n",
    "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
    "\n",
    "    tv_kwargs = {}\n",
    "\n",
    "    # Only set kwargs that actually exist in the installed version\n",
    "    if \"cardinality_threshold\" in allowed:\n",
    "        tv_kwargs[\"cardinality_threshold\"] = globals().get(\"CARDINALITY_THRESHOLD\", 1000)\n",
    "\n",
    "    # Some versions expose this; others don't, guard it\n",
    "    if \"high_cardinality_transformer\" in allowed:\n",
    "        tv_kwargs[\"high_cardinality_transformer\"] = globals().get(\"HIGH_CARD_TRANSFORMER\", \"hashing\")\n",
    "\n",
    "    # Optional knobs if you define them globally and the version supports them\n",
    "    if \"text_separator\" in allowed and \"TEXT_SEPARATOR\" in globals():\n",
    "        tv_kwargs[\"text_separator\"] = globals()[\"TEXT_SEPARATOR\"]\n",
    "    if \"numerical_transformer\" in allowed and \"NUMERICAL_TRANSFORMER\" in globals():\n",
    "        tv_kwargs[\"numerical_transformer\"] = globals()[\"NUMERICAL_TRANSFORMER\"]\n",
    "    if \"categorical_transformer\" in allowed and \"CATEGORICAL_TRANSFORMER\" in globals():\n",
    "        tv_kwargs[\"categorical_transformer\"] = globals()[\"CATEGORICAL_TRANSFORMER\"]\n",
    "\n",
    "    return TableVectorizer(**tv_kwargs)\n",
    "\n",
    "# Converts a sparse matrix to a dense NumPy array, handling cases where the input is already dense or does not support `.toarray()`.\n",
    "def _to_dense(X):\n",
    "    try:\n",
    "        # scipy sparse matrices have .toarray()\n",
    "        return X.toarray() if hasattr(X, \"toarray\") else X\n",
    "    except Exception:\n",
    "        return X\n",
    "\n",
    "# Vectorizes the training, validation, and test splits using a `TableVectorizer`. It initializes the vectorizer, fits it on the training data, and transforms all splits into dense NumPy arrays. Returns the vectorizer and the transformed matrices.\n",
    "def vectorize_splits(X_train, X_val, X_test):\n",
    "    # Fit only on training data to prevent data leakage\n",
    "    tv = _make_table_vectorizer()\n",
    "    Xt = _to_dense(tv.fit_transform(X_train))\n",
    "    Xv = _to_dense(tv.transform(X_val))\n",
    "    Xs = _to_dense(tv.transform(X_test))\n",
    "    return tv, Xt, Xv, Xs\n"
   ],
   "id": "ffb43a27c3ac51cb",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:32:10.789607Z",
     "start_time": "2025-08-19T11:32:10.786115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This function subsamples the training data to a maximum size defined by `TABPFN_MAX`. If the dataset is smaller than this cap, it returns the full dataset; otherwise, it randomly samples without replacement.\n",
    "def _subsample(X, y, cap=TABPFN_MAX, seed=SEED):\n",
    "    if len(X) <= cap:\n",
    "        return X, y, np.arange(len(X))\n",
    "    idx = np.random.RandomState(seed).choice(len(X), size=cap, replace=False)\n",
    "    if hasattr(X, \"iloc\"):\n",
    "        Xs = X.iloc[idx]\n",
    "    else:\n",
    "        Xs = X[idx]\n",
    "    ys = y[idx]\n",
    "    return Xs, ys, idx\n",
    "\n",
    "# Fits a TabPFN model (either classifier or regressor) based on the task type. It initializes the model with the specified device and number of estimators, then fits it to the provided training data.\n",
    "def _fit_tabpfn(task, Xt, yt):\n",
    "    if task.task_type == TaskType.REGRESSION and TabPFNRegressor is not None:\n",
    "        model = TabPFNRegressor(\n",
    "            device=DEVICE,\n",
    "            #n_estimators=int(N_ESTIMATORS),\n",
    "            ignore_pretraining_limits=True,\n",
    "        )\n",
    "    else:\n",
    "        model = TabPFNClassifier(\n",
    "            device=DEVICE,\n",
    "            #n_estimators=int(N_ESTIMATORS),\n",
    "            ignore_pretraining_limits=True,\n",
    "        )\n",
    "    model.fit(Xt, yt)\n",
    "    return model\n",
    "\n",
    "# Helper function to make predictions for a given task using the fitted model. It handles different task types (regression, binary classification, multiclass/multilabel) and returns the appropriate prediction format.\n",
    "def _predict_for_task(task, model, X):\n",
    "    # align with RelBench evaluators: AUROC expects probabilities for the positive class\n",
    "    if task.task_type == TaskType.REGRESSION:\n",
    "        return model.predict(X)\n",
    "    proba = model.predict_proba(X)\n",
    "    if task.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "        return proba[:, 1]\n",
    "    else:\n",
    "        # multiclass/multilabel: pass full probability matrix\n",
    "        return proba"
   ],
   "id": "5c53f649ce01b948",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build a hetero-temporal graph for experiments\n",
   "id": "b93e756ef255ccaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T11:32:43.131600Z",
     "start_time": "2025-08-19T11:32:10.803784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "col_to_stype_dict = get_stype_proposal(db)\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=GloveTextEmbedding(device=torch.device(DEVICE)), batch_size=256\n",
    ")\n",
    "data, col_stats_dict = make_pkey_fkey_graph(\n",
    "    db,\n",
    "    col_to_stype_dict=col_to_stype_dict,\n",
    "    text_embedder_cfg=text_embedder_cfg,\n",
    "    cache_dir=None,\n",
    ")"
   ],
   "id": "748b9595f3f8950a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding raw data in mini-batch:  60%|█████▉    | 1082/1811 [00:14<00:09, 74.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m col_to_stype_dict \u001B[38;5;241m=\u001B[39m get_stype_proposal(db)\n\u001B[1;32m      2\u001B[0m text_embedder_cfg \u001B[38;5;241m=\u001B[39m TextEmbedderConfig(\n\u001B[1;32m      3\u001B[0m     text_embedder\u001B[38;5;241m=\u001B[39mGloveTextEmbedding(device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(DEVICE)), batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m\n\u001B[1;32m      4\u001B[0m )\n\u001B[0;32m----> 5\u001B[0m data, col_stats_dict \u001B[38;5;241m=\u001B[39m \u001B[43mmake_pkey_fkey_graph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcol_to_stype_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcol_to_stype_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext_embedder_cfg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_embedder_cfg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/relbench/modeling/graph.py:71\u001B[0m, in \u001B[0;36mmake_pkey_fkey_graph\u001B[0;34m(db, col_to_stype_dict, text_embedder_cfg, cache_dir)\u001B[0m\n\u001B[1;32m     65\u001B[0m     df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__const__\u001B[39m\u001B[38;5;124m\"\u001B[39m: np\u001B[38;5;241m.\u001B[39mones(\u001B[38;5;28mlen\u001B[39m(table\u001B[38;5;241m.\u001B[39mdf)), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfkey_dict})\n\u001B[1;32m     67\u001B[0m path \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m cache_dir \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(cache_dir, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     69\u001B[0m )\n\u001B[0;32m---> 71\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcol_to_stype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcol_to_stype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcol_to_text_embedder_cfg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_embedder_cfg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaterialize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m data[table_name]\u001B[38;5;241m.\u001B[39mtf \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mtensor_frame\n\u001B[1;32m     78\u001B[0m col_stats_dict[table_name] \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mcol_stats\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch_frame/data/dataset.py:631\u001B[0m, in \u001B[0;36mDataset.materialize\u001B[0;34m(self, device, path, col_stats)\u001B[0m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;66;03m# 2. Create the `TensorFrame`:\u001B[39;00m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_to_tensor_frame_converter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_tensorframe_converter()\n\u001B[0;32m--> 631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tensor_frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_to_tensor_frame_converter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;66;03m# 3. Update col stats based on `TensorFrame`\u001B[39;00m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_col_stats()\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch_frame/data/dataset.py:301\u001B[0m, in \u001B[0;36mDataFrameToTensorFrameConverter.__call__\u001B[0;34m(self, df, device)\u001B[0m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m stype, col_names \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcol_names_dict\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m col_names:\n\u001B[0;32m--> 301\u001B[0m         out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_mapper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    302\u001B[0m         xs_dict[stype]\u001B[38;5;241m.\u001B[39mappend(out)\n\u001B[1;32m    304\u001B[0m feat_dict \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch_frame/data/mapper.py:426\u001B[0m, in \u001B[0;36mEmbeddingTensorMapper.forward\u001B[0;34m(self, ser, device)\u001B[0m\n\u001B[1;32m    423\u001B[0m emb_list \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    424\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(ser_list), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size),\n\u001B[1;32m    425\u001B[0m               desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbedding raw data in mini-batch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 426\u001B[0m     emb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mser_list\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m:\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    427\u001B[0m     emb_list\u001B[38;5;241m.\u001B[39mappend(emb\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[1;32m    428\u001B[0m values \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(emb_list, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "Cell \u001B[0;32mIn[5], line 10\u001B[0m, in \u001B[0;36mGloveTextEmbedding.__call__\u001B[0;34m(self, sentences)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, sentences: List[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1087\u001B[0m, in \u001B[0;36mSentenceTransformer.encode\u001B[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001B[0m\n\u001B[1;32m   1085\u001B[0m             \u001B[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001B[39;00m\n\u001B[1;32m   1086\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m convert_to_numpy:\n\u001B[0;32m-> 1087\u001B[0m                 embeddings \u001B[38;5;241m=\u001B[39m \u001B[43membeddings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1089\u001B[0m         all_embeddings\u001B[38;5;241m.\u001B[39mextend(embeddings)\n\u001B[1;32m   1091\u001B[0m all_embeddings \u001B[38;5;241m=\u001B[39m [all_embeddings[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39margsort(length_sorted_idx)]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Run TabPFN on Selected Tasks\n",
    "\n",
    "Runs TabPFN on a specified dataset and task, handling both single-table and merged-table modes. It vectorizes the data, fits the model, makes predictions, and evaluates performance using RelBench’s evaluators. Returns a dictionary with results."
   ],
   "id": "21448305df117a22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_tabpfn_on_task(dataset_name: str, task_name: str, mode: str = \"single\") -> Dict[str, Any]:\n",
    "    # Load dataset and task splits\n",
    "    input_db = get_dataset(dataset_name, download=DOWNLOAD).get_db()\n",
    "    task, splits = fetch_splits(dataset_name, task_name, download=DOWNLOAD)\n",
    "\n",
    "    # Ensure the task is compatible with TabPFN\n",
    "    if mode == \"single\":\n",
    "        frames = build_single_table_frames(task, splits)\n",
    "    elif mode == \"merged\":\n",
    "        frames = build_merged_table_frames(data, input_db, task, splits)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'single' or 'merged'\")\n",
    "\n",
    "    # Extract features and targets for each split\n",
    "    (Xtr, ytr, _dftr) = frames[\"train\"]\n",
    "    (Xva, yva, dfva)  = frames[\"val\"]\n",
    "    (Xte, yte, dfte)  = frames[\"test\"]\n",
    "\n",
    "    # Vectorize\n",
    "    tv, Xt, Xv, Xs = vectorize_splits(Xtr, Xva, Xte)\n",
    "\n",
    "    # Respect TabPFN's sample cap\n",
    "    Xt_cap, yt_cap, _ = _subsample(Xt, ytr, cap=TABPFN_MAX, seed=SEED)\n",
    "\n",
    "    # Fit\n",
    "    model = _fit_tabpfn(task, Xt_cap, yt_cap)\n",
    "\n",
    "    # Predict & Evaluate with RelBench evaluators\n",
    "    val_pred  = _predict_for_task(task, model, Xv)\n",
    "    test_pred = _predict_for_task(task, model, Xs)\n",
    "\n",
    "    # Align predictions with original DataFrame indices for evaluation\n",
    "    val_metrics  = task.evaluate(val_pred,  splits[\"val\"])\n",
    "    test_metrics = task.evaluate(test_pred, splits[\"test\"])\n",
    "\n",
    "    # Convert metrics to a dictionary, ensuring all values are floats\n",
    "    out = {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"task\": task_name,\n",
    "        \"mode\": mode,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"n_train_used\": len(Xt_cap),\n",
    "        \"n_train_total\": len(Xt),\n",
    "        \"n_val\": len(Xv),\n",
    "        \"n_test\": len(Xs),\n",
    "    }\n",
    "    return out"
   ],
   "id": "52860c3ba39daf4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Orchestrator for Benchmark Runs\n",
    "\n",
    "Iterates over all discovered tasks and runs TabPFN in both **single** and **merged** modes. Collects performance metrics for validation and test splits into a results table, handling failures gracefully. Sorts results for easier comparison.\n"
   ],
   "id": "1390db1f75d264ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MODES = globals().get(\"MODES\", [\"single\", \"merged\"])\n",
    "\n",
    "records = []\n",
    "failures = []\n",
    "\n",
    "# Run TabPFN on all tasks in both modes and collect results\n",
    "for task_name in TASKS:\n",
    "    for mode in MODES:\n",
    "        try:\n",
    "            res = run_tabpfn_on_task(DATASET, task_name, mode=mode)\n",
    "            # Flatten metrics for val and test, one metric per row\n",
    "            for split in [\"val\", \"test\"]:\n",
    "                metrics = res.get(f\"{split}_metrics\") or {}\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    # Only add rows with non-empty metric_value\n",
    "                    if metric_value is not None and not (isinstance(metric_value, float) and np.isnan(metric_value)):\n",
    "                        records.append({\n",
    "                            \"dataset\": res.get(\"dataset\", DATASET),\n",
    "                            \"task\": res.get(\"task\", task_name),\n",
    "                            \"split\": split,\n",
    "                            \"mode\": res.get(\"mode\", mode),\n",
    "                            \"method\": \"TabPFN_experimental_v1.0\",\n",
    "                            \"metric\": metric_name,\n",
    "                            \"score\": metric_value,\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            msg = f\"[{DATASET} | {task_name} | {mode}] failed: {e!s}\"\n",
    "            print(msg)\n",
    "            failures.append(msg)\n",
    "\n",
    "# Convert collected records into a DataFrame\n",
    "results_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# If no successful runs were recorded, display a message and create an empty DataFrame\n",
    "if results_df.empty:\n",
    "    print(\"No successful runs were recorded. Check the failure messages above.\")\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=[\"dataset\", \"task\", \"split\", \"mode\", \"method\", \"metric\", \"score\"]\n",
    "    )\n",
    "else:\n",
    "    # Ensure required sort keys exist even if some rows missed them\n",
    "    for col in [\"task\", \"mode\"]:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = pd.NA\n",
    "    # Sort only by the columns that exist to avoid KeyError\n",
    "    sort_keys = [c for c in [\"task\", \"mode\", \"metric\"] if c in results_df.columns]\n",
    "    if sort_keys:\n",
    "        results_df = results_df.sort_values(sort_keys)\n",
    "\n",
    "display(results_df)\n"
   ],
   "id": "be6242da806a8050",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save Results to CSV",
   "id": "63c64e43fa5f49a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify output directory\n",
    "out_dir = globals().get(\"OUT_DIR\", \"outputs\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Change timestamp format to \"dd.mm.yyyy-hh:mm\"\n",
    "timestamp = time.strftime(\"%d.%m.%Y-%H:%M\")\n",
    "csv_name = f\"tabpfn_{DATASET}_{timestamp}.csv\"\n",
    "csv_path = os.path.join(out_dir, csv_name)\n",
    "\n",
    "# Round all numerical results to 4 decimal places before saving\n",
    "if \"score\" in results_df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(results_df[\"score\"]):\n",
    "        results_df[\"score\"] = results_df[\"score\"].round(4)\n",
    "    else:\n",
    "        # Optionally, try to convert to numeric first\n",
    "        results_df[\"score\"] = pd.to_numeric(results_df[\"score\"], errors=\"coerce\").round(4)\n",
    "\n",
    "# Filter out rows with empty score values before saving\n",
    "if \"score\" in results_df.columns:\n",
    "    results_df = results_df[results_df[\"score\"].notnull()]\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved results to: {csv_path}\")\n"
   ],
   "id": "aa929637db7d0d54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "51d929bb0341da09",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
