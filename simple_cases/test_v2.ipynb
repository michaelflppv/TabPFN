{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-13T15:07:22.531035Z",
     "start_time": "2025-08-13T15:07:22.526718Z"
    }
   },
   "source": [
    "# %%\n",
    "import os\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    mean_absolute_error,\n",
    "    #mean_squared_error,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "# RelBench\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "\n",
    "# TabPFN\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "\n",
    "# Device preference\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:07:22.550976Z",
     "start_time": "2025-08-13T15:07:22.548116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = time.perf_counter()\n",
    "    yield lambda: time.perf_counter() - start\n",
    "\n",
    "\n",
    "def classification_metrics(y_true, y_pred, y_prob=None) -> Dict[str, float]:\n",
    "    out = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            out[\"roc_auc\"] = roc_auc_score(y_true, y_prob)\n",
    "        except Exception:\n",
    "            out[\"roc_auc\"] = np.nan\n",
    "    else:\n",
    "        out[\"roc_auc\"] = np.nan\n",
    "    return out\n",
    "\n",
    "\n",
    "def regression_metrics(y_true, y_pred) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        #\"mse\": mean_squared_error(y_true, y_pred),\n",
    "    }\n"
   ],
   "id": "4f09b9768ee9fdcb",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:07:22.577762Z",
     "start_time": "2025-08-13T15:07:22.562293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "dataset = get_dataset(\"rel-f1\")\n",
    "db = dataset.get_db()\n",
    "\n",
    "def to_pandas(table):\n",
    "    if hasattr(table, \"to_pandas\"):\n",
    "        return table.to_pandas()\n",
    "    if hasattr(table, \"df\"):\n",
    "        return table.df\n",
    "    raise ValueError(\"Unknown table type\")\n",
    "\n",
    "# Load key tables\n",
    "tables = {}\n",
    "for name in db.table_dict:\n",
    "    tables[name] = to_pandas(db.table_dict[name])\n",
    "\n",
    "for t in tables.values():\n",
    "    for col in t.columns:\n",
    "        if \"date\" in col.lower():\n",
    "            t[col] = pd.to_datetime(t[col], errors=\"coerce\")\n"
   ],
   "id": "830e3b724db7392",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:07:22.591625Z",
     "start_time": "2025-08-13T15:07:22.587129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_single_table_experiment(task_name: str):\n",
    "    global prob_val, prob_test\n",
    "    task = get_task(\"rel-f1\", task_name)\n",
    "\n",
    "    train_table = task.get_table(\"train\")\n",
    "    val_table = task.get_table(\"val\")\n",
    "    test_table = task.get_table(\"test\", mask_input_cols=False)\n",
    "\n",
    "    df = train_table.df\n",
    "    # Limit samples to 1000\n",
    "    df = df.sample(n=min(1000, len(df)), random_state=42)\n",
    "    X_train = df.drop(columns=[task.target_col]).select_dtypes(include=[np.number])\n",
    "    y_train = df[task.target_col]\n",
    "\n",
    "    df = val_table.df\n",
    "    df = df.sample(n=min(1000, len(df)), random_state=42)\n",
    "    X_val = df.drop(columns=[task.target_col]).select_dtypes(include=[np.number])\n",
    "    y_val = df[task.target_col]\n",
    "\n",
    "    df = test_table.df\n",
    "    df = df.sample(n=min(1000, len(df)), random_state=42)\n",
    "    X_test = df.drop(columns=[task.target_col]).select_dtypes(include=[np.number])\n",
    "    y_test = df[task.target_col]\n",
    "\n",
    "    if task_name == \"driver-position\":\n",
    "        model = TabPFNRegressor(device=DEVICE)\n",
    "        metric_fn = regression_metrics\n",
    "        prob_val = prob_test = None\n",
    "    else:\n",
    "        model = TabPFNClassifier(device=DEVICE, ignore_pretraining_limits=True)\n",
    "        metric_fn = classification_metrics\n",
    "\n",
    "    with elapsed_timer() as t:\n",
    "        model.fit(X_train, y_train)\n",
    "    fit_time = t()\n",
    "\n",
    "    with elapsed_timer() as t:\n",
    "        y_val_pred = model.predict(X_val)\n",
    "    pred_time_val = t()\n",
    "\n",
    "    with elapsed_timer() as t:\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    pred_time_test = t()\n",
    "\n",
    "    if task_name != \"driver-position\":\n",
    "        try:\n",
    "            prob_val  = model.predict_proba(X_val)[:, 1]\n",
    "            prob_test = model.predict_proba(X_test)[:, 1]\n",
    "        except Exception:\n",
    "            prob_val = prob_test = None\n",
    "\n",
    "    primary_metric_val = task.evaluate(\n",
    "        prob_val if prob_val is not None else y_val_pred,\n",
    "        target_table=task.get_table(\"val\", mask_input_cols=False)\n",
    "    )\n",
    "    primary_metric_test = task.evaluate(\n",
    "        prob_test if prob_test is not None else y_test_pred,\n",
    "        target_table=task.get_table(\"test\", mask_input_cols=False)\n",
    "    )\n",
    "    res = {\n",
    "        \"val\": {\n",
    "            **metric_fn(y_val, y_val_pred, prob_val),\n",
    "            \"fit_time\": fit_time,\n",
    "            \"predict_time\": pred_time_val,\n",
    "            \"primary_metric_relbench\": primary_metric_val,\n",
    "        },\n",
    "        \"test\": {\n",
    "            **metric_fn(y_test, y_test_pred, prob_test),\n",
    "            \"fit_time\": fit_time,\n",
    "            \"predict_time\": pred_time_test,\n",
    "            \"primary_metric_relbench\": primary_metric_test,\n",
    "        }\n",
    "    }\n",
    "    return res"
   ],
   "id": "dea407ba0ff8f22f",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:07:22.629089Z",
     "start_time": "2025-08-13T15:07:22.600429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Python\n",
    "def engineer_driver_features():\n",
    "    results = tables[\"results\"].merge(\n",
    "        tables[\"races\"][[\"raceId\", \"date\"]],\n",
    "        on=\"raceId\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    print(\"results columns after merge:\", results.columns.tolist())\n",
    "    print(\"Number of rows after merge:\", results.shape[0])\n",
    "    results = results.dropna(subset=[\"driverId\", \"date_y\"])\n",
    "    print(\"Number of rows after dropna:\", results.shape[0])\n",
    "\n",
    "    results[\"dnf_flag\"] = (~results[\"positionOrder\"].isna()).astype(int)\n",
    "    feats = results.groupby(\"driverId\").expanding().agg({\n",
    "        \"positionOrder\": \"mean\",\n",
    "        \"points\": \"mean\",\n",
    "        \"dnf_flag\": \"mean\",\n",
    "        \"laps\": \"mean\"\n",
    "    }).reset_index()\n",
    "    feats = feats.rename(columns={\n",
    "        \"positionOrder\": \"avg_position\",\n",
    "        \"points\": \"avg_points\",\n",
    "        \"dnf_flag\": \"dnf_rate\",\n",
    "        \"laps\": \"avg_laps\"\n",
    "    })\n",
    "    feats[\"date\"] = results[\"date_y\"].values\n",
    "    return feats\n",
    "\n",
    "driver_feats = engineer_driver_features()\n"
   ],
   "id": "15446aa30203650c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results columns after merge: ['resultId', 'raceId', 'driverId', 'constructorId', 'number', 'grid', 'position', 'positionOrder', 'points', 'laps', 'milliseconds', 'fastestLap', 'rank', 'statusId', 'date_x', 'date_y']\n",
      "Number of rows after merge: 20323\n",
      "Number of rows after dropna: 20323\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:07:22.639775Z",
     "start_time": "2025-08-13T15:07:22.634339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def run_merged_table_experiment(task_name: str):\n",
    "    global prob_val, prob_test\n",
    "    task = get_task(\"rel-f1\", task_name)\n",
    "\n",
    "    train_table = task.get_table(\"train\")\n",
    "    val_table = task.get_table(\"val\")\n",
    "    test_table = task.get_table(\"test\", mask_input_cols=False)\n",
    "\n",
    "    df_train = train_table.df\n",
    "    df_train = df_train.sample(n=min(1000, len(df_train)), random_state=42)\n",
    "    idx_train = list(zip(df_train[\"driverId\"], df_train[\"date\"]))\n",
    "    X_train = df_train.drop(columns=[task.target_col]).select_dtypes(include=[np.number])\n",
    "    y_train = df_train[task.target_col]\n",
    "\n",
    "    df_val = val_table.df\n",
    "    df_val = df_val.sample(n=min(1000, len(df_val)), random_state=42)\n",
    "    idx_val   = list(zip(df_val[\"driverId\"], df_val[\"date\"]))\n",
    "    X_val = df_val.drop(columns=[task.target_col]).select_dtypes(include=[np.number])\n",
    "    y_val = df_val[task.target_col]\n",
    "\n",
    "    df_test = test_table.df\n",
    "    df_test = df_test.sample(n=min(1000, len(df_test)), random_state=42)\n",
    "    idx_test  = list(zip(df_test[\"driverId\"], df_test[\"date\"]))\n",
    "    X_test = df_test.drop(columns=[task.target_col]).select_dtypes(include=[np.number])\n",
    "    y_test = df_test[task.target_col]\n",
    "\n",
    "    # Merge features by driverId and date (backward asof join)\n",
    "    def enrich(X, idx):\n",
    "        df = X.copy()\n",
    "        idx_df = pd.DataFrame(idx, columns=[\"driverId\", \"date\"])\n",
    "        idx_df[\"driverId\"] = idx_df[\"driverId\"].astype(\"int64\")\n",
    "        idx_df[\"date\"] = pd.to_datetime(idx_df[\"date\"])\n",
    "        driver_feats_fixed = driver_feats.copy()\n",
    "        driver_feats_fixed[\"driverId\"] = driver_feats_fixed[\"driverId\"].astype(\"int64\")\n",
    "        merged = pd.merge_asof(\n",
    "            idx_df.sort_values(\"date\"),\n",
    "            driver_feats_fixed.sort_values(\"date\"),\n",
    "            on=\"date\", by=\"driverId\",\n",
    "            direction=\"backward\", tolerance=pd.Timedelta(\"3650D\")\n",
    "        )\n",
    "        merged = merged.drop(columns=[\"driverId\", \"date\"])\n",
    "        df = pd.concat([df.reset_index(drop=True), merged.reset_index(drop=True)], axis=1)\n",
    "        return df\n",
    "\n",
    "    X_train_en = enrich(X_train, idx_train)\n",
    "    X_val_en   = enrich(X_val, idx_val)\n",
    "    X_test_en  = enrich(X_test, idx_test)\n",
    "\n",
    "    if task_name == \"driver-position\":\n",
    "        model = TabPFNRegressor(device=DEVICE)\n",
    "        metric_fn = regression_metrics\n",
    "        prob_val = prob_test = None\n",
    "    else:\n",
    "        model = TabPFNClassifier(device=DEVICE)\n",
    "        metric_fn = classification_metrics\n",
    "\n",
    "    with elapsed_timer() as t:\n",
    "        model.fit(X_train_en, y_train)\n",
    "    fit_time = t()\n",
    "\n",
    "    with elapsed_timer() as t:\n",
    "        y_val_pred = model.predict(X_val_en)\n",
    "    pred_time_val = t()\n",
    "\n",
    "    with elapsed_timer() as t:\n",
    "        y_test_pred = model.predict(X_test_en)\n",
    "    pred_time_test = t()\n",
    "\n",
    "    if task_name != \"driver-position\":\n",
    "        try:\n",
    "            prob_val  = model.predict_proba(X_val_en)[:, 1]\n",
    "            prob_test = model.predict_proba(X_test_en)[:, 1]\n",
    "        except Exception:\n",
    "            prob_val = prob_test = None\n",
    "\n",
    "    primary_metric_val = task.evaluate(\n",
    "        prob_val if prob_val is not None else y_val_pred,\n",
    "        target_table=task.get_table(\"val\", mask_input_cols=False)\n",
    "    )\n",
    "    primary_metric_test = task.evaluate(\n",
    "        prob_test if prob_test is not None else y_test_pred,\n",
    "        target_table=task.get_table(\"test\", mask_input_cols=False)\n",
    "    )\n",
    "    res = {\n",
    "        \"val\": {\n",
    "            **metric_fn(y_val, y_val_pred, prob_val),\n",
    "            \"fit_time\": fit_time,\n",
    "            \"predict_time\": pred_time_val,\n",
    "            \"primary_metric_relbench\": primary_metric_val,\n",
    "        },\n",
    "        \"test\": {\n",
    "            **metric_fn(y_test, y_test_pred, prob_test),\n",
    "            \"fit_time\": fit_time,\n",
    "            \"predict_time\": pred_time_test,\n",
    "            \"primary_metric_relbench\": primary_metric_test,\n",
    "        }\n",
    "    }\n",
    "    return res\n"
   ],
   "id": "a83a2661d3059bc3",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:07:22.654538Z",
     "start_time": "2025-08-13T15:07:22.653241Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "de82238dba8c2502",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:27:45.245414Z",
     "start_time": "2025-08-13T15:07:22.665601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "TASKS = [\"driver-dnf\", \"driver-top3\", \"driver-position\"]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for task in TASKS:\n",
    "    print(f\"=== {task} | Single Table ===\")\n",
    "    res_single = run_single_table_experiment(task)\n",
    "    single_results = []\n",
    "    for split, metrics in res_single.items():\n",
    "        result = {\"task\": task, \"setting\": \"single\", \"split\": split, **metrics}\n",
    "        all_results.append(result)\n",
    "        single_results.append(result)\n",
    "    single_df = pd.DataFrame(single_results)\n",
    "    print(\"Table Contents (Single Table):\")\n",
    "    print(single_df)\n",
    "    single_df.to_csv(f\"results_{task}_single.csv\", index=False)\n",
    "\n",
    "    print(f\"=== {task} | Merged Table ===\")\n",
    "    res_merged = run_merged_table_experiment(task)\n",
    "    merged_results = []\n",
    "    for split, metrics in res_merged.items():\n",
    "        result = {\"task\": task, \"setting\": \"merged\", \"split\": split, **metrics}\n",
    "        all_results.append(result)\n",
    "        merged_results.append(result)\n",
    "    merged_df = pd.DataFrame(merged_results)\n",
    "    print(\"Table Contents (Merged Table):\")\n",
    "    print(merged_df)\n",
    "    merged_df.to_csv(f\"results_{task}_merged.csv\", index=False)\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"results_summary.csv\", index=False)\n",
    "results_df\n"
   ],
   "id": "deda7044b39dafda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== driver-dnf | Single Table ===\n",
      "Table Contents (Single Table):\n",
      "         task setting split  accuracy  f1_macro   roc_auc  fit_time  \\\n",
      "0  driver-dnf  single   val  0.779152  0.437934  0.639311   1.61867   \n",
      "1  driver-dnf  single  test  0.705128  0.413534  0.643210   1.61867   \n",
      "\n",
      "   predict_time                            primary_metric_relbench  \n",
      "0     63.224763  {'average_precision': 0.8072204308442181, 'acc...  \n",
      "1     64.385103  {'average_precision': 0.7048644862250875, 'acc...  \n",
      "=== driver-dnf | Merged Table ===\n",
      "Table Contents (Merged Table):\n",
      "         task setting split  accuracy  f1_macro   roc_auc  fit_time  \\\n",
      "0  driver-dnf  merged   val  0.779152  0.437934  0.649832  1.109662   \n",
      "1  driver-dnf  merged  test  0.705128  0.413534  0.662133  1.109662   \n",
      "\n",
      "   predict_time                            primary_metric_relbench  \n",
      "0     70.684271  {'average_precision': 0.8026523198683857, 'acc...  \n",
      "1     71.078754  {'average_precision': 0.6893341578285747, 'acc...  \n",
      "=== driver-top3 | Single Table ===\n",
      "Table Contents (Single Table):\n",
      "          task setting split  accuracy  f1_macro   roc_auc  fit_time  \\\n",
      "0  driver-top3  single   val  0.794218  0.529640  0.548843  2.859073   \n",
      "1  driver-top3  single  test  0.792011  0.460807  0.553244  2.859073   \n",
      "\n",
      "   predict_time                            primary_metric_relbench  \n",
      "0     64.889846  {'average_precision': 0.20858739353475653, 'ac...  \n",
      "1     64.772215  {'average_precision': 0.16788434449607598, 'ac...  \n",
      "=== driver-top3 | Merged Table ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/TabPFN/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:1650: RuntimeWarning: All-NaN slice encountered\n",
      "  return fnb._ureduce(a,\n",
      "/opt/anaconda3/envs/TabPFN/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:1650: RuntimeWarning: All-NaN slice encountered\n",
      "  return fnb._ureduce(a,\n",
      "/opt/anaconda3/envs/TabPFN/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:1650: RuntimeWarning: All-NaN slice encountered\n",
      "  return fnb._ureduce(a,\n",
      "/opt/anaconda3/envs/TabPFN/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:1650: RuntimeWarning: All-NaN slice encountered\n",
      "  return fnb._ureduce(a,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Contents (Merged Table):\n",
      "          task setting split  accuracy  f1_macro   roc_auc  fit_time  \\\n",
      "0  driver-top3  merged   val  0.794218  0.529640  0.555688  2.866353   \n",
      "1  driver-top3  merged  test  0.792011  0.460807  0.555282  2.866353   \n",
      "\n",
      "   predict_time                            primary_metric_relbench  \n",
      "0     70.156459  {'average_precision': 0.1982632097990248, 'acc...  \n",
      "1     70.307139  {'average_precision': 0.1677856289136237, 'acc...  \n",
      "=== driver-position | Single Table ===\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'squared'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[70], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m task \u001B[38;5;129;01min\u001B[39;00m TASKS:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=== \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtask\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Single Table ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 8\u001B[0m     res_single \u001B[38;5;241m=\u001B[39m \u001B[43mrun_single_table_experiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     single_results \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m split, metrics \u001B[38;5;129;01min\u001B[39;00m res_single\u001B[38;5;241m.\u001B[39mitems():\n",
      "Cell \u001B[0;32mIn[67], line 52\u001B[0m, in \u001B[0;36mrun_single_table_experiment\u001B[0;34m(task_name)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m     50\u001B[0m         prob_val \u001B[38;5;241m=\u001B[39m prob_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 52\u001B[0m primary_metric_val \u001B[38;5;241m=\u001B[39m \u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprob_val\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mprob_val\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43my_val_pred\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget_table\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_table\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mval\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask_input_cols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m primary_metric_test \u001B[38;5;241m=\u001B[39m task\u001B[38;5;241m.\u001B[39mevaluate(\n\u001B[1;32m     57\u001B[0m     prob_test \u001B[38;5;28;01mif\u001B[39;00m prob_test \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m y_test_pred,\n\u001B[1;32m     58\u001B[0m     target_table\u001B[38;5;241m=\u001B[39mtask\u001B[38;5;241m.\u001B[39mget_table(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m, mask_input_cols\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     59\u001B[0m )\n\u001B[1;32m     60\u001B[0m res \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m     62\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmetric_fn(y_val, y_val_pred, prob_val),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     72\u001B[0m     }\n\u001B[1;32m     73\u001B[0m }\n",
      "File \u001B[0;32m/opt/anaconda3/envs/TabPFN/lib/python3.10/site-packages/relbench/base/task_entity.py:63\u001B[0m, in \u001B[0;36mEntityTask.evaluate\u001B[0;34m(self, pred, target_table, metrics)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pred) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(target):\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe length of pred and target must be the same (got \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(pred)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(target)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, respectively).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     61\u001B[0m     )\n\u001B[0;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {fn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m: fn(target, pred) \u001B[38;5;28;01mfor\u001B[39;00m fn \u001B[38;5;129;01min\u001B[39;00m metrics}\n",
      "File \u001B[0;32m/opt/anaconda3/envs/TabPFN/lib/python3.10/site-packages/relbench/base/task_entity.py:63\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pred) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(target):\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe length of pred and target must be the same (got \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(pred)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(target)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, respectively).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     61\u001B[0m     )\n\u001B[0;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {fn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m: \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpred\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m fn \u001B[38;5;129;01min\u001B[39;00m metrics}\n",
      "File \u001B[0;32m/opt/anaconda3/envs/TabPFN/lib/python3.10/site-packages/relbench/metrics.py:80\u001B[0m, in \u001B[0;36mrmse\u001B[0;34m(true, pred)\u001B[0m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrmse\u001B[39m(true: NDArray[np\u001B[38;5;241m.\u001B[39mfloat64], pred: NDArray[np\u001B[38;5;241m.\u001B[39mfloat64]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[0;32m---> 80\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mskm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean_squared_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msquared\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/TabPFN/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:196\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    193\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n\u001B[0;32m--> 196\u001B[0m params \u001B[38;5;241m=\u001B[39m \u001B[43mfunc_sig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    197\u001B[0m params\u001B[38;5;241m.\u001B[39mapply_defaults()\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# ignore self/cls and positional/keyword markers\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/TabPFN/lib/python3.10/inspect.py:3186\u001B[0m, in \u001B[0;36mSignature.bind\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3181\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mbind\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m/\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   3182\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001B[39;00m\n\u001B[1;32m   3183\u001B[0m \u001B[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001B[39;00m\n\u001B[1;32m   3184\u001B[0m \u001B[38;5;124;03m    if the passed arguments can not be bound.\u001B[39;00m\n\u001B[1;32m   3185\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/TabPFN/lib/python3.10/inspect.py:3175\u001B[0m, in \u001B[0;36mSignature._bind\u001B[0;34m(self, args, kwargs, partial)\u001B[0m\n\u001B[1;32m   3173\u001B[0m         arguments[kwargs_param\u001B[38;5;241m.\u001B[39mname] \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[1;32m   3174\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3175\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m   3176\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgot an unexpected keyword argument \u001B[39m\u001B[38;5;132;01m{arg!r}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   3177\u001B[0m                 arg\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(kwargs))))\n\u001B[1;32m   3179\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_arguments_cls(\u001B[38;5;28mself\u001B[39m, arguments)\n",
      "\u001B[0;31mTypeError\u001B[0m: got an unexpected keyword argument 'squared'"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%\n",
    "def plot_metric(metric):\n",
    "    sub = results_df[(results_df[\"split\"] == \"test\") & results_df[metric].notna()]\n",
    "    if sub.empty:\n",
    "        return\n",
    "    pivot = sub.pivot(index=\"task\", columns=\"setting\", values=metric)\n",
    "    pivot.plot(kind=\"bar\", figsize=(8, 4), title=metric)\n",
    "    plt.grid(True, axis=\"y\")\n",
    "    plt.show()\n",
    "\n",
    "for m in [\"roc_auc\", \"f1_macro\", \"mae\", \"fit_time\", \"predict_time\"]:\n",
    "    plot_metric(m)\n"
   ],
   "id": "6013122562f73f86"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
