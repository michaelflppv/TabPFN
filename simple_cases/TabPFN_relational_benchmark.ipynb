{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:29:26.002074Z",
     "start_time": "2025-08-15T13:29:23.299117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "from tqdm import tqdm  # Add tqdm for progress bar\n",
    "from itertools import product\n",
    "\n",
    "# RelBench\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "\n",
    "import relbench.metrics\n",
    "import inspect\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "# TabPFN\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "\n",
    "from relbench.modeling.utils import get_stype_proposal\n",
    "from relbench.modeling.graph import make_pkey_fkey_graph, get_node_train_table_input\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- ADD: Import Model, test, BCEWithLogitsLoss, L1Loss from train_model.ipynb ---t\n",
    "from torch.nn import BCEWithLogitsLoss, L1Loss\n",
    "# --- END ADD ---\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "# Device preference\n",
    "#if torch.backends.mps.is_available():\n",
    "#    DEVICE = \"mps\"\n",
    "#elif torch.cuda.is_available():\n",
    "#    DEVICE = \"cuda\"\n",
    "#else:\n",
    "#    DEVICE = \"cpu\"\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Define global dataset variable\n",
    "#DATASET = \"rel-f1\"\n"
   ],
   "id": "57624fe02c838842",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:29:26.015702Z",
     "start_time": "2025-08-15T13:29:26.011282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Dataset/task discovery (dataset-agnostic) ---\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task, get_task_names\n",
    "from relbench.base import TaskType\n",
    "\n",
    "# Reuse existing config if present, otherwise set defaults\n",
    "DATASET = globals().get(\"DATASET\", \"rel-f1\")\n",
    "DOWNLOAD = globals().get(\"DOWNLOAD\", True)\n",
    "\n",
    "# Discover tasks and keep only entity-level cls/reg tasks TabPFN can handle\n",
    "def _is_tabpfn_friendly(task):\n",
    "    return task.task_type in (\n",
    "        TaskType.BINARY_CLASSIFICATION,\n",
    "        TaskType.MULTICLASS_CLASSIFICATION,\n",
    "        TaskType.MULTILABEL_CLASSIFICATION,\n",
    "        TaskType.REGRESSION,\n",
    "    )\n",
    "\n",
    "_all = get_task_names(DATASET)  # shown in tutorials\n",
    "TASKS = []\n",
    "for tname in _all:\n",
    "    try:\n",
    "        t = get_task(DATASET, tname, download=DOWNLOAD)\n",
    "        if _is_tabpfn_friendly(t):\n",
    "            TASKS.append(tname)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {tname}: {e!s}\")\n",
    "\n",
    "print(f\"{DATASET}: {len(TASKS)} TabPFN-friendly tasks -> {TASKS}\")\n"
   ],
   "id": "d94075cf88bf6806",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel-f1: 3 TabPFN-friendly tasks -> ['driver-position', 'driver-dnf', 'driver-top3']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:29:26.099625Z",
     "start_time": "2025-08-15T13:29:26.098151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Patch relbench.metrics.skm.mean_squared_error to local mean_squared_error\n",
    "relbench.metrics.skm.mean_squared_error = mean_squared_error\n",
    "\n",
    "def patched_rmse(true, pred):\n",
    "    if \"squared\" in inspect.signature(mean_squared_error).parameters:\n",
    "        return mean_squared_error(true, pred, squared=False)\n",
    "    else:\n",
    "        return np.sqrt(mean_squared_error(true, pred))\n",
    "\n",
    "relbench.metrics.rmse = patched_rmse"
   ],
   "id": "46fcd0d47afb30c1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:29:26.106854Z",
     "start_time": "2025-08-15T13:29:26.105233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GloveTextEmbedding:\n",
    "    def __init__(self, device: Optional[torch.device] = None):\n",
    "        self.model = SentenceTransformer(\n",
    "            \"sentence-transformers/average_word_embeddings_glove.6B.300d\",\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def __call__(self, sentences: List[str]) -> Tensor:\n",
    "        return torch.from_numpy(self.model.encode(sentences))"
   ],
   "id": "78e9b5ef45024fb3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:29:31.972459Z",
     "start_time": "2025-08-15T13:29:26.180401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = time.perf_counter()\n",
    "    yield lambda: time.perf_counter() - start\n",
    "\n",
    "def classification_metrics(y_true, y_pred, y_prob=None) -> Dict[str, float]:\n",
    "    out = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            out[\"roc_auc\"] = roc_auc_score(y_true, y_prob)\n",
    "        except Exception:\n",
    "            out[\"roc_auc\"] = np.nan\n",
    "    else:\n",
    "        out[\"roc_auc\"] = np.nan\n",
    "    return out\n",
    "\n",
    "def regression_metrics(y_true, y_pred, y_prob=None) -> Dict[str, float]:\n",
    "    # Accepts y_prob for compatibility, but ignores it\n",
    "    return {\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        \"mse\": mean_squared_error(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "dataset = get_dataset(DATASET)\n",
    "db = dataset.get_db()\n",
    "\n",
    "def to_pandas(table):\n",
    "    if hasattr(table, \"to_pandas\"):\n",
    "        return table.to_pandas()\n",
    "    if hasattr(table, \"df\"):\n",
    "        return table.df\n",
    "    raise ValueError(\"Unknown table type\")\n",
    "\n",
    "# Convert all tables to pandas DataFrames\n",
    "tables = {}\n",
    "for name in db.table_dict:\n",
    "    tables[name] = to_pandas(db.table_dict[name])\n",
    "\n",
    "for df in tables.values():\n",
    "    for col in df.columns:\n",
    "        if \"date\" in col.lower():\n",
    "            # 1) Standardize and coerce to UTC\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "            # 2) Fill missing with earliest valid timestamp\n",
    "            df[col] = df[col].fillna(df[col].min())\n",
    "            # 3) Extract calendar components\n",
    "            df[f\"{col}_year\"]    = df[col].dt.year\n",
    "            df[f\"{col}_month\"]   = df[col].dt.month\n",
    "            df[f\"{col}_day\"]     = df[col].dt.day\n",
    "            df[f\"{col}_weekday\"] = df[col].dt.weekday\n",
    "            df[f\"{col}_quarter\"] = df[col].dt.quarter\n",
    "            # 4) Binary indicators for edges of period\n",
    "            df[f\"{col}_is_month_start\"] = df[col].dt.is_month_start.astype(int)\n",
    "            df[f\"{col}_is_month_end\"]   = df[col].dt.is_month_end.astype(int)\n",
    "            df[f\"{col}_is_weekend\"]     = (df[col].dt.weekday >= 5).astype(int)\n",
    "            # 5) Cyclical encodings to preserve periodicity\n",
    "            df[f\"{col}_month_sin\"]   = np.sin(2 * np.pi * df[f\"{col}_month\"]   / 12)\n",
    "            df[f\"{col}_month_cos\"]   = np.cos(2 * np.pi * df[f\"{col}_month\"]   / 12)\n",
    "            df[f\"{col}_weekday_sin\"] = np.sin(2 * np.pi * df[f\"{col}_weekday\"] /  7)\n",
    "            df[f\"{col}_weekday_cos\"] = np.cos(2 * np.pi * df[f\"{col}_weekday\"] /  7)\n",
    "            # 6) Relative elapsed days since earliest date\n",
    "            df[f\"{col}_elapsed_days\"] = (df[col] - df[col].min()).dt.days\n",
    "            # Strip timezone so to_unix_time sees datetime64[ns]\n",
    "            df[col] = df[col].dt.tz_localize(None)\n",
    "# --- ADD: push processed tables back into db.table_dict ---\n",
    "for name, df in tables.items():\n",
    "    db.table_dict[name].df = df\n",
    "# --- END ADD ---\n",
    "\n",
    "# --- ADD THIS BLOCK: Build the hetero-temporal graph for GNN experiments ---\n",
    "col_to_stype_dict = get_stype_proposal(db)\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=GloveTextEmbedding(device=torch.device(DEVICE)), batch_size=256\n",
    ")\n",
    "data, col_stats_dict = make_pkey_fkey_graph(\n",
    "    db,\n",
    "    col_to_stype_dict=col_to_stype_dict,\n",
    "    text_embedder_cfg=text_embedder_cfg,\n",
    "    cache_dir=None,\n",
    ")"
   ],
   "id": "4da5c0d65181834d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /Users/michaelflppv/Library/Caches/relbench/rel-f1/db...\n",
      "Done in 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding raw data in mini-batch: 100%|██████████| 4/4 [00:00<00:00, 831.25it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 4/4 [00:00<00:00, 766.57it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 4/4 [00:00<00:00, 781.86it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 4/4 [00:00<00:00, 868.07it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 4/4 [00:00<00:00, 808.89it/s]\n",
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch_frame/data/stats.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ser = pd.to_datetime(ser, format=time_format)\n",
      "Embedding raw data in mini-batch: 100%|██████████| 4/4 [00:00<00:00, 603.32it/s]\n",
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch_frame/data/mapper.py:291: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ser = pd.to_datetime(ser, format=self.format, errors='coerce')\n",
      "Embedding raw data in mini-batch: 100%|██████████| 1/1 [00:00<00:00, 536.97it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 1/1 [00:00<00:00, 531.33it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 1/1 [00:00<00:00, 568.18it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 1/1 [00:00<00:00, 993.20it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 1/1 [00:00<00:00, 677.16it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 1/1 [00:00<00:00, 781.94it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 1/1 [00:00<00:00, 529.12it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:29:31.987848Z",
     "start_time": "2025-08-15T13:29:31.983042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Split fetch + X/y builders (dataset-agnostic) ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fetch_splits(dataset_name: str, task_name: str, download: bool = True):\n",
    "    task = get_task(dataset_name, task_name, download=download)\n",
    "    # keep original columns (tutorial shows mask_input_cols flag)\n",
    "    splits = {\n",
    "        split: task.get_table(split, mask_input_cols=False)\n",
    "        for split in (\"train\", \"val\", \"test\")\n",
    "    }\n",
    "    return task, splits\n",
    "\n",
    "def to_Xy(df: pd.DataFrame, target_col: str):\n",
    "    y = df[target_col].to_numpy()\n",
    "    X = df.drop(columns=[target_col])\n",
    "    return X, y\n",
    "\n",
    "# --- Naive merged table via one-hop FK→PK joins using dataset.get_db() ---\n",
    "# Uses only duck-typing on Table objects; falls back gracefully if metadata is missing.\n",
    "def _infer_pk(table_obj, df: pd.DataFrame):\n",
    "    # best-effort: check common attribute names first, then infer by uniqueness\n",
    "    for attr in (\"primary_key_col\", \"pkey\", \"pk\", \"primary_key\", \"id_col\"):\n",
    "        if hasattr(table_obj, attr):\n",
    "            cand = getattr(table_obj, attr)\n",
    "            if isinstance(cand, str) and cand in df.columns:\n",
    "                return cand\n",
    "    # fallback: take the first unique column if exists\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            if df[c].is_unique:\n",
    "                return c\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def denormalize_one_hop(dataset, base_df: pd.DataFrame):\n",
    "    try:\n",
    "        db = dataset.get_db()  # documented in README\n",
    "    except Exception:\n",
    "        return base_df  # cannot access DB; return base table\n",
    "\n",
    "    tables = getattr(db, \"tables\", None)\n",
    "    if not isinstance(tables, dict):\n",
    "        return base_df\n",
    "\n",
    "    # Build a map: PK column name -> (table_name, table_df, pk_col)\n",
    "    pkmap = {}\n",
    "    for name, tbl in tables.items():\n",
    "        df = getattr(tbl, \"df\", None)\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            pk = _infer_pk(tbl, df)\n",
    "            if pk and pk in df.columns:\n",
    "                pkmap.setdefault(pk, []).append((name, df, pk))\n",
    "\n",
    "    df_out = base_df.copy()\n",
    "    # For each column in base that matches a PK in the DB, left join the non-key attributes\n",
    "    for fk_col in list(df_out.columns):\n",
    "        if fk_col in pkmap:\n",
    "            for (tname, tdf, pk) in pkmap[fk_col]:\n",
    "                right = tdf.drop(columns=[pk], errors=\"ignore\").copy()\n",
    "                if right.empty:\n",
    "                    continue\n",
    "                # prefix joined columns to avoid collisions\n",
    "                right = right.add_prefix(f\"{tname}__\")\n",
    "                # attach the join key back (unprefixed) for merge\n",
    "                right[fk_col] = tdf[pk]\n",
    "                try:\n",
    "                    df_out = df_out.merge(right, on=fk_col, how=\"left\")\n",
    "                except Exception:\n",
    "                    # keep going; some keys may be non-joinable due to dtype issues\n",
    "                    pass\n",
    "    return df_out\n",
    "\n",
    "def build_single_table_frames(task, splits):\n",
    "    frames = {}\n",
    "    for split, table in splits.items():\n",
    "        df = table.df.copy()\n",
    "        # IMPORTANT: do not touch column masking/order; just drop the target to form X\n",
    "        X, y = to_Xy(df, task.target_col)\n",
    "        frames[split] = (X, y, df)  # keep original df for evaluation alignment\n",
    "    return frames\n",
    "\n",
    "def build_merged_table_frames(dataset, task, splits):\n",
    "    frames = {}\n",
    "    for split, table in splits.items():\n",
    "        base_df = table.df.copy()\n",
    "        merged_df = denormalize_one_hop(dataset, base_df)\n",
    "        X, y = to_Xy(merged_df, task.target_col)\n",
    "        frames[split] = (X, y, merged_df)\n",
    "    return frames\n"
   ],
   "id": "1cfbaca7c6d3b12c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:29:32.001609Z",
     "start_time": "2025-08-15T13:29:31.998989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- NEW CELL: skrub preprocessing helpers ---\n",
    "\n",
    "def build_tv():\n",
    "    \"\"\"\n",
    "    TableVectorizer turns mixed (numeric + categorical + text + datetime) columns\n",
    "    into a numeric feature matrix. We keep defaults to match the tutorials’ simplicity.\n",
    "    \"\"\"\n",
    "    return TableVectorizer()\n",
    "\n",
    "def fit_transform_splits(tv, X_train_df, X_val_df=None, X_test_df=None):\n",
    "    \"\"\"\n",
    "    Fit TV on train only; transform val/test. Cast to float32 for TabPFN.\n",
    "    \"\"\"\n",
    "    # Ensure no NaN or infinite values before vectorizing\n",
    "    X_train_df = X_train_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    if X_val_df is not None:\n",
    "        X_val_df = X_val_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    if X_test_df is not None:\n",
    "        X_test_df = X_test_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    Xt_train = tv.fit_transform(X_train_df).astype(np.float32)\n",
    "    Xt_val   = tv.transform(X_val_df).astype(np.float32) if X_val_df is not None else None\n",
    "    Xt_test  = tv.transform(X_test_df).astype(np.float32) if X_test_df is not None else None\n",
    "    return Xt_train, Xt_val, Xt_test, tv\n",
    "\n",
    "def _get_df(table):\n",
    "    # Your tutorial-style accessor: works whether object has `.df` or `.to_pandas()`\n",
    "    if hasattr(table, \"df\"):\n",
    "        return table.df\n",
    "    if hasattr(table, \"to_pandas\"):\n",
    "        return table.to_pandas()\n",
    "    raise ValueError(\"Unknown table type for conversion to DataFrame.\")\n"
   ],
   "id": "2ec47853dd80bd98",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:29:32.014267Z",
     "start_time": "2025-08-15T13:29:32.011578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Vectorization wrapper (version-safe for skrub/dirty_cat) ---\n",
    "# Place this where your previous TableVectorizer/vectorize_splits block was.\n",
    "\n",
    "def _make_table_vectorizer():\n",
    "    sig = inspect.signature(TableVectorizer.__init__)\n",
    "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
    "\n",
    "    tv_kwargs = {}\n",
    "\n",
    "    # Only set kwargs that actually exist in the installed version\n",
    "    if \"cardinality_threshold\" in allowed:\n",
    "        tv_kwargs[\"cardinality_threshold\"] = globals().get(\"CARDINALITY_THRESHOLD\", 1000)\n",
    "\n",
    "    # Some versions expose this; others don't — guard it\n",
    "    if \"high_cardinality_transformer\" in allowed:\n",
    "        tv_kwargs[\"high_cardinality_transformer\"] = globals().get(\"HIGH_CARD_TRANSFORMER\", \"hashing\")\n",
    "\n",
    "    # Optional knobs if you happen to define them globally and the version supports them\n",
    "    if \"text_separator\" in allowed and \"TEXT_SEPARATOR\" in globals():\n",
    "        tv_kwargs[\"text_separator\"] = globals()[\"TEXT_SEPARATOR\"]\n",
    "    if \"numerical_transformer\" in allowed and \"NUMERICAL_TRANSFORMER\" in globals():\n",
    "        tv_kwargs[\"numerical_transformer\"] = globals()[\"NUMERICAL_TRANSFORMER\"]\n",
    "    if \"categorical_transformer\" in allowed and \"CATEGORICAL_TRANSFORMER\" in globals():\n",
    "        tv_kwargs[\"categorical_transformer\"] = globals()[\"CATEGORICAL_TRANSFORMER\"]\n",
    "\n",
    "    return TableVectorizer(**tv_kwargs)\n",
    "\n",
    "def _to_dense(X):\n",
    "    try:\n",
    "        # scipy sparse matrices have .toarray()\n",
    "        return X.toarray() if hasattr(X, \"toarray\") else X\n",
    "    except Exception:\n",
    "        return X\n",
    "\n",
    "def vectorize_splits(X_train, X_val, X_test):\n",
    "    tv = _make_table_vectorizer()\n",
    "    Xt = _to_dense(tv.fit_transform(X_train))\n",
    "    Xv = _to_dense(tv.transform(X_val))\n",
    "    Xs = _to_dense(tv.transform(X_test))\n",
    "    return tv, Xt, Xv, Xs\n"
   ],
   "id": "ffb43a27c3ac51cb",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:29:32.028619Z",
     "start_time": "2025-08-15T13:29:32.023973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Generic TabPFN experiment for a given task & mode ('single' or 'merged') ---\n",
    "import math\n",
    "from typing import Dict, Any\n",
    "\n",
    "try:\n",
    "    from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "except Exception:\n",
    "    # Older TabPFN installs may not expose the Regressor; fall back to classifier-only path\n",
    "    TabPFNRegressor = None\n",
    "\n",
    "SEED   = globals().get(\"SEED\", 42)\n",
    "DEVICE = globals().get(\"DEVICE\", \"cuda\" if hasattr(np, \"__cuda_array_interface__\") else \"cpu\")\n",
    "N_ENSEMBLE = globals().get(\"N_ENSEMBLE\", 16)\n",
    "TABPFN_MAX = globals().get(\"TABPFN_MAX\", 1000)  # hard ceiling for TabPFN\n",
    "\n",
    "def _subsample(X, y, cap=TABPFN_MAX, seed=SEED):\n",
    "    if len(X) <= cap:\n",
    "        return X, y, np.arange(len(X))\n",
    "    idx = np.random.RandomState(seed).choice(len(X), size=cap, replace=False)\n",
    "    if hasattr(X, \"iloc\"):\n",
    "        Xs = X.iloc[idx]\n",
    "    else:\n",
    "        Xs = X[idx]\n",
    "    ys = y[idx]\n",
    "    return Xs, ys, idx\n",
    "\n",
    "def _fit_tabpfn(task, Xt, yt):\n",
    "    if task.task_type == TaskType.REGRESSION and TabPFNRegressor is not None:\n",
    "        model = TabPFNRegressor(\n",
    "            device=DEVICE,\n",
    "            #n_estimators=int(N_ENSEMBLE),\n",
    "        )\n",
    "    else:\n",
    "        model = TabPFNClassifier(\n",
    "            device=DEVICE,\n",
    "            #n_estimators=int(N_ENSEMBLE),\n",
    "        )\n",
    "    model.fit(Xt, yt)\n",
    "    return model\n",
    "\n",
    "def _predict_for_task(task, model, X):\n",
    "    # align with RelBench evaluators: AUROC expects probabilities for the positive class\n",
    "    if task.task_type == TaskType.REGRESSION:\n",
    "        return model.predict(X)\n",
    "    proba = model.predict_proba(X)\n",
    "    if task.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "        return proba[:, 1]\n",
    "    else:\n",
    "        # multiclass/multilabel: pass full probability matrix\n",
    "        return proba\n",
    "\n",
    "def run_tabpfn_on_task(dataset_name: str, task_name: str, mode: str = \"single\") -> Dict[str, Any]:\n",
    "    dataset = get_dataset(dataset_name, download=DOWNLOAD)\n",
    "    task, splits = fetch_splits(dataset_name, task_name, download=DOWNLOAD)\n",
    "\n",
    "    if mode == \"single\":\n",
    "        frames = build_single_table_frames(task, splits)\n",
    "    elif mode == \"merged\":\n",
    "        frames = build_merged_table_frames(dataset, task, splits)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'single' or 'merged'\")\n",
    "\n",
    "    (Xtr, ytr, _dftr) = frames[\"train\"]\n",
    "    (Xva, yva, dfva)  = frames[\"val\"]\n",
    "    (Xte, yte, dfte)  = frames[\"test\"]\n",
    "\n",
    "    # Vectorize\n",
    "    tv, Xt, Xv, Xs = vectorize_splits(Xtr, Xva, Xte)\n",
    "\n",
    "    # Respect TabPFN's sample cap\n",
    "    Xt_cap, yt_cap, _ = _subsample(Xt, ytr, cap=TABPFN_MAX, seed=SEED)\n",
    "\n",
    "    # Fit\n",
    "    model = _fit_tabpfn(task, Xt_cap, yt_cap)\n",
    "\n",
    "    # Predict & Evaluate with RelBench evaluators\n",
    "    val_pred  = _predict_for_task(task, model, Xv)\n",
    "    test_pred = _predict_for_task(task, model, Xs)\n",
    "\n",
    "    val_metrics  = task.evaluate(val_pred,  splits[\"val\"])   # documented API\n",
    "    test_metrics = task.evaluate(test_pred, splits[\"test\"])  # documented API\n",
    "\n",
    "    out = {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"task\": task_name,\n",
    "        \"mode\": mode,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"n_train_used\": len(Xt_cap),\n",
    "        \"n_train_total\": len(Xt),\n",
    "        \"n_val\": len(Xv),\n",
    "        \"n_test\": len(Xs),\n",
    "    }\n",
    "    return out\n"
   ],
   "id": "5c53f649ce01b948",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:29:32.067198Z",
     "start_time": "2025-08-15T13:29:32.038073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create features for drivers based on their past performance\n",
    "# This function creates features like average position, points, DNF rate, and average laps\n",
    "def engineer_driver_features():\n",
    "    # Extract the race dates and results\n",
    "    results = tables[\"results\"].merge(\n",
    "        tables[\"races\"][[\"raceId\", \"date\"]],\n",
    "        on=\"raceId\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    print(\"results columns after merge:\", results.columns.tolist())\n",
    "    print(\"Number of rows after merge:\", results.shape[0])\n",
    "    results = results.dropna(subset=[\"driverId\", \"date_y\"])\n",
    "    print(\"Number of rows after dropna:\", results.shape[0])\n",
    "\n",
    "    # Create a did not finish (DNF) flag as indicator for future race outcomes\n",
    "    results[\"dnf_flag\"] = (~results[\"positionOrder\"].isna()).astype(int)\n",
    "    # For each driver, calculate the average position, points, DNF rate, and average laps\n",
    "    # Only the information before the current race is used\n",
    "    feats = results.groupby(\"driverId\").expanding().agg({\n",
    "        \"positionOrder\": \"mean\",\n",
    "        \"points\": \"mean\",\n",
    "        \"dnf_flag\": \"mean\",\n",
    "        \"laps\": \"mean\"\n",
    "    }).reset_index()\n",
    "    feats = feats.rename(columns={\n",
    "        \"positionOrder\": \"avg_position\",\n",
    "        \"points\": \"avg_points\",\n",
    "        \"dnf_flag\": \"dnf_rate\",\n",
    "        \"laps\": \"avg_laps\"\n",
    "    })\n",
    "    feats[\"date\"] = results[\"date_y\"].values\n",
    "    return feats\n",
    "\n",
    "driver_feats = engineer_driver_features()"
   ],
   "id": "de1f9fe2a4bd02ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results columns after merge: ['resultId', 'raceId', 'driverId', 'constructorId', 'number', 'grid', 'position', 'positionOrder', 'points', 'laps', 'milliseconds', 'fastestLap', 'rank', 'statusId', 'date_x', 'date_year', 'date_month', 'date_day', 'date_weekday', 'date_quarter', 'date_is_month_start', 'date_is_month_end', 'date_is_weekend', 'date_month_sin', 'date_month_cos', 'date_weekday_sin', 'date_weekday_cos', 'date_elapsed_days', 'date_y']\n",
      "Number of rows after merge: 20323\n",
      "Number of rows after dropna: 20323\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:29:32.148063Z",
     "start_time": "2025-08-15T13:29:32.077891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleDict\n",
    "from torch_frame.data.stats import StatType\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import MLP\n",
    "from torch_geometric.typing import NodeType\n",
    "\n",
    "from relbench.modeling.nn import HeteroEncoder, HeteroGraphSAGE, HeteroTemporalEncoder\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: HeteroData,\n",
    "        col_stats_dict: Dict[str, Dict[str, Dict[StatType, Any]]],\n",
    "        num_layers: int,\n",
    "        channels: int,\n",
    "        out_channels: int,\n",
    "        aggr: str,\n",
    "        norm: str,\n",
    "        # List of node types to add shallow embeddings to input\n",
    "        shallow_list: List[NodeType] = [],\n",
    "        # ID awareness\n",
    "        id_awareness: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = HeteroEncoder(\n",
    "            channels=channels,\n",
    "            node_to_col_names_dict={\n",
    "                node_type: data[node_type].tf.col_names_dict\n",
    "                for node_type in data.node_types\n",
    "            },\n",
    "            node_to_col_stats=col_stats_dict,\n",
    "        )\n",
    "        self.temporal_encoder = HeteroTemporalEncoder(\n",
    "            node_types=[\n",
    "                node_type for node_type in data.node_types if \"time\" in data[node_type]\n",
    "            ],\n",
    "            channels=channels,\n",
    "        )\n",
    "        self.gnn = HeteroGraphSAGE(\n",
    "            node_types=data.node_types,\n",
    "            edge_types=data.edge_types,\n",
    "            channels=channels,\n",
    "            aggr=aggr,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.head = MLP(\n",
    "            channels,\n",
    "            out_channels=out_channels,\n",
    "            norm=norm,\n",
    "            num_layers=1,\n",
    "        )\n",
    "        self.embedding_dict = ModuleDict(\n",
    "            {\n",
    "                node: Embedding(data.num_nodes_dict[node], channels)\n",
    "                for node in shallow_list\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.id_awareness_emb = None\n",
    "        if id_awareness:\n",
    "            self.id_awareness_emb = torch.nn.Embedding(1, channels)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.encoder.reset_parameters()\n",
    "        self.temporal_encoder.reset_parameters()\n",
    "        self.gnn.reset_parameters()\n",
    "        self.head.reset_parameters()\n",
    "        for embedding in self.embedding_dict.values():\n",
    "            torch.nn.init.normal_(embedding.weight, std=0.1)\n",
    "        if self.id_awareness_emb is not None:\n",
    "            self.id_awareness_emb.reset_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch: HeteroData,\n",
    "        entity_table: NodeType,\n",
    "    ) -> Tensor:\n",
    "        seed_time = batch[entity_table].seed_time\n",
    "        x_dict = self.encoder(batch.tf_dict)\n",
    "\n",
    "        rel_time_dict = self.temporal_encoder(\n",
    "            seed_time, batch.time_dict, batch.batch_dict\n",
    "        )\n",
    "\n",
    "        for node_type, rel_time in rel_time_dict.items():\n",
    "            x_dict[node_type] = x_dict[node_type] + rel_time\n",
    "\n",
    "        for node_type, embedding in self.embedding_dict.items():\n",
    "            x_dict[node_type] = x_dict[node_type] + embedding(batch[node_type].n_id)\n",
    "\n",
    "        x_dict = self.gnn(\n",
    "            x_dict,\n",
    "            batch.edge_index_dict,\n",
    "            batch.num_sampled_nodes_dict,\n",
    "            batch.num_sampled_edges_dict,\n",
    "        )\n",
    "\n",
    "        return self.head(x_dict[entity_table][: seed_time.size(0)])\n",
    "\n",
    "    def forward_dst_readout(\n",
    "        self,\n",
    "        batch: HeteroData,\n",
    "        entity_table: NodeType,\n",
    "        dst_table: NodeType,\n",
    "    ) -> Tensor:\n",
    "        if self.id_awareness_emb is None:\n",
    "            raise RuntimeError(\n",
    "                \"id_awareness must be set True to use forward_dst_readout\"\n",
    "            )\n",
    "        seed_time = batch[entity_table].seed_time\n",
    "        x_dict = self.encoder(batch.tf_dict)\n",
    "        # Add ID-awareness to the root node\n",
    "        x_dict[entity_table][: seed_time.size(0)] += self.id_awareness_emb.weight\n",
    "\n",
    "        rel_time_dict = self.temporal_encoder(\n",
    "            seed_time, batch.time_dict, batch.batch_dict\n",
    "        )\n",
    "\n",
    "        for node_type, rel_time in rel_time_dict.items():\n",
    "            x_dict[node_type] = x_dict[node_type] + rel_time\n",
    "\n",
    "        for node_type, embedding in self.embedding_dict.items():\n",
    "            x_dict[node_type] = x_dict[node_type] + embedding(batch[node_type].n_id)\n",
    "\n",
    "        x_dict = self.gnn(\n",
    "            x_dict,\n",
    "            batch.edge_index_dict,\n",
    "        )\n",
    "\n",
    "        return self.head(x_dict[dst_table])\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    data=data,\n",
    "    col_stats_dict=col_stats_dict,\n",
    "    num_layers=2,\n",
    "    channels=128,\n",
    "    out_channels=1,\n",
    "    aggr=\"sum\",\n",
    "    norm=\"batch_norm\",\n",
    ").to(DEVICE)"
   ],
   "id": "b42b4743087c99f0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:29:32.161017Z",
     "start_time": "2025-08-15T13:29:32.158946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- MODIFY: make test accept entity_table ---\n",
    "@torch.no_grad()\n",
    "def test(loader: NeighborLoader, entity_table: NodeType) -> np.ndarray:\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        pred = model(batch, entity_table)\n",
    "        pred = pred.view(-1) if pred.dim() > 1 and pred.size(1) == 1 else pred\n",
    "        pred_list.append(pred.detach().cpu())\n",
    "    return torch.cat(pred_list, dim=0).numpy()\n",
    "# --- END MODIFY ---"
   ],
   "id": "276a0d6b36910e13",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:31:30.434935Z",
     "start_time": "2025-08-15T13:29:32.173234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Orchestrator: run all discovered tasks in single & merged modes (robust) ---\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "MODES = globals().get(\"MODES\", [\"single\", \"merged\"])\n",
    "\n",
    "records = []\n",
    "failures = []\n",
    "\n",
    "for task_name in TASKS:\n",
    "    for mode in MODES:\n",
    "        try:\n",
    "            res = run_tabpfn_on_task(DATASET, task_name, mode=mode)\n",
    "            row = {\n",
    "                \"dataset\": res.get(\"dataset\", DATASET),\n",
    "                \"task\": res.get(\"task\", task_name),\n",
    "                \"mode\": res.get(\"mode\", mode),\n",
    "                \"n_train_used\": res.get(\"n_train_used\", None),\n",
    "                \"n_train_total\": res.get(\"n_train_total\", None),\n",
    "                \"n_val\": res.get(\"n_val\", None),\n",
    "                \"n_test\": res.get(\"n_test\", None),\n",
    "            }\n",
    "            # flatten metrics dicts if present\n",
    "            for k, v in (res.get(\"val_metrics\") or {}).items():\n",
    "                row[f\"val_{k}\"] = v\n",
    "            for k, v in (res.get(\"test_metrics\") or {}).items():\n",
    "                row[f\"test_{k}\"] = v\n",
    "            records.append(row)\n",
    "        except Exception as e:\n",
    "            msg = f\"[{DATASET} | {task_name} | {mode}] failed: {e!s}\"\n",
    "            print(msg)\n",
    "            failures.append(msg)\n",
    "\n",
    "results_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "if results_df.empty:\n",
    "    print(\"No successful runs were recorded. Check the failure messages above.\")\n",
    "    # Create an empty, well-formed frame so downstream plotting code doesn't crash\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=[\"dataset\", \"task\", \"mode\", \"n_train_used\", \"n_train_total\", \"n_val\", \"n_test\"]\n",
    "    )\n",
    "else:\n",
    "    # Ensure required sort keys exist even if some rows missed them\n",
    "    for col in [\"task\", \"mode\"]:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = pd.NA\n",
    "    # Sort only by the columns that exist to avoid KeyError\n",
    "    sort_keys = [c for c in [\"task\", \"mode\"] if c in results_df.columns]\n",
    "    if sort_keys:\n",
    "        results_df = results_df.sort_values(sort_keys)\n",
    "\n",
    "display(results_df)\n"
   ],
   "id": "be6242da806a8050",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/regressor.py:494: UserWarning: Running on CPU with more than 200 samples may be slow.\n",
      "Consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client\n",
      "  check_cpu_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rel-f1 | driver-position | single] failed: got an unexpected keyword argument 'squared'\n",
      "Loading Database object from /Users/michaelflppv/Library/Caches/relbench/rel-f1/db...\n",
      "Done in 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/regressor.py:494: UserWarning: Running on CPU with more than 200 samples may be slow.\n",
      "Consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client\n",
      "  check_cpu_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rel-f1 | driver-position | merged] failed: got an unexpected keyword argument 'squared'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/classifier.py:462: UserWarning: Running on CPU with more than 200 samples may be slow.\n",
      "Consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client\n",
      "  check_cpu_warning(\n",
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/classifier.py:462: UserWarning: Running on CPU with more than 200 samples may be slow.\n",
      "Consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client\n",
      "  check_cpu_warning(\n",
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/classifier.py:462: UserWarning: Running on CPU with more than 200 samples may be slow.\n",
      "Consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client\n",
      "  check_cpu_warning(\n",
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/classifier.py:462: UserWarning: Running on CPU with more than 200 samples may be slow.\n",
      "Consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client\n",
      "  check_cpu_warning(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  dataset         task    mode  n_train_used  n_train_total  n_val  n_test  \\\n",
       "1  rel-f1   driver-dnf  merged          1000          11411    566     702   \n",
       "0  rel-f1   driver-dnf  single          1000          11411    566     702   \n",
       "3  rel-f1  driver-top3  merged          1000           1353    588     726   \n",
       "2  rel-f1  driver-top3  single          1000           1353    588     726   \n",
       "\n",
       "   val_average_precision  val_accuracy    val_f1  val_roc_auc  \\\n",
       "1               0.762023      0.779152  0.875869     0.467338   \n",
       "0               0.762023      0.779152  0.875869     0.467338   \n",
       "3               0.262874      0.780612  0.145695     0.531705   \n",
       "2               0.262874      0.780612  0.145695     0.531705   \n",
       "\n",
       "   test_average_precision  test_accuracy   test_f1  test_roc_auc  \n",
       "1                0.707345       0.705128  0.827068      0.429278  \n",
       "0                0.707345       0.705128  0.827068      0.429278  \n",
       "3                0.206704       0.792011  0.038217      0.584161  \n",
       "2                0.206704       0.792011  0.038217      0.584161  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>task</th>\n",
       "      <th>mode</th>\n",
       "      <th>n_train_used</th>\n",
       "      <th>n_train_total</th>\n",
       "      <th>n_val</th>\n",
       "      <th>n_test</th>\n",
       "      <th>val_average_precision</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_roc_auc</th>\n",
       "      <th>test_average_precision</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rel-f1</td>\n",
       "      <td>driver-dnf</td>\n",
       "      <td>merged</td>\n",
       "      <td>1000</td>\n",
       "      <td>11411</td>\n",
       "      <td>566</td>\n",
       "      <td>702</td>\n",
       "      <td>0.762023</td>\n",
       "      <td>0.779152</td>\n",
       "      <td>0.875869</td>\n",
       "      <td>0.467338</td>\n",
       "      <td>0.707345</td>\n",
       "      <td>0.705128</td>\n",
       "      <td>0.827068</td>\n",
       "      <td>0.429278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rel-f1</td>\n",
       "      <td>driver-dnf</td>\n",
       "      <td>single</td>\n",
       "      <td>1000</td>\n",
       "      <td>11411</td>\n",
       "      <td>566</td>\n",
       "      <td>702</td>\n",
       "      <td>0.762023</td>\n",
       "      <td>0.779152</td>\n",
       "      <td>0.875869</td>\n",
       "      <td>0.467338</td>\n",
       "      <td>0.707345</td>\n",
       "      <td>0.705128</td>\n",
       "      <td>0.827068</td>\n",
       "      <td>0.429278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rel-f1</td>\n",
       "      <td>driver-top3</td>\n",
       "      <td>merged</td>\n",
       "      <td>1000</td>\n",
       "      <td>1353</td>\n",
       "      <td>588</td>\n",
       "      <td>726</td>\n",
       "      <td>0.262874</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.145695</td>\n",
       "      <td>0.531705</td>\n",
       "      <td>0.206704</td>\n",
       "      <td>0.792011</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>0.584161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rel-f1</td>\n",
       "      <td>driver-top3</td>\n",
       "      <td>single</td>\n",
       "      <td>1000</td>\n",
       "      <td>1353</td>\n",
       "      <td>588</td>\n",
       "      <td>726</td>\n",
       "      <td>0.262874</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.145695</td>\n",
       "      <td>0.531705</td>\n",
       "      <td>0.206704</td>\n",
       "      <td>0.792011</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>0.584161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:31:30.458011Z",
     "start_time": "2025-08-15T13:31:30.454710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Persist results to CSV ---\n",
    "\n",
    "out_dir = globals().get(\"OUT_DIR\", \"outputs\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "csv_name = f\"tabpfn_{DATASET}_{timestamp}.csv\"\n",
    "csv_path = os.path.join(out_dir, csv_name)\n",
    "\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved results to: {csv_path}\")\n"
   ],
   "id": "aa929637db7d0d54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: outputs/tabpfn_rel-f1_20250815-153130.csv\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
