{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **TabPFN Relational Benchmark**\n",
    "This notebook benchmarks the performance of TabPFN models on datasets from RelBench in two scenarios:\n",
    "1. **Single Table** – Using only the target entity table.\n",
    "2. **Merged Table** – Using a naively denormalized table obtained by joining related tables.\n",
    "\n",
    "It automates dataset loading, preprocessing (including date feature engineering), vectorization, model training, prediction, and evaluation for all compatible tasks within a chosen RelBench dataset. The results allow comparing model performance between single-table and merged-table configurations.\n"
   ],
   "id": "7ab5a9bf5466de0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "id": "ac93e97c56d4b905"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import time\n",
    "import inspect\n",
    "from contextlib import contextmanager\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "\n",
    "# --- PyTorch and PyTorch Geometric ---\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleDict\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import MLP\n",
    "from torch_geometric.typing import NodeType\n",
    "\n",
    "# --- Torch Frame ---\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from torch_frame.data.stats import StatType\n",
    "\n",
    "# --- Skrub and Sentence Transformers ---\n",
    "from skrub import TableVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- RelBench ---\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task, get_task_names\n",
    "from relbench.base import TaskType\n",
    "import relbench.metrics\n",
    "from relbench.modeling.utils import get_stype_proposal\n",
    "from relbench.modeling.graph import make_pkey_fkey_graph\n",
    "from relbench.modeling.nn import HeteroEncoder, HeteroGraphSAGE, HeteroTemporalEncoder\n",
    "\n",
    "# --- TabPFN ---\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n"
   ],
   "id": "57624fe02c838842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set Global Configuration",
   "id": "7154198e21fe2a01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Device preference\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Define global dataset variable (any available dataset from relbench.datasets)\n",
    "# \"rel-f1\" is the default, but can be overridden by setting DATASET variable\n",
    "DATASET = \"rel-avito\"\n",
    "\n",
    "# Global configuration variables with defaults\n",
    "SEED   = globals().get(\"SEED\", 42)\n",
    "N_ESTIMATORS = globals().get(\"N_ESTIMATORS\", 16) # number of TabPFN estimators\n",
    "TABPFN_MAX = globals().get(\"TABPFN_MAX\", 10000)  # hard ceiling for TabPFN"
   ],
   "id": "c5959e09ecd4c20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Notebook Configuration and Dataset Selection\n",
    "\n",
    "\n",
    "Sets the dataset name (`DATASET`) and download flag (`DOWNLOAD`), then discovers all available tasks for the selected dataset using RelBench’s APIs. Filters tasks to only those compatible with TabPFN (classification and regression).\n"
   ],
   "id": "a86eaa89573fdf63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reuse existing config if present, otherwise set defaults\n",
    "DATASET = globals().get(\"DATASET\", \"rel-f1\")\n",
    "DOWNLOAD = globals().get(\"DOWNLOAD\", True)\n",
    "\n",
    "# Discover tasks and keep only entity-level cls/reg tasks TabPFN can handle\n",
    "def _is_tabpfn_friendly(task):\n",
    "    return task.task_type in (\n",
    "        TaskType.BINARY_CLASSIFICATION,\n",
    "        TaskType.MULTICLASS_CLASSIFICATION,\n",
    "        TaskType.MULTILABEL_CLASSIFICATION,\n",
    "        TaskType.REGRESSION,\n",
    "    )\n",
    "\n",
    "_all = get_task_names(DATASET)  # shown in tutorials\n",
    "TASKS = []\n",
    "for tname in _all:\n",
    "    try:\n",
    "        t = get_task(DATASET, tname, download=DOWNLOAD)\n",
    "        if _is_tabpfn_friendly(t):\n",
    "            TASKS.append(tname)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {tname}: {e!s}\")\n",
    "\n",
    "print(f\"{DATASET}: {len(TASKS)} TabPFN-friendly tasks -> {TASKS}\")\n"
   ],
   "id": "d94075cf88bf6806",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Patch RelBench Metrics (Optional)",
   "id": "affc15acb6a0f8aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Patch relbench.metrics.skm.mean_squared_error to local mean_squared_error\n",
    "relbench.metrics.skm.mean_squared_error = mean_squared_error\n",
    "\n",
    "def patched_rmse(true, pred):\n",
    "    if \"squared\" in inspect.signature(mean_squared_error).parameters:\n",
    "        return mean_squared_error(true, pred, squared=False)\n",
    "    else:\n",
    "        return np.sqrt(mean_squared_error(true, pred))\n",
    "\n",
    "relbench.metrics.rmse = patched_rmse"
   ],
   "id": "46fcd0d47afb30c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Text Embedding Configuration",
   "id": "5629815fd3214b7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GloveTextEmbedding:\n",
    "    def __init__(self, device: Optional[torch.device] = None):\n",
    "        self.model = SentenceTransformer(\n",
    "            \"sentence-transformers/average_word_embeddings_glove.6B.300d\",\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def __call__(self, sentences: List[str]) -> Tensor:\n",
    "        return torch.from_numpy(self.model.encode(sentences))"
   ],
   "id": "78e9b5ef45024fb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Utility Functions for Metrics and Timing\n",
   "id": "bfba563c94cd29a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = time.perf_counter()\n",
    "    yield lambda: time.perf_counter() - start\n",
    "\n",
    "def classification_metrics(y_true, y_pred, y_prob=None) -> Dict[str, float]:\n",
    "    out = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"roc_auc\": np.nan,\n",
    "    }\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            out[\"roc_auc\"] = roc_auc_score(y_true, y_prob)\n",
    "        except Exception:\n",
    "            out[\"roc_auc\"] = np.nan\n",
    "    else:\n",
    "        out[\"roc_auc\"] = np.nan\n",
    "    return out\n",
    "\n",
    "def regression_metrics(y_true, y_pred, y_prob=None) -> Dict[str, float]:\n",
    "    # Accepts y_prob for compatibility, but ignores it\n",
    "    return {\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        \"mse\": mean_squared_error(y_true, y_pred),\n",
    "    }"
   ],
   "id": "4da5c0d65181834d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Functions for Dataset Loading and Table Processing\n",
   "id": "2d748a1a5dea1d28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Date Feature Engineering and Optional Graph Build\n",
    "\n",
    "Processes all tables in the dataset to detect and parse date columns, replacing missing values and generating engineered date-related features (e.g., year, month, weekday, cyclical encodings).\n",
    "Also includes a step to construct a hetero-temporal graph for GNN experiments using text embeddings.\n"
   ],
   "id": "76f494f170e324f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = get_dataset(DATASET)\n",
    "db = dataset.get_db()\n",
    "\n",
    "def to_pandas(table):\n",
    "    if hasattr(table, \"to_pandas\"):\n",
    "        return table.to_pandas()\n",
    "    if hasattr(table, \"df\"):\n",
    "        return table.df\n",
    "    raise ValueError(\"Unknown table type\")\n",
    "\n",
    "# Convert all tables to pandas DataFrames\n",
    "tables = {name: to_pandas(tbl) for name, tbl in db.table_dict.items()}\n",
    "\n",
    "# Batch‐collect date features and concat at once\n",
    "for name, df in tables.items():\n",
    "    date_cols = [col for col in df.columns if \"date\" in col.lower()]\n",
    "    if not date_cols:\n",
    "        continue\n",
    "    feats_list = []\n",
    "    for col in date_cols:\n",
    "        dt = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "        dt_filled = dt.fillna(dt.min())\n",
    "        df[col] = dt_filled.dt.tz_localize(None)\n",
    "        feats = pd.DataFrame({\n",
    "            f\"{col}_year\": dt_filled.dt.year,\n",
    "            f\"{col}_month\": dt_filled.dt.month,\n",
    "            f\"{col}_day\": dt_filled.dt.day,\n",
    "            f\"{col}_weekday\": dt_filled.dt.weekday,\n",
    "            f\"{col}_quarter\": dt_filled.dt.quarter,\n",
    "            f\"{col}_is_month_start\": dt_filled.dt.is_month_start.astype(int),\n",
    "            f\"{col}_is_month_end\": dt_filled.dt.is_month_end.astype(int),\n",
    "            f\"{col}_is_weekend\": (dt_filled.dt.weekday >= 5).astype(int),\n",
    "            f\"{col}_month_sin\": np.sin(2 * np.pi * dt_filled.dt.month / 12),\n",
    "            f\"{col}_month_cos\": np.cos(2 * np.pi * dt_filled.dt.month / 12),\n",
    "            f\"{col}_weekday_sin\": np.sin(2 * np.pi * dt_filled.dt.weekday / 7),\n",
    "            f\"{col}_weekday_cos\": np.cos(2 * np.pi * dt_filled.dt.weekday / 7),\n",
    "            f\"{col}_elapsed_days\": (dt_filled - dt_filled.min()).dt.days,\n",
    "        }, index=df.index)\n",
    "        feats_list.append(feats)\n",
    "    if feats_list:\n",
    "        df = pd.concat([df] + feats_list, axis=1)\n",
    "    tables[name] = df\n",
    "\n",
    "# --- ADD: push processed tables back into db.table_dict ---\n",
    "for name, df in tables.items():\n",
    "    db.table_dict[name].df = df\n",
    "# --- END ADD ---\n",
    "\n",
    "# --- ADD THIS BLOCK: Build the hetero-temporal graph for GNN experiments ---\n",
    "col_to_stype_dict = get_stype_proposal(db)\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=GloveTextEmbedding(device=torch.device(DEVICE)), batch_size=256\n",
    ")\n",
    "data, col_stats_dict = make_pkey_fkey_graph(\n",
    "    db,\n",
    "    col_to_stype_dict=col_to_stype_dict,\n",
    "    text_embedder_cfg=text_embedder_cfg,\n",
    "    cache_dir=None,\n",
    ")"
   ],
   "id": "d37c8ff4b032ae52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fetch Dataset Splits\n",
    "\n",
    "Utility functions to load a task’s splits (`train`, `val`, `test`), convert them to pandas DataFrames, and extract features (`X`) and targets (`y`). Includes functions to build:\n",
    "- **Single-table frames** directly from the target entity table.\n",
    "- **Merged-table frames** by performing a one-hop foreign key → primary key join to denormalize data.\n"
   ],
   "id": "1a35d633effdd263"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fetch_splits(dataset_name: str, task_name: str, download: bool = True):\n",
    "    task = get_task(dataset_name, task_name, download=download)\n",
    "    # keep original columns (tutorial shows mask_input_cols flag)\n",
    "    splits = {\n",
    "        split: task.get_table(split, mask_input_cols=False)\n",
    "        for split in (\"train\", \"val\", \"test\")\n",
    "    }\n",
    "    return task, splits\n",
    "\n",
    "def to_Xy(df: pd.DataFrame, target_col: str):\n",
    "    y = df[target_col].to_numpy()\n",
    "    X = df.drop(columns=[target_col])\n",
    "    return X, y\n",
    "\n",
    "# --- Naive merged table via one-hop FK→PK joins using dataset.get_db() ---\n",
    "# Uses only duck-typing on Table objects; falls back gracefully if metadata is missing.\n",
    "def _infer_pk(table_obj, df: pd.DataFrame):\n",
    "    # best-effort: check common attribute names first, then infer by uniqueness\n",
    "    for attr in (\"primary_key_col\", \"pkey\", \"pk\", \"primary_key\", \"id_col\"):\n",
    "        if hasattr(table_obj, attr):\n",
    "            cand = getattr(table_obj, attr)\n",
    "            if isinstance(cand, str) and cand in df.columns:\n",
    "                return cand\n",
    "    # fallback: take the first unique column if exists\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            if df[c].is_unique:\n",
    "                return c\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def denormalize_one_hop(dataset, base_df: pd.DataFrame):\n",
    "    try:\n",
    "        db = dataset.get_db()  # documented in README\n",
    "    except Exception:\n",
    "        return base_df  # cannot access DB; return base table\n",
    "\n",
    "    tables = getattr(db, \"tables\", None)\n",
    "    if not isinstance(tables, dict):\n",
    "        return base_df\n",
    "\n",
    "    # Build a map: PK column name -> (table_name, table_df, pk_col)\n",
    "    pkmap = {}\n",
    "    for name, tbl in tables.items():\n",
    "        df = getattr(tbl, \"df\", None)\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            pk = _infer_pk(tbl, df)\n",
    "            if pk and pk in df.columns:\n",
    "                pkmap.setdefault(pk, []).append((name, df, pk))\n",
    "\n",
    "    df_out = base_df.copy()\n",
    "    # For each column in base that matches a PK in the DB, left join the non-key attributes\n",
    "    for fk_col in list(df_out.columns):\n",
    "        if fk_col in pkmap:\n",
    "            for (tname, tdf, pk) in pkmap[fk_col]:\n",
    "                right = tdf.drop(columns=[pk], errors=\"ignore\").copy()\n",
    "                if right.empty:\n",
    "                    continue\n",
    "                # prefix joined columns to avoid collisions\n",
    "                right = right.add_prefix(f\"{tname}__\")\n",
    "                # attach the join key back (unprefixed) for merge\n",
    "                right[fk_col] = tdf[pk]\n",
    "                try:\n",
    "                    df_out = df_out.merge(right, on=fk_col, how=\"left\")\n",
    "                except Exception:\n",
    "                    # keep going; some keys may be non-joinable due to dtype issues\n",
    "                    pass\n",
    "    return df_out\n",
    "\n",
    "def build_single_table_frames(task, splits):\n",
    "    frames = {}\n",
    "    for split, table in splits.items():\n",
    "        df = table.df.copy()\n",
    "        # IMPORTANT: do not touch column masking/order; just drop the target to form X\n",
    "        X, y = to_Xy(df, task.target_col)\n",
    "        frames[split] = (X, y, df)  # keep original df for evaluation alignment\n",
    "    return frames\n",
    "\n",
    "def build_merged_table_frames(dataset, task, splits):\n",
    "    frames = {}\n",
    "    for split, table in splits.items():\n",
    "        base_df = table.df.copy()\n",
    "        merged_df = denormalize_one_hop(dataset, base_df)\n",
    "        X, y = to_Xy(merged_df, task.target_col)\n",
    "        frames[split] = (X, y, merged_df)\n",
    "    return frames\n"
   ],
   "id": "1cfbaca7c6d3b12c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vectorization Wrapper (Version-Safe)\n",
    "\n",
    "Initializes a `TableVectorizer` with only supported arguments for the installed `skrub` or `dirty_cat` version, ensuring compatibility. Transforms `train`, `val`, and `test` splits into numerical feature matrices, converting them to dense format if necessary.\n"
   ],
   "id": "f7cf59b1861625f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Helper Functions for Vectorization",
   "id": "ff99875cd0db904d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Vectorization wrapper (version-safe for skrub/dirty_cat) ---\n",
    "# Place this where your previous TableVectorizer/vectorize_splits block was.\n",
    "\n",
    "def _make_table_vectorizer():\n",
    "    sig = inspect.signature(TableVectorizer.__init__)\n",
    "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
    "\n",
    "    tv_kwargs = {}\n",
    "\n",
    "    # Only set kwargs that actually exist in the installed version\n",
    "    if \"cardinality_threshold\" in allowed:\n",
    "        tv_kwargs[\"cardinality_threshold\"] = globals().get(\"CARDINALITY_THRESHOLD\", 1000)\n",
    "\n",
    "    # Some versions expose this; others don't — guard it\n",
    "    if \"high_cardinality_transformer\" in allowed:\n",
    "        tv_kwargs[\"high_cardinality_transformer\"] = globals().get(\"HIGH_CARD_TRANSFORMER\", \"hashing\")\n",
    "\n",
    "    # Optional knobs if you happen to define them globally and the version supports them\n",
    "    if \"text_separator\" in allowed and \"TEXT_SEPARATOR\" in globals():\n",
    "        tv_kwargs[\"text_separator\"] = globals()[\"TEXT_SEPARATOR\"]\n",
    "    if \"numerical_transformer\" in allowed and \"NUMERICAL_TRANSFORMER\" in globals():\n",
    "        tv_kwargs[\"numerical_transformer\"] = globals()[\"NUMERICAL_TRANSFORMER\"]\n",
    "    if \"categorical_transformer\" in allowed and \"CATEGORICAL_TRANSFORMER\" in globals():\n",
    "        tv_kwargs[\"categorical_transformer\"] = globals()[\"CATEGORICAL_TRANSFORMER\"]\n",
    "\n",
    "    return TableVectorizer(**tv_kwargs)\n",
    "\n",
    "def _to_dense(X):\n",
    "    try:\n",
    "        # scipy sparse matrices have .toarray()\n",
    "        return X.toarray() if hasattr(X, \"toarray\") else X\n",
    "    except Exception:\n",
    "        return X\n",
    "\n",
    "def vectorize_splits(X_train, X_val, X_test):\n",
    "    tv = _make_table_vectorizer()\n",
    "    Xt = _to_dense(tv.fit_transform(X_train))\n",
    "    Xv = _to_dense(tv.transform(X_val))\n",
    "    Xs = _to_dense(tv.transform(X_test))\n",
    "    return tv, Xt, Xv, Xs\n"
   ],
   "id": "ffb43a27c3ac51cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _subsample(X, y, cap=TABPFN_MAX, seed=SEED):\n",
    "    if len(X) <= cap:\n",
    "        return X, y, np.arange(len(X))\n",
    "    idx = np.random.RandomState(seed).choice(len(X), size=cap, replace=False)\n",
    "    if hasattr(X, \"iloc\"):\n",
    "        Xs = X.iloc[idx]\n",
    "    else:\n",
    "        Xs = X[idx]\n",
    "    ys = y[idx]\n",
    "    return Xs, ys, idx\n",
    "\n",
    "def _fit_tabpfn(task, Xt, yt):\n",
    "    if task.task_type == TaskType.REGRESSION and TabPFNRegressor is not None:\n",
    "        model = TabPFNRegressor(\n",
    "            device=DEVICE,\n",
    "            #n_estimators=int(N_ESTIMATORS),\n",
    "        )\n",
    "    else:\n",
    "        model = TabPFNClassifier(\n",
    "            device=DEVICE,\n",
    "            #n_estimators=int(N_ESTIMATORS),\n",
    "        )\n",
    "    model.fit(Xt, yt)\n",
    "    return model\n",
    "\n",
    "def _predict_for_task(task, model, X):\n",
    "    # align with RelBench evaluators: AUROC expects probabilities for the positive class\n",
    "    if task.task_type == TaskType.REGRESSION:\n",
    "        return model.predict(X)\n",
    "    proba = model.predict_proba(X)\n",
    "    if task.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "        return proba[:, 1]\n",
    "    else:\n",
    "        # multiclass/multilabel: pass full probability matrix\n",
    "        return proba"
   ],
   "id": "5c53f649ce01b948",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Run TabPFN on Selected Tasks\n",
    "\n",
    "Runs TabPFN on a specified dataset and task, handling both single-table and merged-table modes. It vectorizes the data, fits the model, makes predictions, and evaluates performance using RelBench’s evaluators. Returns a dictionary with results."
   ],
   "id": "21448305df117a22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_tabpfn_on_task(dataset_name: str, task_name: str, mode: str = \"single\") -> Dict[str, Any]:\n",
    "    dataset = get_dataset(dataset_name, download=DOWNLOAD)\n",
    "    task, splits = fetch_splits(dataset_name, task_name, download=DOWNLOAD)\n",
    "\n",
    "    if mode == \"single\":\n",
    "        frames = build_single_table_frames(task, splits)\n",
    "    elif mode == \"merged\":\n",
    "        frames = build_merged_table_frames(dataset, task, splits)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'single' or 'merged'\")\n",
    "\n",
    "    (Xtr, ytr, _dftr) = frames[\"train\"]\n",
    "    (Xva, yva, dfva)  = frames[\"val\"]\n",
    "    (Xte, yte, dfte)  = frames[\"test\"]\n",
    "\n",
    "    # Vectorize\n",
    "    tv, Xt, Xv, Xs = vectorize_splits(Xtr, Xva, Xte)\n",
    "\n",
    "    # Respect TabPFN's sample cap\n",
    "    Xt_cap, yt_cap, _ = _subsample(Xt, ytr, cap=TABPFN_MAX, seed=SEED)\n",
    "\n",
    "    # Fit\n",
    "    model = _fit_tabpfn(task, Xt_cap, yt_cap)\n",
    "\n",
    "    # Predict & Evaluate with RelBench evaluators\n",
    "    val_pred  = _predict_for_task(task, model, Xv)\n",
    "    test_pred = _predict_for_task(task, model, Xs)\n",
    "\n",
    "    val_metrics  = task.evaluate(val_pred,  splits[\"val\"])   # documented API\n",
    "    test_metrics = task.evaluate(test_pred, splits[\"test\"])  # documented API\n",
    "\n",
    "    out = {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"task\": task_name,\n",
    "        \"mode\": mode,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"n_train_used\": len(Xt_cap),\n",
    "        \"n_train_total\": len(Xt),\n",
    "        \"n_val\": len(Xv),\n",
    "        \"n_test\": len(Xs),\n",
    "    }\n",
    "    return out"
   ],
   "id": "52860c3ba39daf4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Orchestrator for Benchmark Runs\n",
    "\n",
    "Iterates over all discovered tasks and runs TabPFN in both **single** and **merged** modes. Collects performance metrics for validation and test splits into a results table, handling failures gracefully. Sorts results for easier comparison.\n"
   ],
   "id": "1390db1f75d264ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MODES = globals().get(\"MODES\", [\"single\", \"merged\"])\n",
    "\n",
    "records = []\n",
    "failures = []\n",
    "\n",
    "for task_name in TASKS:\n",
    "    for mode in MODES:\n",
    "        try:\n",
    "            res = run_tabpfn_on_task(DATASET, task_name, mode=mode)\n",
    "            # Flatten metrics for val and test, one metric per row\n",
    "            for split in [\"val\", \"test\"]:\n",
    "                metrics = res.get(f\"{split}_metrics\") or {}\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    # Only add rows with non-empty metric_value\n",
    "                    if metric_value is not None and not (isinstance(metric_value, float) and np.isnan(metric_value)):\n",
    "                        records.append({\n",
    "                            \"dataset\": res.get(\"dataset\", DATASET),\n",
    "                            \"task\": res.get(\"task\", task_name),\n",
    "                            \"mode\": res.get(\"mode\", mode),\n",
    "                            \"method\": \"TabPFN_experimental_v1.0\",\n",
    "                            \"metric\": f\"{split}_{metric_name}\",\n",
    "                            \"score\": metric_value,\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            msg = f\"[{DATASET} | {task_name} | {mode}] failed: {e!s}\"\n",
    "            print(msg)\n",
    "            failures.append(msg)\n",
    "\n",
    "results_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "if results_df.empty:\n",
    "    print(\"No successful runs were recorded. Check the failure messages above.\")\n",
    "    # Create an empty, well-formed frame so downstream plotting code doesn't crash\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=[\"dataset\", \"task\", \"mode\", \"method\", \"metric\", \"score\"]\n",
    "    )\n",
    "else:\n",
    "    # Ensure required sort keys exist even if some rows missed them\n",
    "    for col in [\"task\", \"mode\"]:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = pd.NA\n",
    "    # Sort only by the columns that exist to avoid KeyError\n",
    "    sort_keys = [c for c in [\"task\", \"mode\", \"metric\"] if c in results_df.columns]\n",
    "    if sort_keys:\n",
    "        results_df = results_df.sort_values(sort_keys)\n",
    "\n",
    "display(results_df)\n"
   ],
   "id": "be6242da806a8050",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Persist results to CSV ---\n",
    "\n",
    "out_dir = globals().get(\"OUT_DIR\", \"outputs\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Change timestamp format to \"dd.mm.yyyy-hh:mm\"\n",
    "timestamp = time.strftime(\"%d.%m.%Y-%H:%M\")\n",
    "csv_name = f\"tabpfn_{DATASET}_{timestamp}.csv\"\n",
    "csv_path = os.path.join(out_dir, csv_name)\n",
    "\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved results to: {csv_path}\")\n"
   ],
   "id": "aa929637db7d0d54",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
