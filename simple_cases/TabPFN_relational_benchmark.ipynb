{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **TabPFN Relational Benchmark**\n",
    "This notebook benchmarks the performance of TabPFN models on datasets from RelBench in two scenarios:\n",
    "1. **Single Table** – Using only the target entity table.\n",
    "2. **Merged Table** – Using a naively denormalized table obtained by joining related tables.\n",
    "\n",
    "It automates dataset loading, preprocessing (including date feature engineering), vectorization, model training, prediction, and evaluation for all compatible tasks within a chosen RelBench dataset. The results allow comparing model performance between single-table and merged-table configurations.\n"
   ],
   "id": "7ab5a9bf5466de0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "id": "ac93e97c56d4b905"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import time\n",
    "import inspect\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- PyTorch / TorchFrame (already used) ---\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "\n",
    "# --- Skrub / Sentence Transformers (keep as-is for your vectorization) ---\n",
    "from skrub import TableVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- RelBench (keep as-is) ---\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task, get_task_names\n",
    "from relbench.base import TaskType\n",
    "import relbench.metrics\n",
    "from relbench.modeling.utils import get_stype_proposal\n",
    "from relbench.modeling.graph import make_pkey_fkey_graph\n",
    "\n",
    "# --- TabPFN (keep as-is) ---\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "\n",
    "# --- getML (new) ---\n",
    "from getml import data as gdata\n",
    "from getml import engine as geng\n",
    "from getml import pipeline as gpipeline\n",
    "from getml import feature_learning as gfl\n",
    "from getml import preprocessors as gprep\n",
    "from getml import predictors as gpred\n"
   ],
   "id": "57624fe02c838842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set Global Configuration",
   "id": "7154198e21fe2a01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Device preference\n",
    "#if torch.backends.mps.is_available():\n",
    "#    DEVICE = \"mps\"\n",
    "#elif torch.cuda.is_available():\n",
    "#    DEVICE = \"cuda\"\n",
    "#else:\n",
    "#    DEVICE = \"cpu\"\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Define global dataset variable (any available dataset from relbench.datasets)\n",
    "# \"rel-f1\" is the default, but can be overridden by setting DATASET variable\n",
    "DATASET = \"rel-f1\"\n",
    "\n",
    "# Global configuration variables with defaults\n",
    "SEED   = globals().get(\"SEED\", 42)\n",
    "N_ESTIMATORS = globals().get(\"N_ESTIMATORS\", 16) # number of TabPFN estimators\n",
    "TABPFN_MAX = globals().get(\"TABPFN_MAX\", 1000)  # hard ceiling for TabPFN\n",
    "\n",
    "# Define project for getML engine\n",
    "GETML_PROJECT = globals().get(\"GETML_PROJECT\", \"default_project\")\n",
    "\n",
    "# Global for getML engine state\n",
    "_ENGINE_STARTED = False"
   ],
   "id": "c5959e09ecd4c20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Notebook Configuration and Dataset Selection\n",
    "\n",
    "\n",
    "Sets the dataset name (`DATASET`) and download flag (`DOWNLOAD`), then discovers all available tasks for the selected dataset using RelBench’s APIs. Filters tasks to only those compatible with TabPFN (classification and regression).\n"
   ],
   "id": "a86eaa89573fdf63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reuse existing config if present, otherwise set defaults\n",
    "DATASET = globals().get(\"DATASET\", \"rel-f1\")\n",
    "DOWNLOAD = globals().get(\"DOWNLOAD\", True)\n",
    "\n",
    "# Discover tasks and keep only entity-level cls/reg tasks TabPFN can handle\n",
    "def _is_tabpfn_friendly(task):\n",
    "    return task.task_type in (\n",
    "        TaskType.BINARY_CLASSIFICATION,\n",
    "        TaskType.MULTICLASS_CLASSIFICATION,\n",
    "        TaskType.MULTILABEL_CLASSIFICATION,\n",
    "        TaskType.REGRESSION,\n",
    "    )\n",
    "\n",
    "_all = get_task_names(DATASET)  # shown in tutorials\n",
    "TASKS = []\n",
    "for tname in _all:\n",
    "    try:\n",
    "        t = get_task(DATASET, tname, download=DOWNLOAD)\n",
    "        if _is_tabpfn_friendly(t):\n",
    "            TASKS.append(tname)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {tname}: {e!s}\")\n",
    "\n",
    "print(f\"{DATASET}: {len(TASKS)} TabPFN-friendly tasks -> {TASKS}\")\n"
   ],
   "id": "d94075cf88bf6806",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Patch RelBench Metrics (Optional)",
   "id": "affc15acb6a0f8aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Patch relbench.metrics.skm.mean_squared_error to local mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "relbench.metrics.skm.mean_squared_error = mean_squared_error\n",
    "\n",
    "def patched_rmse(true, pred):\n",
    "    if \"squared\" in inspect.signature(mean_squared_error).parameters:\n",
    "        return mean_squared_error(true, pred, squared=False)\n",
    "    else:\n",
    "        return np.sqrt(mean_squared_error(true, pred))\n",
    "\n",
    "relbench.metrics.rmse = patched_rmse"
   ],
   "id": "46fcd0d47afb30c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Text Embedding Configuration\n",
    "\n",
    "This part provides a callable class to embed text columns using GloVe embeddings."
   ],
   "id": "5629815fd3214b7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a text embedding class using GloVe embeddings from Sentence Transformers.\n",
    "class GloveTextEmbedding:\n",
    "    def __init__(self, device: Optional[torch.device] = None):\n",
    "        self.model = SentenceTransformer(\n",
    "            \"sentence-transformers/average_word_embeddings_glove.6B.300d\",\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def __call__(self, sentences: List[str]) -> Tensor:\n",
    "        return torch.from_numpy(self.model.encode(sentences))"
   ],
   "id": "78e9b5ef45024fb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fetch Dataset Splits\n",
    "\n",
    "Utility functions to load a task’s splits (`train`, `val`, `test`), convert them to pandas DataFrames, and extract features (`X`) and targets (`y`). Includes functions to:\n",
    "* Load train/val/test splits for a task.\n",
    "* Extract features/targets.\n",
    "* Infer primary keys.\n",
    "* Denormalize tables (one-hop join).\n",
    "* Build data frames for both single-table and merged-table scenarios.\n"
   ],
   "id": "1a35d633effdd263"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Dataset loaders / frame builders\n",
    "# ---------------------------\n",
    "\n",
    "def fetch_splits(dataset_name: str, task_name: str, download: bool = True):\n",
    "    task = get_task(dataset_name, task_name, download=download)\n",
    "    # keep original columns (mask_input_cols=False so we see raw fields)\n",
    "    splits = {\n",
    "        split: task.get_table(split, mask_input_cols=False)\n",
    "        for split in (\"train\", \"val\", \"test\")\n",
    "    }\n",
    "    return task, splits\n",
    "\n",
    "def to_Xy(df: pd.DataFrame, target_col: str):\n",
    "    y = df[target_col].to_numpy()\n",
    "    X = df.drop(columns=[target_col])\n",
    "    return X, y\n",
    "\n",
    "def build_single_table_frames(task, splits):\n",
    "    \"\"\"\n",
    "    Single-table mode: do NOT engineer features here.\n",
    "    Just return raw base table X, y per split (target dropped from X).\n",
    "    \"\"\"\n",
    "    frames = {}\n",
    "    for split, table in splits.items():\n",
    "        df = table.df.copy()\n",
    "        X, y = to_Xy(df, task.target_col)\n",
    "        frames[split] = (X, y, df)\n",
    "    return frames\n"
   ],
   "id": "ac5f4a739cba0093",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _ensure_getml_engine():\n",
    "    \"\"\"Idempotently launch getML engine and set project.\"\"\"\n",
    "    global _ENGINE_STARTED, GETML_PROJECT\n",
    "    if _ENGINE_STARTED:\n",
    "        return\n",
    "    try:\n",
    "        geng.launch(\n",
    "            allow_push_notifications=False,\n",
    "            allow_remote_ips=False,\n",
    "            in_memory=True,\n",
    "            launch_browser=False,\n",
    "            log=False,\n",
    "            quiet=True,\n",
    "        )\n",
    "        geng.set_project(GETML_PROJECT)\n",
    "        _ENGINE_STARTED = True\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Failed to launch the getML engine.\") from e\n",
    "\n",
    "def _shutdown_getml_engine():\n",
    "    \"\"\"Shutdown getML engine if started.\"\"\"\n",
    "    global _ENGINE_STARTED\n",
    "    if not _ENGINE_STARTED:\n",
    "        return\n",
    "    try:\n",
    "        geng.shutdown()\n",
    "    except Exception:\n",
    "        pass\n",
    "    _ENGINE_STARTED = False\n",
    "\n",
    "def _first_datetime_col(df: pd.DataFrame) -> Optional[str]:\n",
    "    # Prefer already-typed datetime64 columns\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            return col\n",
    "    # Try to parse common timestamp strings without mutating df\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            try:\n",
    "                pd.to_datetime(df[col], errors=\"raise\")\n",
    "                return col\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _guess_primary_key(table_name: str, df: pd.DataFrame) -> Optional[str]:\n",
    "    candidates = [\n",
    "        f\"{table_name}_id\",\n",
    "        f\"{table_name[:-1]}_id\" if table_name.endswith(\"s\") else None,\n",
    "        \"id\",\n",
    "        \"ID\",\n",
    "        f\"{table_name}Id\",\n",
    "        f\"{table_name}_pk\",\n",
    "    ]\n",
    "    candidates = [c for c in candidates if c and c in df.columns]\n",
    "    # prefer unique columns\n",
    "    for c in candidates:\n",
    "        if df[c].is_unique:\n",
    "            return c\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "\n",
    "def _guess_foreign_key(pop_df: pd.DataFrame, per_name: str, per_pk: str) -> Optional[str]:\n",
    "    if per_pk in pop_df.columns:\n",
    "        return per_pk\n",
    "    candidates = [\n",
    "        f\"{per_name}_id\",\n",
    "        f\"{per_name[:-1]}_id\" if per_name.endswith(\"s\") else None,\n",
    "        f\"{per_name}{per_pk.capitalize()}\" if per_pk != \"id\" else None,\n",
    "    ]\n",
    "    for c in [c for c in candidates if c]:\n",
    "        if c in pop_df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def _roles_from_df(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: Optional[str],\n",
    "    join_keys: List[str],\n",
    "    time_col: Optional[str],\n",
    "):\n",
    "    num_cols, cat_cols = [], []\n",
    "    skip = set(join_keys)\n",
    "    if time_col:\n",
    "        skip.add(time_col)\n",
    "    if target_col:\n",
    "        skip.add(target_col)\n",
    "\n",
    "    for c in df.columns:\n",
    "        if c in skip:\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            num_cols.append(c)\n",
    "        else:\n",
    "            cat_cols.append(c)\n",
    "\n",
    "    return {\n",
    "        \"target\": target_col,\n",
    "        \"join_key\": join_keys,\n",
    "        \"time_stamp\": time_col,\n",
    "        \"numerical\": num_cols,\n",
    "        \"categorical\": cat_cols,\n",
    "    }\n",
    "\n",
    "\n",
    "def _df_set_roles(gdf: gdata.DataFrame, roles: Dict[str, Any]):\n",
    "    # Assign roles on a getML DataFrame\n",
    "    if roles.get(\"target\"):\n",
    "        gdf.set_role(roles[\"target\"], gdata.roles.target)\n",
    "    jk = roles.get(\"join_key\") or []\n",
    "    if jk:\n",
    "        gdf.set_role(jk, gdata.roles.join_key)\n",
    "    if roles.get(\"time_stamp\"):\n",
    "        gdf.set_role(roles[\"time_stamp\"], gdata.roles.time_stamp)\n",
    "    if roles.get(\"numerical\"):\n",
    "        gdf.set_role(roles[\"numerical\"], gdata.roles.numerical)\n",
    "    if roles.get(\"categorical\"):\n",
    "        gdf.set_role(roles[\"categorical\"], gdata.roles.categorical)\n",
    "\n",
    "\n",
    "def _harmonize_join_keys(pop_by_split: Dict[str, pd.DataFrame], per_spec: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Ensure the population has columns named like each peripheral's PK.\n",
    "    If population FK name != peripheral PK name, mirror FK into a new column\n",
    "    with the PK name (required because getML matches join_keys by name).\n",
    "    Returns the filtered per_spec with only peripherals we can actually join.\n",
    "    \"\"\"\n",
    "    valid_specs = []\n",
    "    for spec in per_spec:\n",
    "        pk = spec[\"pk\"]\n",
    "        fk = spec[\"fk\"]\n",
    "        can_use = True\n",
    "        for split, df in pop_by_split.items():\n",
    "            if pk in df.columns:\n",
    "                continue  # already aligned\n",
    "            if fk in df.columns:\n",
    "                df[pk] = df[fk]\n",
    "            else:\n",
    "                can_use = False\n",
    "                break\n",
    "        if can_use:\n",
    "            valid_specs.append(spec)\n",
    "    return valid_specs\n"
   ],
   "id": "25acbfecc3c83da3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_merged_table_frames(dataset, task, splits):\n",
    "    \"\"\"\n",
    "    Build merged features with getML:\n",
    "    - Automatically infers PK/FK candidates,\n",
    "    - Harmonizes population FK names to match peripheral PK names,\n",
    "    - Generates relational aggregates via FastProp,\n",
    "    - Returns (X, y, engineered_df) per split (same contract as the rest of the notebook).\n",
    "    \"\"\"\n",
    "    _ensure_getml_engine()\n",
    "\n",
    "    # --- Load all DB tables into pandas ---\n",
    "    db = dataset.get_db()\n",
    "    all_tables: Dict[str, pd.DataFrame] = {\n",
    "        name: (tbl.df.copy() if hasattr(tbl, \"df\") else tbl.to_pandas())\n",
    "        for name, tbl in db.table_dict.items()\n",
    "    }\n",
    "\n",
    "    # --- Base (population) tables by split (the task table) ---\n",
    "    pop: Dict[str, pd.DataFrame] = {split: tbl.df.copy() for split, tbl in splits.items()}\n",
    "    base_name = getattr(splits[\"train\"], \"name\", \"population\")\n",
    "\n",
    "    # --- Infer candidate peripherals and PK/FK mapping from TRAIN schema ---\n",
    "    peripheral_names = [t for t in all_tables if t != base_name]\n",
    "    pop_train = pop[\"train\"]\n",
    "    time_col = _first_datetime_col(pop_train)\n",
    "\n",
    "    per_spec = []\n",
    "    for per_name in peripheral_names:\n",
    "        per_df = all_tables[per_name]\n",
    "        per_pk = _guess_primary_key(per_name, per_df)\n",
    "        if not per_pk:\n",
    "            continue\n",
    "        pop_fk = _guess_foreign_key(pop_train, per_name, per_pk)\n",
    "        if not pop_fk:\n",
    "            continue\n",
    "        per_spec.append({\"name\": per_name, \"pk\": per_pk, \"fk\": pop_fk})\n",
    "\n",
    "    # Align population join key names to peripheral PKs (required by getML)\n",
    "    per_spec = _harmonize_join_keys(pop, per_spec)\n",
    "\n",
    "    # If nothing to join, fall back to single-table frames without changing downstream code paths\n",
    "    if not per_spec:\n",
    "        frames = {}\n",
    "        for split, df in pop.items():\n",
    "            X, y = to_Xy(df, task.target_col)\n",
    "            frames[split] = (X, y, df)\n",
    "        return frames\n",
    "\n",
    "    # --- Build getML Containers per split using from_pandas (fixes zero-column error) ---\n",
    "    def _make_container(split: str) -> gdata.Container:\n",
    "        # Population\n",
    "        g_pop = gdata.DataFrame.from_pandas(pop[split], name=f\"{base_name}_{split}\")\n",
    "        roles_pop = _roles_from_df(\n",
    "            pop[split],\n",
    "            target_col=task.target_col,                          # target ONLY on population\n",
    "            join_keys=[spec[\"pk\"] for spec in per_spec],         # (now) aligned names\n",
    "            time_col=time_col,\n",
    "        )\n",
    "        _df_set_roles(g_pop, roles_pop)\n",
    "\n",
    "        # Peripherals\n",
    "        g_per = []\n",
    "        for spec in per_spec:\n",
    "            per_df = all_tables[spec[\"name\"]]\n",
    "            gdf = gdata.DataFrame.from_pandas(per_df, name=f\"{spec['name']}_{split}\")\n",
    "            roles_per = _roles_from_df(\n",
    "                per_df,\n",
    "                target_col=None,                                  # NO target on peripherals\n",
    "                join_keys=[spec[\"pk\"]],\n",
    "                time_col=None,\n",
    "            )\n",
    "            _df_set_roles(gdf, roles_per)\n",
    "            g_per.append(gdf)\n",
    "        return gdata.Container(population=g_pop, peripherals=g_per)\n",
    "\n",
    "    train_c = _make_container(\"train\")\n",
    "    val_c   = _make_container(\"val\")\n",
    "    test_c  = _make_container(\"test\")\n",
    "\n",
    "    # --- Build and run the pipeline ---\n",
    "    predictor = (\n",
    "        gpred.LinearRegression()\n",
    "        if task.task_type == TaskType.REGRESSION\n",
    "        else gpred.LogisticRegression()\n",
    "    )\n",
    "\n",
    "    pipe = gpipeline.Pipeline(\n",
    "        feature_learners=[gfl.FastProp()],\n",
    "        preprocessors=[gprep.Imputation()],\n",
    "        predictors=[predictor],\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: fit on a single Container (no subset kwargs),\n",
    "    # then transform other splits separately to avoid the 'mutually exclusive' error.\n",
    "    pipe.fit(train_c)\n",
    "\n",
    "    fe_train = pipe.transform(train_c).to_pandas()\n",
    "    fe_val   = pipe.transform(val_c).to_pandas()\n",
    "    fe_test  = pipe.transform(test_c).to_pandas()\n",
    "\n",
    "    # --- Extract X, y keeping the notebook's original contract ---\n",
    "    # If the target is not present in engineered features (common), fall back to original split y.\n",
    "    def _to_Xy_safe(fe_df: pd.DataFrame, raw_df: pd.DataFrame):\n",
    "        if task.target_col in fe_df.columns:\n",
    "            return to_Xy(fe_df, task.target_col)\n",
    "        else:\n",
    "            y = raw_df[task.target_col].to_numpy()\n",
    "            X = fe_df.copy()\n",
    "            return X, y\n",
    "\n",
    "    Xtr, ytr = _to_Xy_safe(fe_train, pop[\"train\"])\n",
    "    Xva, yva = _to_Xy_safe(fe_val,   pop[\"val\"])\n",
    "    Xte, yte = _to_Xy_safe(fe_test,  pop[\"test\"])\n",
    "\n",
    "    return {\n",
    "        \"train\": (Xtr, ytr, fe_train),\n",
    "        \"val\":   (Xva, yva, fe_val),\n",
    "        \"test\":  (Xte, yte, fe_test),\n",
    "    }\n"
   ],
   "id": "1e53fa004c6c4217",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vectorization Wrapper (Version-Safe)\n",
    "\n",
    "Initializes a `TableVectorizer` with only supported arguments for the installed `skrub` or `dirty_cat` version, ensuring compatibility. Transforms `train`, `val`, and `test` splits into numerical feature matrices, converting them to dense format if necessary.\n"
   ],
   "id": "f7cf59b1861625f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Helper Functions for Vectorization",
   "id": "ff99875cd0db904d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This function creates a `TableVectorizer` instance with version-specific arguments.\n",
    "def _make_table_vectorizer():\n",
    "    sig = inspect.signature(TableVectorizer.__init__)\n",
    "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
    "\n",
    "    tv_kwargs = {}\n",
    "\n",
    "    # Only set kwargs that actually exist in the installed version\n",
    "    if \"cardinality_threshold\" in allowed:\n",
    "        tv_kwargs[\"cardinality_threshold\"] = globals().get(\"CARDINALITY_THRESHOLD\", 1000)\n",
    "\n",
    "    # Some versions expose this; others don't, guard it\n",
    "    if \"high_cardinality_transformer\" in allowed:\n",
    "        tv_kwargs[\"high_cardinality_transformer\"] = globals().get(\"HIGH_CARD_TRANSFORMER\", \"hashing\")\n",
    "\n",
    "    # Optional knobs if you define them globally and the version supports them\n",
    "    if \"text_separator\" in allowed and \"TEXT_SEPARATOR\" in globals():\n",
    "        tv_kwargs[\"text_separator\"] = globals()[\"TEXT_SEPARATOR\"]\n",
    "    if \"numerical_transformer\" in allowed and \"NUMERICAL_TRANSFORMER\" in globals():\n",
    "        tv_kwargs[\"numerical_transformer\"] = globals()[\"NUMERICAL_TRANSFORMER\"]\n",
    "    if \"categorical_transformer\" in allowed and \"CATEGORICAL_TRANSFORMER\" in globals():\n",
    "        tv_kwargs[\"categorical_transformer\"] = globals()[\"CATEGORICAL_TRANSFORMER\"]\n",
    "\n",
    "    return TableVectorizer(**tv_kwargs)\n",
    "\n",
    "# Converts a sparse matrix to a dense NumPy array, handling cases where the input is already dense or does not support `.toarray()`.\n",
    "def _to_dense(X):\n",
    "    try:\n",
    "        # scipy sparse matrices have .toarray()\n",
    "        return X.toarray() if hasattr(X, \"toarray\") else X\n",
    "    except Exception:\n",
    "        return X\n",
    "\n",
    "# Vectorizes the training, validation, and test splits using a `TableVectorizer`. It initializes the vectorizer, fits it on the training data, and transforms all splits into dense NumPy arrays. Returns the vectorizer and the transformed matrices.\n",
    "def vectorize_splits(X_train, X_val, X_test):\n",
    "    # Fit only on training data to prevent data leakage\n",
    "    tv = _make_table_vectorizer()\n",
    "    Xt = _to_dense(tv.fit_transform(X_train))\n",
    "    Xv = _to_dense(tv.transform(X_val))\n",
    "    Xs = _to_dense(tv.transform(X_test))\n",
    "    return tv, Xt, Xv, Xs\n"
   ],
   "id": "ffb43a27c3ac51cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training and Prediction Helpers",
   "id": "e135f05177e4b188"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This function subsamples the training data to a maximum size defined by `TABPFN_MAX`. If the dataset is smaller than this cap, it returns the full dataset; otherwise, it randomly samples without replacement.\n",
    "def _subsample(X, y, cap=TABPFN_MAX, seed=SEED):\n",
    "    if len(X) <= cap:\n",
    "        return X, y, np.arange(len(X))\n",
    "    idx = np.random.RandomState(seed).choice(len(X), size=cap, replace=False)\n",
    "    if hasattr(X, \"iloc\"):\n",
    "        Xs = X.iloc[idx]\n",
    "    else:\n",
    "        Xs = X[idx]\n",
    "    ys = y[idx]\n",
    "    return Xs, ys, idx\n",
    "\n",
    "# Fits a TabPFN model (either classifier or regressor) based on the task type. It initializes the model with the specified device and number of estimators, then fits it to the provided training data.\n",
    "def _fit_tabpfn(task, Xt, yt):\n",
    "    if task.task_type == TaskType.REGRESSION and TabPFNRegressor is not None:\n",
    "        model = TabPFNRegressor(\n",
    "            device=DEVICE,\n",
    "            #n_estimators=int(N_ESTIMATORS),\n",
    "            ignore_pretraining_limits=True,\n",
    "        )\n",
    "    else:\n",
    "        model = TabPFNClassifier(\n",
    "            device=DEVICE,\n",
    "            #n_estimators=int(N_ESTIMATORS),\n",
    "            ignore_pretraining_limits=True,\n",
    "        )\n",
    "    model.fit(Xt, yt)\n",
    "    return model\n",
    "\n",
    "# Helper function to make predictions for a given task using the fitted model. It handles different task types (regression, binary classification, multiclass/multilabel) and returns the appropriate prediction format.\n",
    "def _predict_for_task(task, model, X):\n",
    "    # align with RelBench evaluators: AUROC expects probabilities for the positive class\n",
    "    if task.task_type == TaskType.REGRESSION:\n",
    "        return model.predict(X)\n",
    "    proba = model.predict_proba(X)\n",
    "    if task.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "        return proba[:, 1]\n",
    "    else:\n",
    "        # multiclass/multilabel: pass full probability matrix\n",
    "        return proba"
   ],
   "id": "5c53f649ce01b948",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Run TabPFN on Selected Tasks\n",
    "\n",
    "Runs TabPFN on a specified dataset and task, handling both single-table and merged-table modes. It vectorizes the data, fits the model, makes predictions, and evaluates performance using RelBench’s evaluators. Returns a dictionary with results."
   ],
   "id": "21448305df117a22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_tabpfn_on_task(dataset_name: str, task_name: str, mode: str = \"single\") -> Dict[str, Any]:\n",
    "    # Load dataset and task splits\n",
    "    dataset = get_dataset(dataset_name, download=DOWNLOAD)\n",
    "    task, splits = fetch_splits(dataset_name, task_name, download=DOWNLOAD)\n",
    "\n",
    "    # Ensure the task is compatible with TabPFN\n",
    "    if mode == \"single\":\n",
    "        frames = build_single_table_frames(task, splits)\n",
    "    elif mode == \"merged\":\n",
    "        try:\n",
    "            frames = build_merged_table_frames(dataset, task, splits)\n",
    "        except Exception as e:\n",
    "            print(f\"merged mode failed: {e}, falling back to single-table mode.\")\n",
    "            frames = build_single_table_frames(task, splits)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'single' or 'merged'\")\n",
    "\n",
    "    # Extract features and targets for each split\n",
    "    (Xtr, ytr, _dftr) = frames[\"train\"]\n",
    "    (Xva, yva, dfva)  = frames[\"val\"]\n",
    "    (Xte, yte, dfte)  = frames[\"test\"]\n",
    "\n",
    "    # Vectorize\n",
    "    tv, Xt, Xv, Xs = vectorize_splits(Xtr, Xva, Xte)\n",
    "\n",
    "    # Respect TabPFN's sample cap\n",
    "    Xt_cap, yt_cap, _ = _subsample(Xt, ytr, cap=TABPFN_MAX, seed=SEED)\n",
    "\n",
    "    # Fit\n",
    "    model = _fit_tabpfn(task, Xt_cap, yt_cap)\n",
    "\n",
    "    # Predict & Evaluate with RelBench evaluators\n",
    "    val_pred  = _predict_for_task(task, model, Xv)\n",
    "    test_pred = _predict_for_task(task, model, Xs)\n",
    "\n",
    "    # Align predictions with original DataFrame indices for evaluation\n",
    "    val_metrics  = task.evaluate(val_pred,  splits[\"val\"])\n",
    "    test_metrics = task.evaluate(test_pred, splits[\"test\"])\n",
    "\n",
    "    # Convert metrics to a dictionary, ensuring all values are floats\n",
    "    out = {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"task\": task_name,\n",
    "        \"mode\": mode,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"n_train_used\": len(Xt_cap),\n",
    "        \"n_train_total\": len(Xt),\n",
    "        \"n_val\": len(Xv),\n",
    "        \"n_test\": len(Xs),\n",
    "    }\n",
    "    return out"
   ],
   "id": "52860c3ba39daf4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Orchestrator for Benchmark Runs\n",
    "\n",
    "Iterates over all discovered tasks and runs TabPFN in both **single** and **merged** modes. Collects performance metrics for validation and test splits into a results table, handling failures gracefully. Sorts results for easier comparison.\n"
   ],
   "id": "67ce1925dce06a6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MODES = globals().get(\"MODES\", [\"single\", \"merged\"])\n",
    "\n",
    "records = []\n",
    "failures = []\n",
    "\n",
    "# Run TabPFN on all tasks in both modes and collect results\n",
    "for task_name in TASKS:\n",
    "    for mode in MODES:\n",
    "        try:\n",
    "            res = run_tabpfn_on_task(DATASET, task_name, mode=mode)\n",
    "            # Flatten metrics for val and test, one metric per row\n",
    "            for split in [\"val\", \"test\"]:\n",
    "                metrics = res.get(f\"{split}_metrics\") or {}\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    # Only add rows with non-empty metric_value\n",
    "                    if metric_value is not None and not (isinstance(metric_value, float) and np.isnan(metric_value)):\n",
    "                        records.append({\n",
    "                            \"dataset\": res.get(\"dataset\", DATASET),\n",
    "                            \"task\": res.get(\"task\", task_name),\n",
    "                            \"split\": split,\n",
    "                            \"mode\": res.get(\"mode\", mode),\n",
    "                            \"method\": \"TabPFN_experimental_v1.0\",\n",
    "                            \"metric\": metric_name,\n",
    "                            \"score\": metric_value,\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            msg = f\"[{DATASET} | {task_name} | {mode}] failed: {e!s}\"\n",
    "            print(msg)\n",
    "            failures.append(msg)\n",
    "\n",
    "# Convert collected records into a DataFrame\n",
    "results_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# If no successful runs were recorded, display a message and create an empty DataFrame\n",
    "if results_df.empty:\n",
    "    print(\"No successful runs were recorded. Check the failure messages above.\")\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=[\"dataset\", \"task\", \"split\", \"mode\", \"method\", \"metric\", \"score\"]\n",
    "    )\n",
    "else:\n",
    "    # Ensure required sort keys exist even if some rows missed them\n",
    "    for col in [\"task\", \"mode\"]:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = pd.NA\n",
    "    # Sort only by the columns that exist to avoid KeyError\n",
    "    sort_keys = [c for c in [\"task\", \"mode\", \"metric\"] if c in results_df.columns]\n",
    "    if sort_keys:\n",
    "        results_df = results_df.sort_values(sort_keys)\n",
    "\n",
    "display(results_df)\n"
   ],
   "id": "8119d8844ebded86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save Results to CSV",
   "id": "77a4dc93cb1858d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify output directory\n",
    "out_dir = globals().get(\"OUT_DIR\", \"outputs\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Change timestamp format to \"dd.mm.yyyy-hh:mm\"\n",
    "timestamp = time.strftime(\"%d.%m.%Y-%H:%M\")\n",
    "csv_name = f\"tabpfn_{DATASET}_{timestamp}.csv\"\n",
    "csv_path = os.path.join(out_dir, csv_name)\n",
    "\n",
    "# Round all numerical results to 4 decimal places before saving\n",
    "if \"score\" in results_df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(results_df[\"score\"]):\n",
    "        results_df[\"score\"] = results_df[\"score\"].round(4)\n",
    "    else:\n",
    "        # Optionally, try to convert to numeric first\n",
    "        results_df[\"score\"] = pd.to_numeric(results_df[\"score\"], errors=\"coerce\").round(4)\n",
    "\n",
    "# Filter out rows with empty score values before saving\n",
    "if \"score\" in results_df.columns:\n",
    "    results_df = results_df[results_df[\"score\"].notnull()]\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved results to: {csv_path}\")\n"
   ],
   "id": "c13a29742326995c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
