{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T15:33:36.291936Z",
     "start_time": "2025-08-15T15:33:31.304291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import time\n",
    "import inspect\n",
    "from contextlib import contextmanager\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# --- Third-Party ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleDict\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import MLP\n",
    "from torch_geometric.typing import NodeType\n",
    "\n",
    "from skrub import TableVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- RelBench ---\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task, get_task_names\n",
    "from relbench.base import TaskType\n",
    "import relbench.metrics\n",
    "from relbench.modeling.utils import get_stype_proposal\n",
    "from relbench.modeling.graph import make_pkey_fkey_graph\n",
    "from relbench.modeling.nn import HeteroEncoder, HeteroGraphSAGE, HeteroTemporalEncoder\n",
    "\n",
    "# --- TabPFN ---\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "\n",
    "# --- Torch Frame ---\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from torch_frame.data.stats import StatType\n",
    "\n",
    "# --- END IMPORTS ---\n",
    "\n",
    "# Device preference\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Define global dataset variable (any available dataset from relbench.datasets)\n",
    "# \"rel-f1\" is the default, but can be overridden by setting DATASET variable\n",
    "DATASET = \"rel-avito\"\n"
   ],
   "id": "57624fe02c838842",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T15:33:36.397978Z",
     "start_time": "2025-08-15T15:33:36.381432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reuse existing config if present, otherwise set defaults\n",
    "DATASET = globals().get(\"DATASET\", \"rel-f1\")\n",
    "DOWNLOAD = globals().get(\"DOWNLOAD\", True)\n",
    "\n",
    "# Discover tasks and keep only entity-level cls/reg tasks TabPFN can handle\n",
    "def _is_tabpfn_friendly(task):\n",
    "    return task.task_type in (\n",
    "        TaskType.BINARY_CLASSIFICATION,\n",
    "        TaskType.MULTICLASS_CLASSIFICATION,\n",
    "        TaskType.MULTILABEL_CLASSIFICATION,\n",
    "        TaskType.REGRESSION,\n",
    "    )\n",
    "\n",
    "_all = get_task_names(DATASET)  # shown in tutorials\n",
    "TASKS = []\n",
    "for tname in _all:\n",
    "    try:\n",
    "        t = get_task(DATASET, tname, download=DOWNLOAD)\n",
    "        if _is_tabpfn_friendly(t):\n",
    "            TASKS.append(tname)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {tname}: {e!s}\")\n",
    "\n",
    "print(f\"{DATASET}: {len(TASKS)} TabPFN-friendly tasks -> {TASKS}\")\n"
   ],
   "id": "d94075cf88bf6806",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel-avito: 3 TabPFN-friendly tasks -> ['ad-ctr', 'user-visits', 'user-clicks']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T15:33:36.406404Z",
     "start_time": "2025-08-15T15:33:36.404665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Patch relbench.metrics.skm.mean_squared_error to local mean_squared_error\n",
    "relbench.metrics.skm.mean_squared_error = mean_squared_error\n",
    "\n",
    "def patched_rmse(true, pred):\n",
    "    if \"squared\" in inspect.signature(mean_squared_error).parameters:\n",
    "        return mean_squared_error(true, pred, squared=False)\n",
    "    else:\n",
    "        return np.sqrt(mean_squared_error(true, pred))\n",
    "\n",
    "relbench.metrics.rmse = patched_rmse"
   ],
   "id": "46fcd0d47afb30c1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T15:33:36.414709Z",
     "start_time": "2025-08-15T15:33:36.412946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GloveTextEmbedding:\n",
    "    def __init__(self, device: Optional[torch.device] = None):\n",
    "        self.model = SentenceTransformer(\n",
    "            \"sentence-transformers/average_word_embeddings_glove.6B.300d\",\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def __call__(self, sentences: List[str]) -> Tensor:\n",
    "        return torch.from_numpy(self.model.encode(sentences))"
   ],
   "id": "78e9b5ef45024fb3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T15:41:07.678064Z",
     "start_time": "2025-08-15T15:33:36.419810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = time.perf_counter()\n",
    "    yield lambda: time.perf_counter() - start\n",
    "\n",
    "def classification_metrics(y_true, y_pred, y_prob=None) -> Dict[str, float]:\n",
    "    out = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"roc_auc\": np.nan,\n",
    "    }\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            out[\"roc_auc\"] = roc_auc_score(y_true, y_prob)\n",
    "        except Exception:\n",
    "            out[\"roc_auc\"] = np.nan\n",
    "    else:\n",
    "        out[\"roc_auc\"] = np.nan\n",
    "    return out\n",
    "\n",
    "def regression_metrics(y_true, y_pred, y_prob=None) -> Dict[str, float]:\n",
    "    # Accepts y_prob for compatibility, but ignores it\n",
    "    return {\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        \"mse\": mean_squared_error(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "dataset = get_dataset(DATASET)\n",
    "db = dataset.get_db()\n",
    "\n",
    "def to_pandas(table):\n",
    "    if hasattr(table, \"to_pandas\"):\n",
    "        return table.to_pandas()\n",
    "    if hasattr(table, \"df\"):\n",
    "        return table.df\n",
    "    raise ValueError(\"Unknown table type\")\n",
    "\n",
    "# Convert all tables to pandas DataFrames\n",
    "tables = {name: to_pandas(tbl) for name, tbl in db.table_dict.items()}\n",
    "\n",
    "# Batch‐collect date features and concat at once\n",
    "for name, df in tables.items():\n",
    "    date_cols = [col for col in df.columns if \"date\" in col.lower()]\n",
    "    if not date_cols:\n",
    "        continue\n",
    "    feats_list = []\n",
    "    for col in date_cols:\n",
    "        dt = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "        dt_filled = dt.fillna(dt.min())\n",
    "        df[col] = dt_filled.dt.tz_localize(None)\n",
    "        feats = pd.DataFrame({\n",
    "            f\"{col}_year\": dt_filled.dt.year,\n",
    "            f\"{col}_month\": dt_filled.dt.month,\n",
    "            f\"{col}_day\": dt_filled.dt.day,\n",
    "            f\"{col}_weekday\": dt_filled.dt.weekday,\n",
    "            f\"{col}_quarter\": dt_filled.dt.quarter,\n",
    "            f\"{col}_is_month_start\": dt_filled.dt.is_month_start.astype(int),\n",
    "            f\"{col}_is_month_end\": dt_filled.dt.is_month_end.astype(int),\n",
    "            f\"{col}_is_weekend\": (dt_filled.dt.weekday >= 5).astype(int),\n",
    "            f\"{col}_month_sin\": np.sin(2 * np.pi * dt_filled.dt.month / 12),\n",
    "            f\"{col}_month_cos\": np.cos(2 * np.pi * dt_filled.dt.month / 12),\n",
    "            f\"{col}_weekday_sin\": np.sin(2 * np.pi * dt_filled.dt.weekday / 7),\n",
    "            f\"{col}_weekday_cos\": np.cos(2 * np.pi * dt_filled.dt.weekday / 7),\n",
    "            f\"{col}_elapsed_days\": (dt_filled - dt_filled.min()).dt.days,\n",
    "        }, index=df.index)\n",
    "        feats_list.append(feats)\n",
    "    if feats_list:\n",
    "        df = pd.concat([df] + feats_list, axis=1)\n",
    "    tables[name] = df\n",
    "\n",
    "# --- ADD: push processed tables back into db.table_dict ---\n",
    "for name, df in tables.items():\n",
    "    db.table_dict[name].df = df\n",
    "# --- END ADD ---\n",
    "\n",
    "# --- ADD THIS BLOCK: Build the hetero-temporal graph for GNN experiments ---\n",
    "col_to_stype_dict = get_stype_proposal(db)\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=GloveTextEmbedding(device=torch.device(DEVICE)), batch_size=256\n",
    ")\n",
    "data, col_stats_dict = make_pkey_fkey_graph(\n",
    "    db,\n",
    "    col_to_stype_dict=col_to_stype_dict,\n",
    "    text_embedder_cfg=text_embedder_cfg,\n",
    "    cache_dir=None,\n",
    ")"
   ],
   "id": "4da5c0d65181834d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /Users/michaelflppv/Library/Caches/relbench/rel-avito/db...\n",
      "Done in 1.23 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding raw data in mini-batch: 100%|██████████| 7763/7763 [00:06<00:00, 1175.44it/s]\n",
      "Embedding raw data in mini-batch: 100%|██████████| 23284/23284 [00:58<00:00, 395.09it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T15:41:07.700609Z",
     "start_time": "2025-08-15T15:41:07.695513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_splits(dataset_name: str, task_name: str, download: bool = True):\n",
    "    task = get_task(dataset_name, task_name, download=download)\n",
    "    # keep original columns (tutorial shows mask_input_cols flag)\n",
    "    splits = {\n",
    "        split: task.get_table(split, mask_input_cols=False)\n",
    "        for split in (\"train\", \"val\", \"test\")\n",
    "    }\n",
    "    return task, splits\n",
    "\n",
    "def to_Xy(df: pd.DataFrame, target_col: str):\n",
    "    y = df[target_col].to_numpy()\n",
    "    X = df.drop(columns=[target_col])\n",
    "    return X, y\n",
    "\n",
    "# --- Naive merged table via one-hop FK→PK joins using dataset.get_db() ---\n",
    "# Uses only duck-typing on Table objects; falls back gracefully if metadata is missing.\n",
    "def _infer_pk(table_obj, df: pd.DataFrame):\n",
    "    # best-effort: check common attribute names first, then infer by uniqueness\n",
    "    for attr in (\"primary_key_col\", \"pkey\", \"pk\", \"primary_key\", \"id_col\"):\n",
    "        if hasattr(table_obj, attr):\n",
    "            cand = getattr(table_obj, attr)\n",
    "            if isinstance(cand, str) and cand in df.columns:\n",
    "                return cand\n",
    "    # fallback: take the first unique column if exists\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            if df[c].is_unique:\n",
    "                return c\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def denormalize_one_hop(dataset, base_df: pd.DataFrame):\n",
    "    try:\n",
    "        db = dataset.get_db()  # documented in README\n",
    "    except Exception:\n",
    "        return base_df  # cannot access DB; return base table\n",
    "\n",
    "    tables = getattr(db, \"tables\", None)\n",
    "    if not isinstance(tables, dict):\n",
    "        return base_df\n",
    "\n",
    "    # Build a map: PK column name -> (table_name, table_df, pk_col)\n",
    "    pkmap = {}\n",
    "    for name, tbl in tables.items():\n",
    "        df = getattr(tbl, \"df\", None)\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            pk = _infer_pk(tbl, df)\n",
    "            if pk and pk in df.columns:\n",
    "                pkmap.setdefault(pk, []).append((name, df, pk))\n",
    "\n",
    "    df_out = base_df.copy()\n",
    "    # For each column in base that matches a PK in the DB, left join the non-key attributes\n",
    "    for fk_col in list(df_out.columns):\n",
    "        if fk_col in pkmap:\n",
    "            for (tname, tdf, pk) in pkmap[fk_col]:\n",
    "                right = tdf.drop(columns=[pk], errors=\"ignore\").copy()\n",
    "                if right.empty:\n",
    "                    continue\n",
    "                # prefix joined columns to avoid collisions\n",
    "                right = right.add_prefix(f\"{tname}__\")\n",
    "                # attach the join key back (unprefixed) for merge\n",
    "                right[fk_col] = tdf[pk]\n",
    "                try:\n",
    "                    df_out = df_out.merge(right, on=fk_col, how=\"left\")\n",
    "                except Exception:\n",
    "                    # keep going; some keys may be non-joinable due to dtype issues\n",
    "                    pass\n",
    "    return df_out\n",
    "\n",
    "def build_single_table_frames(task, splits):\n",
    "    frames = {}\n",
    "    for split, table in splits.items():\n",
    "        df = table.df.copy()\n",
    "        # IMPORTANT: do not touch column masking/order; just drop the target to form X\n",
    "        X, y = to_Xy(df, task.target_col)\n",
    "        frames[split] = (X, y, df)  # keep original df for evaluation alignment\n",
    "    return frames\n",
    "\n",
    "def build_merged_table_frames(dataset, task, splits):\n",
    "    frames = {}\n",
    "    for split, table in splits.items():\n",
    "        base_df = table.df.copy()\n",
    "        merged_df = denormalize_one_hop(dataset, base_df)\n",
    "        X, y = to_Xy(merged_df, task.target_col)\n",
    "        frames[split] = (X, y, merged_df)\n",
    "    return frames\n"
   ],
   "id": "1cfbaca7c6d3b12c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T15:41:07.715651Z",
     "start_time": "2025-08-15T15:41:07.713059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- NEW CELL: skrub preprocessing helpers ---\n",
    "\n",
    "def build_tv():\n",
    "    \"\"\"\n",
    "    TableVectorizer turns mixed (numeric + categorical + text + datetime) columns\n",
    "    into a numeric feature matrix. We keep defaults to match the tutorials’ simplicity.\n",
    "    \"\"\"\n",
    "    return TableVectorizer()\n",
    "\n",
    "def fit_transform_splits(tv, X_train_df, X_val_df=None, X_test_df=None):\n",
    "    \"\"\"\n",
    "    Fit TV on train only; transform val/test. Cast to float32 for TabPFN.\n",
    "    \"\"\"\n",
    "    # Ensure no NaN or infinite values before vectorizing\n",
    "    X_train_df = X_train_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    if X_val_df is not None:\n",
    "        X_val_df = X_val_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    if X_test_df is not None:\n",
    "        X_test_df = X_test_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    Xt_train = tv.fit_transform(X_train_df).astype(np.float32)\n",
    "    Xt_val   = tv.transform(X_val_df).astype(np.float32) if X_val_df is not None else None\n",
    "    Xt_test  = tv.transform(X_test_df).astype(np.float32) if X_test_df is not None else None\n",
    "    return Xt_train, Xt_val, Xt_test, tv\n",
    "\n",
    "def _get_df(table):\n",
    "    # Your tutorial-style accessor: works whether object has `.df` or `.to_pandas()`\n",
    "    if hasattr(table, \"df\"):\n",
    "        return table.df\n",
    "    if hasattr(table, \"to_pandas\"):\n",
    "        return table.to_pandas()\n",
    "    raise ValueError(\"Unknown table type for conversion to DataFrame.\")\n"
   ],
   "id": "2ec47853dd80bd98",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T15:41:07.729613Z",
     "start_time": "2025-08-15T15:41:07.726764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Vectorization wrapper (version-safe for skrub/dirty_cat) ---\n",
    "# Place this where your previous TableVectorizer/vectorize_splits block was.\n",
    "\n",
    "def _make_table_vectorizer():\n",
    "    sig = inspect.signature(TableVectorizer.__init__)\n",
    "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
    "\n",
    "    tv_kwargs = {}\n",
    "\n",
    "    # Only set kwargs that actually exist in the installed version\n",
    "    if \"cardinality_threshold\" in allowed:\n",
    "        tv_kwargs[\"cardinality_threshold\"] = globals().get(\"CARDINALITY_THRESHOLD\", 1000)\n",
    "\n",
    "    # Some versions expose this; others don't — guard it\n",
    "    if \"high_cardinality_transformer\" in allowed:\n",
    "        tv_kwargs[\"high_cardinality_transformer\"] = globals().get(\"HIGH_CARD_TRANSFORMER\", \"hashing\")\n",
    "\n",
    "    # Optional knobs if you happen to define them globally and the version supports them\n",
    "    if \"text_separator\" in allowed and \"TEXT_SEPARATOR\" in globals():\n",
    "        tv_kwargs[\"text_separator\"] = globals()[\"TEXT_SEPARATOR\"]\n",
    "    if \"numerical_transformer\" in allowed and \"NUMERICAL_TRANSFORMER\" in globals():\n",
    "        tv_kwargs[\"numerical_transformer\"] = globals()[\"NUMERICAL_TRANSFORMER\"]\n",
    "    if \"categorical_transformer\" in allowed and \"CATEGORICAL_TRANSFORMER\" in globals():\n",
    "        tv_kwargs[\"categorical_transformer\"] = globals()[\"CATEGORICAL_TRANSFORMER\"]\n",
    "\n",
    "    return TableVectorizer(**tv_kwargs)\n",
    "\n",
    "def _to_dense(X):\n",
    "    try:\n",
    "        # scipy sparse matrices have .toarray()\n",
    "        return X.toarray() if hasattr(X, \"toarray\") else X\n",
    "    except Exception:\n",
    "        return X\n",
    "\n",
    "def vectorize_splits(X_train, X_val, X_test):\n",
    "    tv = _make_table_vectorizer()\n",
    "    Xt = _to_dense(tv.fit_transform(X_train))\n",
    "    Xv = _to_dense(tv.transform(X_val))\n",
    "    Xs = _to_dense(tv.transform(X_test))\n",
    "    return tv, Xt, Xv, Xs\n"
   ],
   "id": "ffb43a27c3ac51cb",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T15:41:07.744445Z",
     "start_time": "2025-08-15T15:41:07.739801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED   = globals().get(\"SEED\", 42)\n",
    "DEVICE = globals().get(\"DEVICE\", \"cuda\" if hasattr(np, \"__cuda_array_interface__\") else \"cpu\")\n",
    "N_ENSEMBLE = globals().get(\"N_ENSEMBLE\", 16)\n",
    "TABPFN_MAX = globals().get(\"TABPFN_MAX\", 1000)  # hard ceiling for TabPFN\n",
    "\n",
    "def _subsample(X, y, cap=TABPFN_MAX, seed=SEED):\n",
    "    if len(X) <= cap:\n",
    "        return X, y, np.arange(len(X))\n",
    "    idx = np.random.RandomState(seed).choice(len(X), size=cap, replace=False)\n",
    "    if hasattr(X, \"iloc\"):\n",
    "        Xs = X.iloc[idx]\n",
    "    else:\n",
    "        Xs = X[idx]\n",
    "    ys = y[idx]\n",
    "    return Xs, ys, idx\n",
    "\n",
    "def _fit_tabpfn(task, Xt, yt):\n",
    "    if task.task_type == TaskType.REGRESSION and TabPFNRegressor is not None:\n",
    "        model = TabPFNRegressor(\n",
    "            device=DEVICE,\n",
    "            #n_estimators=int(N_ENSEMBLE),\n",
    "        )\n",
    "    else:\n",
    "        model = TabPFNClassifier(\n",
    "            device=DEVICE,\n",
    "            #n_estimators=int(N_ENSEMBLE),\n",
    "        )\n",
    "    model.fit(Xt, yt)\n",
    "    return model\n",
    "\n",
    "def _predict_for_task(task, model, X):\n",
    "    # align with RelBench evaluators: AUROC expects probabilities for the positive class\n",
    "    if task.task_type == TaskType.REGRESSION:\n",
    "        return model.predict(X)\n",
    "    proba = model.predict_proba(X)\n",
    "    if task.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "        return proba[:, 1]\n",
    "    else:\n",
    "        # multiclass/multilabel: pass full probability matrix\n",
    "        return proba\n",
    "\n",
    "def run_tabpfn_on_task(dataset_name: str, task_name: str, mode: str = \"single\") -> Dict[str, Any]:\n",
    "    dataset = get_dataset(dataset_name, download=DOWNLOAD)\n",
    "    task, splits = fetch_splits(dataset_name, task_name, download=DOWNLOAD)\n",
    "\n",
    "    if mode == \"single\":\n",
    "        frames = build_single_table_frames(task, splits)\n",
    "    elif mode == \"merged\":\n",
    "        frames = build_merged_table_frames(dataset, task, splits)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'single' or 'merged'\")\n",
    "\n",
    "    (Xtr, ytr, _dftr) = frames[\"train\"]\n",
    "    (Xva, yva, dfva)  = frames[\"val\"]\n",
    "    (Xte, yte, dfte)  = frames[\"test\"]\n",
    "\n",
    "    # Vectorize\n",
    "    tv, Xt, Xv, Xs = vectorize_splits(Xtr, Xva, Xte)\n",
    "\n",
    "    # Respect TabPFN's sample cap\n",
    "    Xt_cap, yt_cap, _ = _subsample(Xt, ytr, cap=TABPFN_MAX, seed=SEED)\n",
    "\n",
    "    # Fit\n",
    "    model = _fit_tabpfn(task, Xt_cap, yt_cap)\n",
    "\n",
    "    # Predict & Evaluate with RelBench evaluators\n",
    "    val_pred  = _predict_for_task(task, model, Xv)\n",
    "    test_pred = _predict_for_task(task, model, Xs)\n",
    "\n",
    "    val_metrics  = task.evaluate(val_pred,  splits[\"val\"])   # documented API\n",
    "    test_metrics = task.evaluate(test_pred, splits[\"test\"])  # documented API\n",
    "\n",
    "    out = {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"task\": task_name,\n",
    "        \"mode\": mode,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"n_train_used\": len(Xt_cap),\n",
    "        \"n_train_total\": len(Xt),\n",
    "        \"n_val\": len(Xv),\n",
    "        \"n_test\": len(Xs),\n",
    "    }\n",
    "    return out\n"
   ],
   "id": "5c53f649ce01b948",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T15:41:07.816742Z",
     "start_time": "2025-08-15T15:41:07.754312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: HeteroData,\n",
    "        col_stats_dict: Dict[str, Dict[str, Dict[StatType, Any]]],\n",
    "        num_layers: int,\n",
    "        channels: int,\n",
    "        out_channels: int,\n",
    "        aggr: str,\n",
    "        norm: str,\n",
    "        # List of node types to add shallow embeddings to input\n",
    "        shallow_list: List[NodeType] = [],\n",
    "        # ID awareness\n",
    "        id_awareness: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = HeteroEncoder(\n",
    "            channels=channels,\n",
    "            node_to_col_names_dict={\n",
    "                node_type: data[node_type].tf.col_names_dict\n",
    "                for node_type in data.node_types\n",
    "            },\n",
    "            node_to_col_stats=col_stats_dict,\n",
    "        )\n",
    "        self.temporal_encoder = HeteroTemporalEncoder(\n",
    "            node_types=[\n",
    "                node_type for node_type in data.node_types if \"time\" in data[node_type]\n",
    "            ],\n",
    "            channels=channels,\n",
    "        )\n",
    "        self.gnn = HeteroGraphSAGE(\n",
    "            node_types=data.node_types,\n",
    "            edge_types=data.edge_types,\n",
    "            channels=channels,\n",
    "            aggr=aggr,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.head = MLP(\n",
    "            channels,\n",
    "            out_channels=out_channels,\n",
    "            norm=norm,\n",
    "            num_layers=1,\n",
    "        )\n",
    "        self.embedding_dict = ModuleDict(\n",
    "            {\n",
    "                node: Embedding(data.num_nodes_dict[node], channels)\n",
    "                for node in shallow_list\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.id_awareness_emb = None\n",
    "        if id_awareness:\n",
    "            self.id_awareness_emb = torch.nn.Embedding(1, channels)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.encoder.reset_parameters()\n",
    "        self.temporal_encoder.reset_parameters()\n",
    "        self.gnn.reset_parameters()\n",
    "        self.head.reset_parameters()\n",
    "        for embedding in self.embedding_dict.values():\n",
    "            torch.nn.init.normal_(embedding.weight, std=0.1)\n",
    "        if self.id_awareness_emb is not None:\n",
    "            self.id_awareness_emb.reset_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch: HeteroData,\n",
    "        entity_table: NodeType,\n",
    "    ) -> Tensor:\n",
    "        seed_time = batch[entity_table].seed_time\n",
    "        x_dict = self.encoder(batch.tf_dict)\n",
    "\n",
    "        rel_time_dict = self.temporal_encoder(\n",
    "            seed_time, batch.time_dict, batch.batch_dict\n",
    "        )\n",
    "\n",
    "        for node_type, rel_time in rel_time_dict.items():\n",
    "            x_dict[node_type] = x_dict[node_type] + rel_time\n",
    "\n",
    "        for node_type, embedding in self.embedding_dict.items():\n",
    "            x_dict[node_type] = x_dict[node_type] + embedding(batch[node_type].n_id)\n",
    "\n",
    "        x_dict = self.gnn(\n",
    "            x_dict,\n",
    "            batch.edge_index_dict,\n",
    "            batch.num_sampled_nodes_dict,\n",
    "            batch.num_sampled_edges_dict,\n",
    "        )\n",
    "\n",
    "        return self.head(x_dict[entity_table][: seed_time.size(0)])\n",
    "\n",
    "    def forward_dst_readout(\n",
    "        self,\n",
    "        batch: HeteroData,\n",
    "        entity_table: NodeType,\n",
    "        dst_table: NodeType,\n",
    "    ) -> Tensor:\n",
    "        if self.id_awareness_emb is None:\n",
    "            raise RuntimeError(\n",
    "                \"id_awareness must be set True to use forward_dst_readout\"\n",
    "            )\n",
    "        seed_time = batch[entity_table].seed_time\n",
    "        x_dict = self.encoder(batch.tf_dict)\n",
    "        # Add ID-awareness to the root node\n",
    "        x_dict[entity_table][: seed_time.size(0)] += self.id_awareness_emb.weight\n",
    "\n",
    "        rel_time_dict = self.temporal_encoder(\n",
    "            seed_time, batch.time_dict, batch.batch_dict\n",
    "        )\n",
    "\n",
    "        for node_type, rel_time in rel_time_dict.items():\n",
    "            x_dict[node_type] = x_dict[node_type] + rel_time\n",
    "\n",
    "        for node_type, embedding in self.embedding_dict.items():\n",
    "            x_dict[node_type] = x_dict[node_type] + embedding(batch[node_type].n_id)\n",
    "\n",
    "        x_dict = self.gnn(\n",
    "            x_dict,\n",
    "            batch.edge_index_dict,\n",
    "        )\n",
    "\n",
    "        return self.head(x_dict[dst_table])\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    data=data,\n",
    "    col_stats_dict=col_stats_dict,\n",
    "    num_layers=2,\n",
    "    channels=128,\n",
    "    out_channels=1,\n",
    "    aggr=\"sum\",\n",
    "    norm=\"batch_norm\",\n",
    ").to(DEVICE)"
   ],
   "id": "b42b4743087c99f0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T15:41:07.830141Z",
     "start_time": "2025-08-15T15:41:07.828385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- MODIFY: make test accept entity_table ---\n",
    "@torch.no_grad()\n",
    "def test(loader: NeighborLoader, entity_table: NodeType) -> np.ndarray:\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        pred = model(batch, entity_table)\n",
    "        pred = pred.view(-1) if pred.dim() > 1 and pred.size(1) == 1 else pred\n",
    "        pred_list.append(pred.detach().cpu())\n",
    "    return torch.cat(pred_list, dim=0).numpy()\n",
    "# --- END MODIFY ---"
   ],
   "id": "276a0d6b36910e13",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T16:59:58.928579Z",
     "start_time": "2025-08-15T15:41:07.842884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODES = globals().get(\"MODES\", [\"single\", \"merged\"])\n",
    "\n",
    "records = []\n",
    "failures = []\n",
    "\n",
    "for task_name in TASKS:\n",
    "    for mode in MODES:\n",
    "        try:\n",
    "            res = run_tabpfn_on_task(DATASET, task_name, mode=mode)\n",
    "            # Flatten metrics for val and test, one metric per row\n",
    "            for split in [\"val\", \"test\"]:\n",
    "                metrics = res.get(f\"{split}_metrics\") or {}\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    # Only add rows with non-empty metric_value\n",
    "                    if metric_value is not None and not (isinstance(metric_value, float) and np.isnan(metric_value)):\n",
    "                        records.append({\n",
    "                            \"dataset\": res.get(\"dataset\", DATASET),\n",
    "                            \"task\": res.get(\"task\", task_name),\n",
    "                            \"mode\": res.get(\"mode\", mode),\n",
    "                            \"method\": \"TabPFN_experimental_v1.0\",\n",
    "                            \"metric\": f\"{split}_{metric_name}\",\n",
    "                            \"score\": metric_value,\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            msg = f\"[{DATASET} | {task_name} | {mode}] failed: {e!s}\"\n",
    "            print(msg)\n",
    "            failures.append(msg)\n",
    "\n",
    "results_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "if results_df.empty:\n",
    "    print(\"No successful runs were recorded. Check the failure messages above.\")\n",
    "    # Create an empty, well-formed frame so downstream plotting code doesn't crash\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=[\"dataset\", \"task\", \"mode\", \"method\", \"metric\", \"score\"]\n",
    "    )\n",
    "else:\n",
    "    # Ensure required sort keys exist even if some rows missed them\n",
    "    for col in [\"task\", \"mode\"]:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = pd.NA\n",
    "    # Sort only by the columns that exist to avoid KeyError\n",
    "    sort_keys = [c for c in [\"task\", \"mode\", \"metric\"] if c in results_df.columns]\n",
    "    if sort_keys:\n",
    "        results_df = results_df.sort_values(sort_keys)\n",
    "\n",
    "display(results_df)\n"
   ],
   "id": "be6242da806a8050",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'rel-avito/db.zip' from 'https://relbench.stanford.edu/download/rel-avito/db.zip' to '/Users/michaelflppv/Library/Caches/relbench'.\n",
      "100%|████████████████████████████████████████| 364M/364M [00:00<00:00, 195GB/s]\n",
      "Unzipping contents of '/Users/michaelflppv/Library/Caches/relbench/rel-avito/db.zip' to '/Users/michaelflppv/Library/Caches/relbench/rel-avito/.'\n",
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/regressor.py:494: UserWarning: Running on CPU with more than 200 samples may be slow.\n",
      "Consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client\n",
      "  check_cpu_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rel-avito | ad-ctr | single] failed: got an unexpected keyword argument 'squared'\n",
      "Loading Database object from /Users/michaelflppv/Library/Caches/relbench/rel-avito/db...\n",
      "Done in 1.40 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/regressor.py:494: UserWarning: Running on CPU with more than 200 samples may be slow.\n",
      "Consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client\n",
      "  check_cpu_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rel-avito | ad-ctr | merged] failed: got an unexpected keyword argument 'squared'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/classifier.py:462: UserWarning: Running on CPU with more than 200 samples may be slow.\n",
      "Consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client\n",
      "  check_cpu_warning(\n",
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/classifier.py:462: UserWarning: Running on CPU with more than 200 samples may be slow.\n",
      "Consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client\n",
      "  check_cpu_warning(\n",
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/classifier.py:462: UserWarning: Running on CPU with more than 200 samples may be slow.\n",
      "Consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client\n",
      "  check_cpu_warning(\n",
      "/Users/michaelflppv/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/classifier.py:462: UserWarning: Running on CPU with more than 200 samples may be slow.\n",
      "Consider using a GPU or the tabpfn-client API: https://github.com/PriorLabs/tabpfn-client\n",
      "  check_cpu_warning(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      dataset         task    mode                    method  \\\n",
       "29  rel-avito  user-clicks  merged  TabPFN_experimental_v1.0   \n",
       "28  rel-avito  user-clicks  merged  TabPFN_experimental_v1.0   \n",
       "30  rel-avito  user-clicks  merged  TabPFN_experimental_v1.0   \n",
       "31  rel-avito  user-clicks  merged  TabPFN_experimental_v1.0   \n",
       "25  rel-avito  user-clicks  merged  TabPFN_experimental_v1.0   \n",
       "24  rel-avito  user-clicks  merged  TabPFN_experimental_v1.0   \n",
       "26  rel-avito  user-clicks  merged  TabPFN_experimental_v1.0   \n",
       "27  rel-avito  user-clicks  merged  TabPFN_experimental_v1.0   \n",
       "21  rel-avito  user-clicks  single  TabPFN_experimental_v1.0   \n",
       "20  rel-avito  user-clicks  single  TabPFN_experimental_v1.0   \n",
       "22  rel-avito  user-clicks  single  TabPFN_experimental_v1.0   \n",
       "23  rel-avito  user-clicks  single  TabPFN_experimental_v1.0   \n",
       "17  rel-avito  user-clicks  single  TabPFN_experimental_v1.0   \n",
       "16  rel-avito  user-clicks  single  TabPFN_experimental_v1.0   \n",
       "18  rel-avito  user-clicks  single  TabPFN_experimental_v1.0   \n",
       "19  rel-avito  user-clicks  single  TabPFN_experimental_v1.0   \n",
       "13  rel-avito  user-visits  merged  TabPFN_experimental_v1.0   \n",
       "12  rel-avito  user-visits  merged  TabPFN_experimental_v1.0   \n",
       "14  rel-avito  user-visits  merged  TabPFN_experimental_v1.0   \n",
       "15  rel-avito  user-visits  merged  TabPFN_experimental_v1.0   \n",
       "9   rel-avito  user-visits  merged  TabPFN_experimental_v1.0   \n",
       "8   rel-avito  user-visits  merged  TabPFN_experimental_v1.0   \n",
       "10  rel-avito  user-visits  merged  TabPFN_experimental_v1.0   \n",
       "11  rel-avito  user-visits  merged  TabPFN_experimental_v1.0   \n",
       "5   rel-avito  user-visits  single  TabPFN_experimental_v1.0   \n",
       "4   rel-avito  user-visits  single  TabPFN_experimental_v1.0   \n",
       "6   rel-avito  user-visits  single  TabPFN_experimental_v1.0   \n",
       "7   rel-avito  user-visits  single  TabPFN_experimental_v1.0   \n",
       "1   rel-avito  user-visits  single  TabPFN_experimental_v1.0   \n",
       "0   rel-avito  user-visits  single  TabPFN_experimental_v1.0   \n",
       "2   rel-avito  user-visits  single  TabPFN_experimental_v1.0   \n",
       "3   rel-avito  user-visits  single  TabPFN_experimental_v1.0   \n",
       "\n",
       "                    metric     score  \n",
       "29           test_accuracy  0.984582  \n",
       "28  test_average_precision  0.016325  \n",
       "30                 test_f1  0.000000  \n",
       "31            test_roc_auc  0.505855  \n",
       "25            val_accuracy  0.964830  \n",
       "24   val_average_precision  0.035672  \n",
       "26                  val_f1  0.000000  \n",
       "27             val_roc_auc  0.500207  \n",
       "21           test_accuracy  0.984582  \n",
       "20  test_average_precision  0.016325  \n",
       "22                 test_f1  0.000000  \n",
       "23            test_roc_auc  0.505855  \n",
       "17            val_accuracy  0.964830  \n",
       "16   val_average_precision  0.035672  \n",
       "18                  val_f1  0.000000  \n",
       "19             val_roc_auc  0.500207  \n",
       "13           test_accuracy  0.850591  \n",
       "12  test_average_precision  0.847905  \n",
       "14                 test_f1  0.919264  \n",
       "15            test_roc_auc  0.495793  \n",
       "9             val_accuracy  0.903499  \n",
       "8    val_average_precision  0.904463  \n",
       "10                  val_f1  0.949303  \n",
       "11             val_roc_auc  0.496614  \n",
       "5            test_accuracy  0.850591  \n",
       "4   test_average_precision  0.847905  \n",
       "6                  test_f1  0.919264  \n",
       "7             test_roc_auc  0.495793  \n",
       "1             val_accuracy  0.903499  \n",
       "0    val_average_precision  0.904463  \n",
       "2                   val_f1  0.949303  \n",
       "3              val_roc_auc  0.496614  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>task</th>\n",
       "      <th>mode</th>\n",
       "      <th>method</th>\n",
       "      <th>metric</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.984582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_average_precision</td>\n",
       "      <td>0.016325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_f1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_roc_auc</td>\n",
       "      <td>0.505855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_accuracy</td>\n",
       "      <td>0.964830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_average_precision</td>\n",
       "      <td>0.035672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_f1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_roc_auc</td>\n",
       "      <td>0.500207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.984582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_average_precision</td>\n",
       "      <td>0.016325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_f1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_roc_auc</td>\n",
       "      <td>0.505855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_accuracy</td>\n",
       "      <td>0.964830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_average_precision</td>\n",
       "      <td>0.035672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_f1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-clicks</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_roc_auc</td>\n",
       "      <td>0.500207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.850591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_average_precision</td>\n",
       "      <td>0.847905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_f1</td>\n",
       "      <td>0.919264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_roc_auc</td>\n",
       "      <td>0.495793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_accuracy</td>\n",
       "      <td>0.903499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_average_precision</td>\n",
       "      <td>0.904463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_f1</td>\n",
       "      <td>0.949303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>merged</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_roc_auc</td>\n",
       "      <td>0.496614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.850591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_average_precision</td>\n",
       "      <td>0.847905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_f1</td>\n",
       "      <td>0.919264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>test_roc_auc</td>\n",
       "      <td>0.495793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_accuracy</td>\n",
       "      <td>0.903499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_average_precision</td>\n",
       "      <td>0.904463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_f1</td>\n",
       "      <td>0.949303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rel-avito</td>\n",
       "      <td>user-visits</td>\n",
       "      <td>single</td>\n",
       "      <td>TabPFN_experimental_v1.0</td>\n",
       "      <td>val_roc_auc</td>\n",
       "      <td>0.496614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T16:59:58.958543Z",
     "start_time": "2025-08-15T16:59:58.954023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Persist results to CSV ---\n",
    "\n",
    "out_dir = globals().get(\"OUT_DIR\", \"outputs\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Change timestamp format to \"dd.mm.yyyy-hh:mm\"\n",
    "timestamp = time.strftime(\"%d.%m.%Y-%H:%M\")\n",
    "csv_name = f\"tabpfn_{DATASET}_{timestamp}.csv\"\n",
    "csv_path = os.path.join(out_dir, csv_name)\n",
    "\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved results to: {csv_path}\")\n"
   ],
   "id": "aa929637db7d0d54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: outputs/tabpfn_rel-avito_15.08.2025-18:59.csv\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
