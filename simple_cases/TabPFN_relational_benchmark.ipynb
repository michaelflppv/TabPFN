{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **TabPFN Relational Benchmark**\n",
    "This notebook benchmarks the performance of TabPFN models on datasets from RelBench in two scenarios:\n",
    "1. **Single Table** – Using only the target entity table.\n",
    "2. **Merged Table** – Using a naively denormalized table obtained by joining related tables.\n",
    "\n",
    "It automates dataset loading, preprocessing (including date feature engineering), vectorization, model training, prediction, and evaluation for all compatible tasks within a chosen RelBench dataset. The results allow comparing model performance between single-table and merged-table configurations.\n"
   ],
   "id": "7ab5a9bf5466de0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "id": "ac93e97c56d4b905"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:37.621884Z",
     "start_time": "2025-08-21T09:32:35.109798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import inspect\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Skrub / Sentence Transformers ---\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "# --- RelBench ---\n",
    "from relbench.datasets import get_dataset\n",
    "from relbench.tasks import get_task, get_task_names\n",
    "from relbench.base import TaskType\n",
    "import relbench.metrics\n",
    "\n",
    "# --- TabPFN ---\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "\n",
    "# --- Featuretools ---\n",
    "import featuretools as ft"
   ],
   "id": "57624fe02c838842",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:37.653308Z",
     "start_time": "2025-08-21T09:32:37.651890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ],
   "id": "ebce2e3934db1e43",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set Global Configuration",
   "id": "7154198e21fe2a01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:37.685083Z",
     "start_time": "2025-08-21T09:32:37.682985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Device selection (CPU, CUDA, or MPS if available)\n",
    "def get_device():\n",
    "    # Uncomment for auto-detection\n",
    "    # if torch.backends.mps.is_available():\n",
    "    #     return \"mps\"\n",
    "    # elif torch.cuda.is_available():\n",
    "    #     return \"cuda\"\n",
    "    # else:\n",
    "    #     return \"cpu\"\n",
    "    return \"cpu\"  # Default: CPU\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Dataset and experiment settings\n",
    "DATASET        = globals().get(\"DATASET\", \"rel-f1\")           # Default dataset\n",
    "SEED           = globals().get(\"SEED\", 42)                    # Random seed\n",
    "N_ESTIMATORS   = globals().get(\"N_ESTIMATORS\", 16)            # TabPFN estimators\n",
    "TABPFN_MAX     = globals().get(\"TABPFN_MAX\", 10000)            # Max TabPFN samples\n",
    "GETML_PROJECT  = globals().get(\"GETML_PROJECT\", \"default_project\")  # getML project name\n",
    "\n",
    "# getML engine state\n",
    "_ENGINE_STARTED = False\n",
    "\n",
    "# Optional: quiet Featuretools logs\n",
    "ft.config.log_print_threshold = 1000000"
   ],
   "id": "c5959e09ecd4c20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Featuretools knobs (safe defaults)",
   "id": "4c5b68b64b21e661"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:37.708370Z",
     "start_time": "2025-08-21T09:32:37.706496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---- Featuretools knobs (safe defaults) ----\n",
    "FT_MAX_DEPTH = globals().get(\"FT_MAX_DEPTH\", 2)   # bump to 2 to surface relational signal\n",
    "\n",
    "# Primitive wishlists resolved at runtime (see resolver cell)\n",
    "FT_AGG_PRIMITIVES_WISHLIST = [\n",
    "    \"mean\", \"sum\", \"count\", \"n_unique\", \"max\", \"min\", \"std\", \"mode\"\n",
    "]\n",
    "FT_TRANS_PRIMITIVES_WISHLIST = [\"day\", \"month\", \"year\", \"weekday\", \"hour\"]\n",
    "\n",
    "# PK/FK inference sensitivity (fraction of child FK values found in parent PK)\n",
    "FK_COVERAGE_THRESHOLD = globals().get(\"FK_COVERAGE_THRESHOLD\", 0.95)\n",
    "\n",
    "# When building model inputs, drop raw join keys to avoid trivial leakage\n",
    "DROP_JOIN_KEYS_FROM_X = globals().get(\"DROP_JOIN_KEYS_FROM_X\", True)\n"
   ],
   "id": "701d9b49628bf0e9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Notebook Configuration and Dataset Selection\n",
    "\n",
    "\n",
    "Sets the dataset name (`DATASET`) and download flag (`DOWNLOAD`), then discovers all available tasks for the selected dataset using RelBench’s APIs. Filters tasks to only those compatible with TabPFN (classification and regression).\n"
   ],
   "id": "a86eaa89573fdf63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:37.727640Z",
     "start_time": "2025-08-21T09:32:37.722652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reuse existing config if present, otherwise set defaults\n",
    "DATASET = globals().get(\"DATASET\", \"rel-f1\")\n",
    "DOWNLOAD = globals().get(\"DOWNLOAD\", True)\n",
    "\n",
    "# Discover tasks and keep only entity-level cls/reg tasks TabPFN can handle\n",
    "def _is_tabpfn_friendly(task):\n",
    "    return task.task_type in (\n",
    "        TaskType.BINARY_CLASSIFICATION,\n",
    "        TaskType.MULTICLASS_CLASSIFICATION,\n",
    "        TaskType.MULTILABEL_CLASSIFICATION,\n",
    "        TaskType.REGRESSION,\n",
    "    )\n",
    "\n",
    "_all = get_task_names(DATASET)  # shown in tutorials\n",
    "TASKS = []\n",
    "for tname in _all:\n",
    "    try:\n",
    "        t = get_task(DATASET, tname, download=DOWNLOAD)\n",
    "        if _is_tabpfn_friendly(t):\n",
    "            TASKS.append(tname)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {tname}: {e!s}\")\n",
    "\n",
    "print(f\"{DATASET}: {len(TASKS)} TabPFN-friendly tasks -> {TASKS}\")\n"
   ],
   "id": "d94075cf88bf6806",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel-f1: 3 TabPFN-friendly tasks -> ['driver-position', 'driver-dnf', 'driver-top3']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Patch RelBench Metrics (Optional)",
   "id": "affc15acb6a0f8aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:37.738856Z",
     "start_time": "2025-08-21T09:32:37.737099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Patch relbench.metrics.skm.mean_squared_error to local mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "relbench.metrics.skm.mean_squared_error = mean_squared_error\n",
    "\n",
    "def patched_rmse(true, pred):\n",
    "    if \"squared\" in inspect.signature(mean_squared_error).parameters:\n",
    "        return mean_squared_error(true, pred, squared=False)\n",
    "    else:\n",
    "        return np.sqrt(mean_squared_error(true, pred))\n",
    "\n",
    "relbench.metrics.rmse = patched_rmse"
   ],
   "id": "46fcd0d47afb30c1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fetch Dataset Splits\n",
    "\n",
    "Utility functions to load a task’s splits (`train`, `val`, `test`), convert them to pandas DataFrames, and extract features (`X`) and targets (`y`). Includes functions to:\n",
    "* Load train/val/test splits for a task.\n",
    "* Extract features/targets.\n",
    "* Infer primary keys.\n",
    "* Denormalize tables (one-hop join).\n",
    "* Build data frames for both single-table and merged-table scenarios.\n"
   ],
   "id": "1a35d633effdd263"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset loaders / frame builders",
   "id": "4887e13ee3e11db1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:37.752843Z",
     "start_time": "2025-08-21T09:32:37.750282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_splits(dataset_name: str, task_name: str, download: bool = True):\n",
    "    task = get_task(dataset_name, task_name, download=download)\n",
    "    # keep original columns (mask_input_cols=False so we see raw fields)\n",
    "    splits = {\n",
    "        split: task.get_table(split, mask_input_cols=False)\n",
    "        for split in (\"train\", \"val\", \"test\")\n",
    "    }\n",
    "    return task, splits\n",
    "\n",
    "def to_Xy(df: pd.DataFrame, target_col: str):\n",
    "    y = df[target_col].to_numpy()\n",
    "    X = df.drop(columns=[target_col])\n",
    "    return X, y\n",
    "\n",
    "def build_single_table_frames(task, splits):\n",
    "    \"\"\"\n",
    "    Single-table mode: do NOT engineer features here.\n",
    "    Just return raw base table X, y per split (target dropped from X).\n",
    "    \"\"\"\n",
    "    frames = {}\n",
    "    for split, table in splits.items():\n",
    "        df = table.df.copy()\n",
    "        X, y = to_Xy(df, task.target_col)\n",
    "        frames[split] = (X, y, df)\n",
    "    return frames\n"
   ],
   "id": "ac5f4a739cba0093",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Featuretools helpers: dtype cleanup, PK/FK inference, ES builders",
   "id": "27fbf5f3011bd7f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:37.771343Z",
     "start_time": "2025-08-21T09:32:37.760977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _clean_for_ft(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make pandas dtypes friendly to Woodwork/Featuretools:\n",
    "    - Keep nullable ints as IntegerNullable where possible,\n",
    "    - Parse datetimes only for columns that strongly look like time fields.\n",
    "    \"\"\"\n",
    "    x = df.copy()\n",
    "    for c in x.columns:\n",
    "        # Ensure datetimes are parsed only for proper time-like names\n",
    "        if _looks_like_time_name(c) and not pd.api.types.is_datetime64_any_dtype(x[c]):\n",
    "            try:\n",
    "                parsed = pd.to_datetime(x[c], errors=\"coerce\", utc=False)\n",
    "                # keep as datetime if we have any valid values and more than 1 unique\n",
    "                if parsed.notna().sum() > 0 and parsed.nunique(dropna=True) > 1:\n",
    "                    x[c] = parsed\n",
    "            except Exception:\n",
    "                pass\n",
    "    return x\n",
    "\n",
    "\n",
    "def _candidate_pk(df: pd.DataFrame, table_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Deterministic PK selection:\n",
    "      1) exact 'id', 'Id', 'ID', f'{table}_id', f'{table}_ID', or any '*_id', '*Id', '*_ID', '*ID' that is unique and non-null\n",
    "      2) otherwise any column that is fully unique and non-null\n",
    "      3) else None  (we will make an index)\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "    priorities = (\n",
    "        [\"id\", \"Id\", \"ID\", f\"{table_name}_id\", f\"{table_name}_ID\"]\n",
    "        + [c for c in cols if c.lower().endswith(\"_id\")]\n",
    "        + [c for c in cols if c.endswith(\"Id\")]\n",
    "        + [c for c in cols if c.endswith(\"ID\")]\n",
    "    )\n",
    "    def is_good_key(s: pd.Series) -> bool:\n",
    "        return s.notna().all() and s.is_unique\n",
    "    for c in priorities:\n",
    "        if c in df.columns and is_good_key(df[c]):\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        s = df[c]\n",
    "        if is_good_key(s):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _infer_pk_fk_graph(tables: Dict[str, pd.DataFrame]) -> Dict[str, Dict[str, Any]]:\n",
    "    pkeys: Dict[str, Optional[str]] = {}\n",
    "    for tname, df in tables.items():\n",
    "        pkeys[tname] = _candidate_pk(df, tname)\n",
    "\n",
    "    parent_values = {}\n",
    "    for parent, pk in pkeys.items():\n",
    "        if pk is None:\n",
    "            continue\n",
    "        vals = pd.Series(tables[parent][pk]).dropna().astype(str).unique()\n",
    "        parent_values[parent] = set(vals.tolist())\n",
    "\n",
    "    fkeys: Dict[str, List[Dict[str, Any]]] = {parent: [] for parent in tables.keys()}\n",
    "    reasons: List[str] = []\n",
    "\n",
    "    for parent, pk in pkeys.items():\n",
    "        if pk is None:\n",
    "            continue\n",
    "        pset = parent_values.get(parent, set())\n",
    "        if not pset:\n",
    "            continue\n",
    "        sample_parent = pd.Series(list(pset))\n",
    "        for child, cdf in tables.items():\n",
    "            if child == parent:\n",
    "                continue\n",
    "            # quick prefilter: columns that look like keys\n",
    "            for col in cdf.columns:\n",
    "                if col.lower().endswith(\"_id\") or col == pk or col.lower() == pk.lower():\n",
    "                    s = cdf[col].dropna()\n",
    "                    if len(s) == 0:\n",
    "                        continue\n",
    "                    # compatible cardinalities\n",
    "                    if s.nunique(dropna=True) == 1:\n",
    "                        continue\n",
    "                    coverage = (s.astype(str).isin(sample_parent)).mean()\n",
    "                    if coverage >= FK_COVERAGE_THRESHOLD:\n",
    "                        fkeys[parent].append({\"child\": child, \"fk\": col, \"coverage\": float(coverage)})\n",
    "                        reasons.append(f\"{parent}.{pk} <- {child}.{col} (coverage={coverage:.3f})\")\n",
    "    return {\"pkeys\": pkeys, \"fkeys\": fkeys, \"debug\": {\"reasons\": reasons}}\n",
    "\n",
    "\n",
    "_TIME_NAME_PATTERN = re.compile(\n",
    "    r\"(^|[_])(\"\n",
    "    r\"timestamp|datetime|event_time|eventtime|time|date|dt|\"\n",
    "    r\"created_at|updated_at|inserted_at|occurred_at|recorded_at\"\n",
    "    r\")([_]|$)\",\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def _looks_like_time_name(colname: str) -> bool:\n",
    "    \"\"\"\n",
    "    Strictly decide if a column name is time-like:\n",
    "    - Matches whole-word tokens (e.g., 'order_date', 'event_time', 'created_at', 'datetime')\n",
    "    - Accepts exact 'ts' or 'ts_*' / '*_ts' specifically, but not substrings inside other words.\n",
    "    \"\"\"\n",
    "    name = str(colname)\n",
    "    if name.lower() in {\"ts\"}:\n",
    "        return True\n",
    "    if name.lower().startswith(\"ts_\") or name.lower().endswith(\"_ts\"):\n",
    "        return True\n",
    "    return _TIME_NAME_PATTERN.search(name) is not None\n",
    "\n",
    "def _detect_time_col(df: pd.DataFrame, pk: Optional[str] = None) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Pick a valid datetime column that is NOT the PK.\n",
    "    - Must be datetime64 after parsing attempt,\n",
    "    - Must have >1 unique non-null values (so aggregates can vary),\n",
    "    - Name must look like a time field using _looks_like_time_name.\n",
    "    \"\"\"\n",
    "    for c in df.columns:\n",
    "        if c == pk:\n",
    "            continue\n",
    "        if not _looks_like_time_name(c):\n",
    "            continue\n",
    "        s = df[c]\n",
    "        if not pd.api.types.is_datetime64_any_dtype(s):\n",
    "            try:\n",
    "                s = pd.to_datetime(s, errors=\"coerce\", utc=False)\n",
    "            except Exception:\n",
    "                continue\n",
    "        if s.notna().sum() > 0 and s.nunique(dropna=True) > 1:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _make_es_for_split(base_name: str,\n",
    "                       pop_df: pd.DataFrame,\n",
    "                       all_tables: Dict[str, pd.DataFrame],\n",
    "                       schema: Dict[str, Dict[str, Any]]) -> ft.EntitySet:\n",
    "    es = ft.EntitySet(id=f\"rb_es_{base_name}\")\n",
    "\n",
    "    # population\n",
    "    pop_df = _clean_for_ft(pop_df)\n",
    "    pop_pk = schema[\"pkeys\"].get(base_name)\n",
    "    pop_time = _detect_time_col(pop_df, pk=pop_pk)\n",
    "\n",
    "    def _add_df_safe(es_obj, name, df, pk_candidate):\n",
    "        # choose index\n",
    "        if pk_candidate is None or pk_candidate not in df.columns:\n",
    "            idx = f\"{name}__ft_index\"\n",
    "            make_idx = True\n",
    "        else:\n",
    "            idx = pk_candidate\n",
    "            make_idx = False\n",
    "        # choose time_index (must be present, datetime64, != index)\n",
    "        tcol = _detect_time_col(df, pk=idx)\n",
    "        if tcol is not None and tcol == idx:\n",
    "            tcol = None\n",
    "        # pass only if truly datetime\n",
    "        if tcol is not None and not pd.api.types.is_datetime64_any_dtype(df[tcol]):\n",
    "            tcol = None\n",
    "\n",
    "        # add\n",
    "        if make_idx:\n",
    "            return es_obj.add_dataframe(\n",
    "                dataframe_name=name, dataframe=df, index=idx, make_index=True, time_index=tcol\n",
    "            ), idx\n",
    "        else:\n",
    "            return es_obj.add_dataframe(\n",
    "                dataframe_name=name, dataframe=df, index=idx, time_index=tcol\n",
    "            ), idx\n",
    "\n",
    "    es, pop_idx = _add_df_safe(es, base_name, pop_df, pop_pk)\n",
    "\n",
    "    # peripherals\n",
    "    for tname, df in all_tables.items():\n",
    "        if tname == base_name:\n",
    "            continue\n",
    "        df2 = _clean_for_ft(df)\n",
    "        pk = schema[\"pkeys\"].get(tname)\n",
    "        es, _ = _add_df_safe(es, tname, df2, pk)\n",
    "\n",
    "    # relationships\n",
    "    for parent, rels in schema[\"fkeys\"].items():\n",
    "        if parent not in es.dataframe_dict:\n",
    "            continue\n",
    "        parent_pk = schema[\"pkeys\"].get(parent) or f\"{parent}__ft_index\"\n",
    "        for r in rels:\n",
    "            child = r[\"child\"]; fk = r[\"fk\"]\n",
    "            if child in es.dataframe_dict and fk in es[child].ww.columns:\n",
    "                es = es.add_relationship(parent_dataframe_name=parent,\n",
    "                                         parent_column_name=parent_pk,\n",
    "                                         child_dataframe_name=child,\n",
    "                                         child_column_name=fk)\n",
    "    # minimal debug\n",
    "    #print(f\"[ES] Added dataframes={list(es.dataframe_dict.keys())} | relationships={len(es.relationships)}\")\n",
    "    return es\n",
    "\n",
    "\n",
    "def _dfs_feature_matrices(es_train: ft.EntitySet,\n",
    "                          es_val: ft.EntitySet,\n",
    "                          es_test: ft.EntitySet,\n",
    "                          target_df: str):\n",
    "    agg_prims, trans_prims = _resolve_ft_primitives(\n",
    "        FT_AGG_PRIMITIVES_WISHLIST, FT_TRANS_PRIMITIVES_WISHLIST\n",
    "    )\n",
    "\n",
    "    # simple sanity logs\n",
    "    #print(f\"[DFS] agg={agg_prims} | trans={trans_prims}\")\n",
    "    #print(f\"[DFS] relationships(train)={len(es_train.relationships)}\")\n",
    "\n",
    "    fm_train, fdefs = ft.dfs(\n",
    "        entityset=es_train,\n",
    "        target_dataframe_name=target_df,\n",
    "        agg_primitives=agg_prims,\n",
    "        trans_primitives=trans_prims,\n",
    "        max_depth=FT_MAX_DEPTH,\n",
    "        features_only=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    fm_val = ft.calculate_feature_matrix(features=fdefs, entityset=es_val, verbose=False)\n",
    "    fm_test = ft.calculate_feature_matrix(features=fdefs, entityset=es_test, verbose=False)\n",
    "\n",
    "    fm_val = fm_val.reindex(columns=fm_train.columns, fill_value=np.nan)\n",
    "    fm_test = fm_test.reindex(columns=fm_train.columns, fill_value=np.nan)\n",
    "    return fm_train, fm_val, fm_test, fdefs\n"
   ],
   "id": "32f1a606727db779",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Resolve FT primitive names to what's actually available",
   "id": "821c7c94217dab48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:37.779870Z",
     "start_time": "2025-08-21T09:32:37.776924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _resolve_ft_primitives(\n",
    "    agg_wishlist: list,\n",
    "    trans_wishlist: list\n",
    "):\n",
    "    \"\"\"\n",
    "    Inspects installed Featuretools primitives and returns (agg_list, trans_list)\n",
    "    with only supported names, mapping common aliases where possible.\n",
    "    \"\"\"\n",
    "    prim_df = ft.primitives.list_primitives()\n",
    "    have_agg = set(prim_df[prim_df.type == \"aggregation\"].name.str.lower())\n",
    "    have_trans = set(prim_df[prim_df.type == \"transform\"].name.str.lower())\n",
    "\n",
    "    # Common alias sets for version differences\n",
    "    alias_groups_agg = [\n",
    "        {\"n_unique\", \"nunique\", \"num_unique\", \"count_unique\"},\n",
    "        {\"mode\", \"mode_agg\"},  # sometimes exposed with a suffix\n",
    "        {\"std\", \"standard_deviation\"},\n",
    "        {\"count\"}  # count is stable but keep set logic uniform\n",
    "    ]\n",
    "    alias_groups_trans = [\n",
    "        {\"weekday\", \"week_day\"},\n",
    "        {\"year\"},\n",
    "        {\"month\"},\n",
    "        {\"day\"},\n",
    "        {\"hour\"},\n",
    "    ]\n",
    "\n",
    "    def pick_available(wishlist, have_set, alias_groups):\n",
    "        out = []\n",
    "        lower_wish = [w.lower() for w in wishlist]\n",
    "        for w in lower_wish:\n",
    "            # direct\n",
    "            if w in have_set:\n",
    "                out.append(w)\n",
    "                continue\n",
    "            # alias\n",
    "            matched = False\n",
    "            for group in alias_groups:\n",
    "                if w in group:\n",
    "                    # pick first available from group\n",
    "                    for cand in group:\n",
    "                        if cand in have_set:\n",
    "                            out.append(cand)\n",
    "                            matched = True\n",
    "                            break\n",
    "                if matched:\n",
    "                    break\n",
    "            # if still not matched, skip silently\n",
    "        # Deduplicate while preserving order\n",
    "        seen = set()\n",
    "        uniq = []\n",
    "        for p in out:\n",
    "            if p not in seen:\n",
    "                seen.add(p)\n",
    "                uniq.append(p)\n",
    "        return uniq\n",
    "\n",
    "    agg = pick_available(agg_wishlist, have_agg, alias_groups_agg)\n",
    "    trans = pick_available(trans_wishlist, have_trans, alias_groups_trans)\n",
    "\n",
    "    if len(agg) == 0:\n",
    "        # minimally guarantee some aggregations\n",
    "        for fallback in [\"mean\", \"sum\", \"count\", \"max\", \"min\"]:\n",
    "            if fallback in have_agg:\n",
    "                agg.append(fallback)\n",
    "        agg = list(dict.fromkeys(agg))  # dedupe\n",
    "\n",
    "    return agg, trans\n"
   ],
   "id": "54f3d090e026aa8c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build Merged-Table Frames",
   "id": "48d1dac2d154369a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:37.793659Z",
     "start_time": "2025-08-21T09:32:37.789815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_merged_table_frames(dataset, task, splits):\n",
    "    \"\"\"\n",
    "    Build merged features with Featuretools:\n",
    "    - Detect PK/FK pairs from the raw tables (deterministic rules, no guessing),\n",
    "    - Build an EntitySet per split with the same relationships,\n",
    "    - Generate relational aggregates via DFS,\n",
    "    - Drop raw join keys (optional) and target from inputs,\n",
    "    - Return (X, y, engineered_df) per split to match the rest of the notebook.\n",
    "    \"\"\"\n",
    "    # --- Load DB tables (pandas) ---\n",
    "    db = dataset.get_db()\n",
    "    all_tables: Dict[str, pd.DataFrame] = {\n",
    "        name: (tbl.df.copy() if hasattr(tbl, \"df\") else tbl.to_pandas())\n",
    "        for name, tbl in db.table_dict.items()\n",
    "    }\n",
    "\n",
    "    # Identify the population table name used by RelBench task tables\n",
    "    base_name = getattr(splits[\"train\"], \"name\", \"population\")\n",
    "\n",
    "    # Clean tables for FT typing\n",
    "    all_tables = {k: _clean_for_ft(v) for k, v in all_tables.items()}\n",
    "\n",
    "    # --- Infer PK/FK graph using Woodwork typing + coverage checks ---\n",
    "    schema = _infer_pk_fk_graph(all_tables)\n",
    "\n",
    "    # If FT didn't find a PK for the population, ensure we produce one later\n",
    "    if schema[\"pkeys\"].get(base_name) is None and base_name in all_tables:\n",
    "        # let _make_es_for_split create an artificial index\n",
    "        pass\n",
    "\n",
    "    # --- Build EntitySets per split ---\n",
    "    pop = {split: tbl.df.copy() for split, tbl in splits.items()}  # keep your original behavior\n",
    "    es_train = _make_es_for_split(base_name, pop[\"train\"], all_tables, schema)\n",
    "    es_val   = _make_es_for_split(base_name, pop[\"val\"],   all_tables, schema)\n",
    "    es_test  = _make_es_for_split(base_name, pop[\"test\"],  all_tables, schema)\n",
    "\n",
    "    # --- DFS -> aligned feature matrices ---\n",
    "    fe_train, fe_val, fe_test, feature_defs = _dfs_feature_matrices(es_train, es_val, es_test, target_df=base_name)\n",
    "\n",
    "    # --- Prepare (X, y), optionally drop raw join keys from model inputs ---\n",
    "    def _drop_keys(dfX: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not DROP_JOIN_KEYS_FROM_X:\n",
    "            return dfX\n",
    "        drop_cols = set()\n",
    "\n",
    "        # population PK\n",
    "        pop_pk = schema[\"pkeys\"].get(base_name)\n",
    "        if pop_pk and pop_pk in dfX.columns:\n",
    "            drop_cols.add(pop_pk)\n",
    "\n",
    "        # any FKs that appear on population table columns\n",
    "        for parent, rels in schema[\"fkeys\"].items():\n",
    "            for r in rels:\n",
    "                fk = r[\"fk\"]\n",
    "                if fk in dfX.columns:\n",
    "                    drop_cols.add(fk)\n",
    "\n",
    "        return dfX.drop(columns=list(drop_cols), errors=\"ignore\")\n",
    "\n",
    "    def _to_Xy_safe(fe_df: pd.DataFrame, raw_df: pd.DataFrame):\n",
    "        # Ensure target is not part of X\n",
    "        X = fe_df.copy()\n",
    "        if task.target_col in X.columns:\n",
    "            X = X.drop(columns=[task.target_col], errors=\"ignore\")\n",
    "        X = _drop_keys(X)\n",
    "        # y from the raw population split\n",
    "        y = raw_df[task.target_col].to_numpy()\n",
    "        return X, y\n",
    "\n",
    "    Xtr, ytr = _to_Xy_safe(fe_train, pop[\"train\"])\n",
    "    Xva, yva = _to_Xy_safe(fe_val,   pop[\"val\"])\n",
    "    Xte, yte = _to_Xy_safe(fe_test,  pop[\"test\"])\n",
    "\n",
    "    return {\n",
    "        \"train\": (Xtr, ytr, fe_train),\n",
    "        \"val\":   (Xva, yva, fe_val),\n",
    "        \"test\":  (Xte, yte, fe_test),\n",
    "    }\n"
   ],
   "id": "1e53fa004c6c4217",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Merged-mode diagnostics",
   "id": "83f90a0a78a37210"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:37.801790Z",
     "start_time": "2025-08-21T09:32:37.798953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------- Diagnostics: single vs merged feature spaces --------\n",
    "def compare_single_vs_merged(di_single, di_merged, target_name: str, label=\"train\"):\n",
    "    Xs, ys, df_s = di_single[label]\n",
    "    Xm, ym, df_m = di_merged[label]\n",
    "\n",
    "    print(f\"[Diag:{label}] X_single: {Xs.shape}, X_merged: {Xm.shape}\")\n",
    "    print(f\"[Diag:{label}] y match: {np.allclose(ys, ym)}\")\n",
    "\n",
    "    # columns present only in merged\n",
    "    only_m = [c for c in Xm.columns if c not in Xs.columns]\n",
    "    only_s = [c for c in Xs.columns if c not in Xm.columns]\n",
    "    print(f\"[Diag:{label}] new cols in merged: {len(only_m)} ; dropped vs merged: {len(only_s)}\")\n",
    "\n",
    "    # low-variance check on merged-only columns\n",
    "    if only_m:\n",
    "        lv = Xm[only_m].nunique(dropna=True)\n",
    "        print(f\"[Diag:{label}] merged-only columns nunique (head):\")\n",
    "        print(lv.sort_values().head(10))\n",
    "\n",
    "    try:\n",
    "        from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "        y = ys\n",
    "        # pick MI type by y dtype\n",
    "        if pd.api.types.is_integer_dtype(y) or pd.api.types.is_bool_dtype(y):\n",
    "            mi_fun = mutual_info_classif\n",
    "        else:\n",
    "            mi_fun = mutual_info_regression\n",
    "\n",
    "        # compute MI for up to 200 merged-only cols to keep it fast\n",
    "        subset = only_m[:200]\n",
    "        if subset:\n",
    "            mi = mi_fun(pd.DataFrame(Xm[subset]).fillna(0), y, discrete_features='auto', random_state=0)\n",
    "            mi_s = pd.Series(mi, index=subset).sort_values(ascending=False)\n",
    "            print(f\"[Diag:{label}] top merged-only MI features:\")\n",
    "            print(mi_s.head(10))\n",
    "        else:\n",
    "            print(f\"[Diag:{label}] no merged-only columns to test MI.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Diag:{label}] MI calc skipped: {e}\")\n"
   ],
   "id": "99a94738a0272101",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vectorization Wrapper (Version-Safe)\n",
    "\n",
    "Initializes a `TableVectorizer` with only supported arguments for the installed `skrub` or `dirty_cat` version, ensuring compatibility. Transforms `train`, `val`, and `test` splits into numerical feature matrices, converting them to dense format if necessary.\n"
   ],
   "id": "f7cf59b1861625f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Helper Functions for Vectorization",
   "id": "ff99875cd0db904d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:37.810811Z",
     "start_time": "2025-08-21T09:32:37.808209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This function creates a `TableVectorizer` instance with version-specific arguments.\n",
    "def _make_table_vectorizer():\n",
    "    sig = inspect.signature(TableVectorizer.__init__)\n",
    "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
    "\n",
    "    tv_kwargs = {}\n",
    "\n",
    "    # Only set kwargs that actually exist in the installed version\n",
    "    if \"cardinality_threshold\" in allowed:\n",
    "        tv_kwargs[\"cardinality_threshold\"] = globals().get(\"CARDINALITY_THRESHOLD\", 1000)\n",
    "\n",
    "    # Some versions expose this; others don't, guard it\n",
    "    if \"high_cardinality_transformer\" in allowed:\n",
    "        tv_kwargs[\"high_cardinality_transformer\"] = globals().get(\"HIGH_CARD_TRANSFORMER\", \"hashing\")\n",
    "\n",
    "    # Optional knobs if you define them globally and the version supports them\n",
    "    if \"text_separator\" in allowed and \"TEXT_SEPARATOR\" in globals():\n",
    "        tv_kwargs[\"text_separator\"] = globals()[\"TEXT_SEPARATOR\"]\n",
    "    if \"numerical_transformer\" in allowed and \"NUMERICAL_TRANSFORMER\" in globals():\n",
    "        tv_kwargs[\"numerical_transformer\"] = globals()[\"NUMERICAL_TRANSFORMER\"]\n",
    "    if \"categorical_transformer\" in allowed and \"CATEGORICAL_TRANSFORMER\" in globals():\n",
    "        tv_kwargs[\"categorical_transformer\"] = globals()[\"CATEGORICAL_TRANSFORMER\"]\n",
    "\n",
    "    return TableVectorizer(**tv_kwargs)\n",
    "\n",
    "# Converts a sparse matrix to a dense NumPy array, handling cases where the input is already dense or does not support `.toarray()`.\n",
    "def _to_dense(X):\n",
    "    try:\n",
    "        # scipy sparse matrices have .toarray()\n",
    "        return X.toarray() if hasattr(X, \"toarray\") else X\n",
    "    except Exception:\n",
    "        return X\n",
    "\n",
    "# Vectorizes the training, validation, and test splits using a `TableVectorizer`. It initializes the vectorizer, fits it on the training data, and transforms all splits into dense NumPy arrays. Returns the vectorizer and the transformed matrices.\n",
    "def vectorize_splits(X_train, X_val, X_test):\n",
    "    # Fit only on training data to prevent data leakage\n",
    "    tv = _make_table_vectorizer()\n",
    "    Xt = _to_dense(tv.fit_transform(X_train))\n",
    "    Xv = _to_dense(tv.transform(X_val))\n",
    "    Xs = _to_dense(tv.transform(X_test))\n",
    "    return tv, Xt, Xv, Xs\n"
   ],
   "id": "ffb43a27c3ac51cb",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training and Prediction Helpers",
   "id": "e135f05177e4b188"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:37.821036Z",
     "start_time": "2025-08-21T09:32:37.818400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This function subsamples the training data to a maximum size defined by `TABPFN_MAX`. If the dataset is smaller than this cap, it returns the full dataset; otherwise, it randomly samples without replacement.\n",
    "def _subsample(X, y, cap=TABPFN_MAX, seed=SEED):\n",
    "    if len(X) <= cap:\n",
    "        return X, y, np.arange(len(X))\n",
    "    idx = np.random.RandomState(seed).choice(len(X), size=cap, replace=False)\n",
    "    if hasattr(X, \"iloc\"):\n",
    "        Xs = X.iloc[idx]\n",
    "    else:\n",
    "        Xs = X[idx]\n",
    "    ys = y[idx]\n",
    "    return Xs, ys, idx\n",
    "\n",
    "# Fits a TabPFN model (either classifier or regressor) based on the task type. It initializes the model with the specified device and number of estimators, then fits it to the provided training data.\n",
    "def _fit_tabpfn(task, Xt, yt):\n",
    "    if task.task_type == TaskType.REGRESSION and TabPFNRegressor is not None:\n",
    "        model = TabPFNRegressor(\n",
    "            device=DEVICE,\n",
    "            #n_estimators=int(N_ESTIMATORS),\n",
    "            ignore_pretraining_limits=True,\n",
    "        )\n",
    "    else:\n",
    "        model = TabPFNClassifier(\n",
    "            device=DEVICE,\n",
    "            #n_estimators=int(N_ESTIMATORS),\n",
    "            ignore_pretraining_limits=True,\n",
    "        )\n",
    "    model.fit(Xt, yt)\n",
    "    return model\n",
    "\n",
    "# Helper function to make predictions for a given task using the fitted model. It handles different task types (regression, binary classification, multiclass/multilabel) and returns the appropriate prediction format.\n",
    "def _predict_for_task(task, model, X):\n",
    "    # align with RelBench evaluators: AUROC expects probabilities for the positive class\n",
    "    if task.task_type == TaskType.REGRESSION:\n",
    "        return model.predict(X)\n",
    "    proba = model.predict_proba(X)\n",
    "    if task.task_type == TaskType.BINARY_CLASSIFICATION:\n",
    "        return proba[:, 1]\n",
    "    else:\n",
    "        # multiclass/multilabel: pass full probability matrix\n",
    "        return proba"
   ],
   "id": "5c53f649ce01b948",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Run TabPFN on Selected Tasks\n",
    "\n",
    "Runs TabPFN on a specified dataset and task, handling both single-table and merged-table modes. It vectorizes the data, fits the model, makes predictions, and evaluates performance using RelBench’s evaluators. Returns a dictionary with results."
   ],
   "id": "21448305df117a22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:39.307293Z",
     "start_time": "2025-08-21T09:32:37.828701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = get_dataset(DATASET, download=DOWNLOAD)\n",
    "\n",
    "for task_name in TASKS:\n",
    "    task, splits = fetch_splits(DATASET, task_name, download=DOWNLOAD)\n",
    "\n",
    "    frames_single = build_single_table_frames(task, splits)\n",
    "    frames_merged = build_merged_table_frames(dataset, task, splits)\n",
    "\n",
    "    # Display head of train, val and test of frames_merged\n",
    "    # Show all column names and count for merged train split\n",
    "    merged_train_df = frames_merged[\"train\"][2]\n",
    "    print(\"Merged columns:\", list(merged_train_df.columns))\n",
    "    print(\"Number of columns:\", len(merged_train_df.columns))\n",
    "\n",
    "\n",
    "    #compare_single_vs_merged(frames_single, frames_merged, target_name=task.target_col, label=\"train\")\n"
   ],
   "id": "1a124faf7a2ca60d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Database object from /Users/michaelflppv/Library/Caches/relbench/rel-f1/db...\n",
      "Done in 0.01 seconds.\n",
      "Merged columns: ['driverId', 'position', 'DAY(date)', 'HOUR(date)', 'MONTH(date)', 'WEEKDAY(date)', 'YEAR(date)']\n",
      "Number of columns: 7\n",
      "Merged columns: ['driverId', 'did_not_finish', 'DAY(date)', 'HOUR(date)', 'MONTH(date)', 'WEEKDAY(date)', 'YEAR(date)']\n",
      "Number of columns: 7\n",
      "Merged columns: ['driverId', 'qualifying', 'DAY(date)', 'HOUR(date)', 'MONTH(date)', 'WEEKDAY(date)', 'YEAR(date)']\n",
      "Number of columns: 7\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:39.321482Z",
     "start_time": "2025-08-21T09:32:39.318477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_tabpfn_on_task(dataset_name: str, task_name: str, mode: str = \"single\") -> Dict[str, Any]:\n",
    "    # Load dataset and task splits\n",
    "    dataset = get_dataset(dataset_name, download=DOWNLOAD)\n",
    "    task, splits = fetch_splits(dataset_name, task_name, download=DOWNLOAD)\n",
    "\n",
    "    # Ensure the task is compatible with TabPFN\n",
    "    if mode == \"single\":\n",
    "        frames = build_single_table_frames(task, splits)\n",
    "    elif mode == \"merged\":\n",
    "        try:\n",
    "            frames = build_merged_table_frames(dataset, task, splits)\n",
    "        except Exception as e:\n",
    "            print(f\"merged mode failed: {e}, falling back to single-table mode.\")\n",
    "            raise\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'single' or 'merged'\")\n",
    "\n",
    "    # Extract features and targets for each split\n",
    "    (Xtr, ytr, _dftr) = frames[\"train\"]\n",
    "    (Xva, yva, dfva)  = frames[\"val\"]\n",
    "    (Xte, yte, dfte)  = frames[\"test\"]\n",
    "\n",
    "    # Vectorize\n",
    "    tv, Xt, Xv, Xs = vectorize_splits(Xtr, Xva, Xte)\n",
    "\n",
    "    # Respect TabPFN's sample cap\n",
    "    Xt_cap, yt_cap, _ = _subsample(Xt, ytr, cap=TABPFN_MAX, seed=SEED)\n",
    "\n",
    "    # Fit\n",
    "    model = _fit_tabpfn(task, Xt_cap, yt_cap)\n",
    "\n",
    "    # Predict & Evaluate with RelBench evaluators\n",
    "    val_pred  = _predict_for_task(task, model, Xv)\n",
    "    test_pred = _predict_for_task(task, model, Xs)\n",
    "\n",
    "    # Align predictions with original DataFrame indices for evaluation\n",
    "    val_metrics  = task.evaluate(val_pred,  splits[\"val\"])\n",
    "    test_metrics = task.evaluate(test_pred, splits[\"test\"])\n",
    "\n",
    "    # Convert metrics to a dictionary, ensuring all values are floats\n",
    "    out = {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"task\": task_name,\n",
    "        \"mode\": mode,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"n_train_used\": len(Xt_cap),\n",
    "        \"n_train_total\": len(Xt),\n",
    "        \"n_val\": len(Xv),\n",
    "        \"n_test\": len(Xs),\n",
    "    }\n",
    "    return out"
   ],
   "id": "52860c3ba39daf4c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Orchestrator for Benchmark Runs\n",
    "\n",
    "Iterates over all discovered tasks and runs TabPFN in both **single** and **merged** modes. Collects performance metrics for validation and test splits into a results table, handling failures gracefully. Sorts results for easier comparison.\n"
   ],
   "id": "67ce1925dce06a6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:45.034056Z",
     "start_time": "2025-08-21T09:32:39.331614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODES = globals().get(\"MODES\", [\"single\", \"merged\"])\n",
    "\n",
    "records = []\n",
    "failures = []\n",
    "\n",
    "# Run TabPFN on all tasks in both modes and collect results\n",
    "for task_name in TASKS:\n",
    "    for mode in MODES:\n",
    "        try:\n",
    "            res = run_tabpfn_on_task(DATASET, task_name, mode=mode)\n",
    "            # Flatten metrics for val and test, one metric per row\n",
    "            for split in [\"val\", \"test\"]:\n",
    "                metrics = res.get(f\"{split}_metrics\") or {}\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    # Only add rows with non-empty metric_value\n",
    "                    if metric_value is not None and not (isinstance(metric_value, float) and np.isnan(metric_value)):\n",
    "                        records.append({\n",
    "                            \"dataset\": res.get(\"dataset\", DATASET),\n",
    "                            \"task\": res.get(\"task\", task_name),\n",
    "                            \"split\": split,\n",
    "                            \"mode\": res.get(\"mode\", mode),\n",
    "                            \"method\": \"TabPFN_experimental_v1.0\",\n",
    "                            \"metric\": metric_name,\n",
    "                            \"score\": metric_value,\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            msg = f\"[{DATASET} | {task_name} | {mode}] failed: {e!s}\"\n",
    "            print(msg)\n",
    "            failures.append(msg)\n",
    "\n",
    "# Convert collected records into a DataFrame\n",
    "results_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# If no successful runs were recorded, display a message and create an empty DataFrame\n",
    "if results_df.empty:\n",
    "    print(\"No successful runs were recorded. Check the failure messages above.\")\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=[\"dataset\", \"task\", \"split\", \"mode\", \"method\", \"metric\", \"score\"]\n",
    "    )\n",
    "else:\n",
    "    # Ensure required sort keys exist even if some rows missed them\n",
    "    for col in [\"task\", \"mode\"]:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = pd.NA\n",
    "    # Sort only by the columns that exist to avoid KeyError\n",
    "    sort_keys = [c for c in [\"task\", \"mode\", \"metric\"] if c in results_df.columns]\n",
    "    if sort_keys:\n",
    "        results_df = results_df.sort_values(sort_keys)\n",
    "\n",
    "results_df"
   ],
   "id": "8119d8844ebded86",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m MODES:\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 10\u001B[0m         res \u001B[38;5;241m=\u001B[39m \u001B[43mrun_tabpfn_on_task\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATASET\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;66;03m# Flatten metrics for val and test, one metric per row\u001B[39;00m\n\u001B[1;32m     12\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "Cell \u001B[0;32mIn[15], line 33\u001B[0m, in \u001B[0;36mrun_tabpfn_on_task\u001B[0;34m(dataset_name, task_name, mode)\u001B[0m\n\u001B[1;32m     30\u001B[0m model \u001B[38;5;241m=\u001B[39m _fit_tabpfn(task, Xt_cap, yt_cap)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Predict & Evaluate with RelBench evaluators\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m val_pred  \u001B[38;5;241m=\u001B[39m \u001B[43m_predict_for_task\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mXv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m test_pred \u001B[38;5;241m=\u001B[39m _predict_for_task(task, model, Xs)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Align predictions with original DataFrame indices for evaluation\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[13], line 34\u001B[0m, in \u001B[0;36m_predict_for_task\u001B[0;34m(task, model, X)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_predict_for_task\u001B[39m(task, model, X):\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;66;03m# align with RelBench evaluators: AUROC expects probabilities for the positive class\u001B[39;00m\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m task\u001B[38;5;241m.\u001B[39mtask_type \u001B[38;5;241m==\u001B[39m TaskType\u001B[38;5;241m.\u001B[39mREGRESSION:\n\u001B[0;32m---> 34\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     proba \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict_proba(X)\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m task\u001B[38;5;241m.\u001B[39mtask_type \u001B[38;5;241m==\u001B[39m TaskType\u001B[38;5;241m.\u001B[39mBINARY_CLASSIFICATION:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:79\u001B[0m, in \u001B[0;36mContextDecorator.__call__.<locals>.inner\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_recreate_cm():\n\u001B[0;32m---> 79\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/regressor.py:800\u001B[0m, in \u001B[0;36mTabPFNRegressor.predict\u001B[0;34m(self, X, output_type, quantiles)\u001B[0m\n\u001B[1;32m    793\u001B[0m X \u001B[38;5;241m=\u001B[39m _process_text_na_dataframe(X, ord_encoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocessor_)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;66;03m# Runs over iteration engine\u001B[39;00m\n\u001B[1;32m    796\u001B[0m (\n\u001B[1;32m    797\u001B[0m     _,\n\u001B[1;32m    798\u001B[0m     outputs,  \u001B[38;5;66;03m# list of tensors [N_est, N_samples, N_borders] (after forward)\u001B[39;00m\n\u001B[1;32m    799\u001B[0m     borders,  \u001B[38;5;66;03m# list of numpy arrays containing borders for each estimator\u001B[39;00m\n\u001B[0;32m--> 800\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_inference_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[38;5;66;03m# --- Translate probs, average, get final logits ---\u001B[39;00m\n\u001B[1;32m    803\u001B[0m transformed_logits \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    804\u001B[0m     translate_probs_across_borders(\n\u001B[1;32m    805\u001B[0m         logits,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m logits, borders_t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(outputs, borders)\n\u001B[1;32m    810\u001B[0m ]\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/regressor.py:928\u001B[0m, in \u001B[0;36mTabPFNRegressor.forward\u001B[0;34m(self, X, use_inference_mode)\u001B[0m\n\u001B[1;32m    925\u001B[0m borders: \u001B[38;5;28mlist\u001B[39m[np\u001B[38;5;241m.\u001B[39mndarray] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    927\u001B[0m \u001B[38;5;66;03m# Iterate over estimators\u001B[39;00m\n\u001B[0;32m--> 928\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m output, config \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecutor_\u001B[38;5;241m.\u001B[39miter_outputs(\n\u001B[1;32m    929\u001B[0m     X,\n\u001B[1;32m    930\u001B[0m     device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_,\n\u001B[1;32m    931\u001B[0m     autocast\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_autocast_,\n\u001B[1;32m    932\u001B[0m ):\n\u001B[1;32m    933\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoftmax_temperature \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    934\u001B[0m         output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mfloat() \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoftmax_temperature  \u001B[38;5;66;03m# noqa: PLW2901\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/inference.py:503\u001B[0m, in \u001B[0;36mInferenceEngineCachePreprocessing.iter_outputs\u001B[0;34m(self, X, device, autocast, only_return_standard_out)\u001B[0m\n\u001B[1;32m    497\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m (\n\u001B[1;32m    500\u001B[0m     get_autocast_context(device, enabled\u001B[38;5;241m=\u001B[39mautocast),\n\u001B[1;32m    501\u001B[0m     torch\u001B[38;5;241m.\u001B[39minference_mode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minference_mode),\n\u001B[1;32m    502\u001B[0m ):\n\u001B[0;32m--> 503\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX_full\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    505\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[43m        \u001B[49m\u001B[43monly_return_standard_out\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monly_return_standard_out\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    507\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcategorical_inds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatched_cat_ix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    510\u001B[0m output \u001B[38;5;241m=\u001B[39m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m output\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m output, config\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/architectures/base/transformer.py:549\u001B[0m, in \u001B[0;36mPerFeatureTransformer.forward\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m    541\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    542\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThere should be no NaNs in the encoded x and y.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    543\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCheck that you do not feed NaNs or use a NaN-handling enocder.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    544\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour embedded x and y returned the following:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    545\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtorch\u001B[38;5;241m.\u001B[39misnan(embedded_x)\u001B[38;5;241m.\u001B[39many()\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m | \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtorch\u001B[38;5;241m.\u001B[39misnan(embedded_y)\u001B[38;5;241m.\u001B[39many()\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    546\u001B[0m     )\n\u001B[1;32m    547\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m embedded_y, embedded_x\n\u001B[0;32m--> 549\u001B[0m encoder_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer_encoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    550\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    551\u001B[0m \u001B[43m        \u001B[49m\u001B[43membedded_input\u001B[49m\n\u001B[1;32m    552\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer_decoder\u001B[49m\n\u001B[1;32m    553\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43membedded_input\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43msingle_eval_pos\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    554\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    555\u001B[0m \u001B[43m    \u001B[49m\u001B[43msingle_eval_pos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msingle_eval_pos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    556\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_trainset_representation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcache_trainset_representation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    557\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# b s f+1 e -> b s f+1 e\u001B[39;00m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;66;03m# If we are using a decoder\u001B[39;00m\n\u001B[1;32m    560\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer_decoder:\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/architectures/base/transformer.py:94\u001B[0m, in \u001B[0;36mLayerStack.forward\u001B[0;34m(self, x, **kwargs)\u001B[0m\n\u001B[1;32m     92\u001B[0m         x \u001B[38;5;241m=\u001B[39m checkpoint(partial(layer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs), x, use_reentrant\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 94\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/architectures/base/layer.py:421\u001B[0m, in \u001B[0;36mPerFeatureEncoderLayer.forward\u001B[0;34m(self, state, single_eval_pos, cache_trainset_representation, att_src)\u001B[0m\n\u001B[1;32m    411\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    412\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPre-norm implementation is wrong, as the residual should never\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    413\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be layer normed here.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    414\u001B[0m     )\n\u001B[1;32m    415\u001B[0m     state \u001B[38;5;241m=\u001B[39m layer_norm(\n\u001B[1;32m    416\u001B[0m         state,\n\u001B[1;32m    417\u001B[0m         allow_inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    418\u001B[0m         save_peak_mem_factor\u001B[38;5;241m=\u001B[39msave_peak_mem_factor,\n\u001B[1;32m    419\u001B[0m     )\n\u001B[0;32m--> 421\u001B[0m state \u001B[38;5;241m=\u001B[39m \u001B[43msublayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpre_norm:\n\u001B[1;32m    423\u001B[0m     state \u001B[38;5;241m=\u001B[39m layer_norm(\n\u001B[1;32m    424\u001B[0m         state,\n\u001B[1;32m    425\u001B[0m         allow_inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    426\u001B[0m         save_peak_mem_factor\u001B[38;5;241m=\u001B[39msave_peak_mem_factor,\n\u001B[1;32m    427\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/architectures/base/layer.py:335\u001B[0m, in \u001B[0;36mPerFeatureEncoderLayer.forward.<locals>.attn_between_items\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    332\u001B[0m     new_x_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m single_eval_pos:\n\u001B[0;32m--> 335\u001B[0m     new_x_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn_between_items\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    336\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43msingle_eval_pos\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    337\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43msingle_eval_pos\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    338\u001B[0m \u001B[43m        \u001B[49m\u001B[43msave_peak_mem_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_peak_mem_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    339\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_kv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_trainset_representation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m        \u001B[49m\u001B[43monly_cache_first_head_kv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_inplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cached_kv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    345\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    346\u001B[0m     new_x_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/architectures/base/attention/full_attention.py:333\u001B[0m, in \u001B[0;36mMultiHeadAttention.forward\u001B[0;34m(self, x, x_kv, cache_kv, add_input, allow_inplace, save_peak_mem_factor, reuse_first_head_kv, only_cache_first_head_kv, use_cached_kv)\u001B[0m\n\u001B[1;32m    316\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_k_cache \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty(\n\u001B[1;32m    317\u001B[0m             batch_size,\n\u001B[1;32m    318\u001B[0m             seqlen_kv,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    322\u001B[0m             dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype,\n\u001B[1;32m    323\u001B[0m         )\n\u001B[1;32m    324\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_v_cache \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty(\n\u001B[1;32m    325\u001B[0m             batch_size,\n\u001B[1;32m    326\u001B[0m             seqlen_kv,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    330\u001B[0m             dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype,\n\u001B[1;32m    331\u001B[0m         )\n\u001B[0;32m--> 333\u001B[0m output: torch\u001B[38;5;241m.\u001B[39mTensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    334\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_kv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    336\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_k_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    337\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_v_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    338\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_kv_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    339\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_kv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_kv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cached_kv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cached_kv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_inplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_inplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_peak_mem_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_peak_mem_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreuse_first_head_kv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreuse_first_head_kv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\u001B[38;5;241m.\u001B[39mreshape(x_shape_after_transpose[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m output\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:])\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/architectures/base/memory.py:95\u001B[0m, in \u001B[0;36msupport_save_peak_mem_factor.<locals>.method_\u001B[0;34m(self, x, add_input, allow_inplace, save_peak_mem_factor, *args, **kwargs)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x_, \u001B[38;5;241m*\u001B[39margs_ \u001B[38;5;129;01min\u001B[39;00m split_args:\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m add_input:\n\u001B[0;32m---> 95\u001B[0m         x_[:] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     97\u001B[0m         x_[:] \u001B[38;5;241m=\u001B[39m method(\u001B[38;5;28mself\u001B[39m, x_, \u001B[38;5;241m*\u001B[39margs_, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/architectures/base/attention/full_attention.py:472\u001B[0m, in \u001B[0;36mMultiHeadAttention._compute\u001B[0;34m(self, x, x_kv, k_cache, v_cache, kv_cache, cache_kv, use_cached_kv, reuse_first_head_kv)\u001B[0m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Attention computation.\u001B[39;00m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;124;03mCalled by 'forward', potentially on shards, once shapes have been normalized.\u001B[39;00m\n\u001B[1;32m    461\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    462\u001B[0m q, k, v, kv, qkv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_qkv(\n\u001B[1;32m    463\u001B[0m     x,\n\u001B[1;32m    464\u001B[0m     x_kv,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    470\u001B[0m     reuse_first_head_kv\u001B[38;5;241m=\u001B[39mreuse_first_head_kv,\n\u001B[1;32m    471\u001B[0m )\n\u001B[0;32m--> 472\u001B[0m attention_head_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mMultiHeadAttention\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_attention_heads\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[43m    \u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    474\u001B[0m \u001B[43m    \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    475\u001B[0m \u001B[43m    \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    476\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    477\u001B[0m \u001B[43m    \u001B[49m\u001B[43mqkv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    478\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    479\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    480\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    481\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39meinsum(\n\u001B[1;32m    482\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m... h d, h d s -> ... s\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    483\u001B[0m     attention_head_outputs,\n\u001B[1;32m    484\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_w_out,\n\u001B[1;32m    485\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/TabPFN/.venv1/lib/python3.10/site-packages/tabpfn/architectures/base/attention/full_attention.py:695\u001B[0m, in \u001B[0;36mMultiHeadAttention.compute_attention_heads\u001B[0;34m(q, k, v, kv, qkv, dropout_p, softmax_scale)\u001B[0m\n\u001B[1;32m    689\u001B[0m logits \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb q h d, b k h d -> b q k h\u001B[39m\u001B[38;5;124m\"\u001B[39m, q, k)\n\u001B[1;32m    690\u001B[0m logits \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    691\u001B[0m     torch\u001B[38;5;241m.\u001B[39msqrt(torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m/\u001B[39m d_k))\u001B[38;5;241m.\u001B[39mto(k\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    692\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m softmax_scale \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    693\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m softmax_scale\n\u001B[1;32m    694\u001B[0m )\n\u001B[0;32m--> 695\u001B[0m ps \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    696\u001B[0m ps \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdropout(ps, dropout_p, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    697\u001B[0m attention_head_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb q k h, b k h d -> b q h d\u001B[39m\u001B[38;5;124m\"\u001B[39m, ps, v)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save Results to CSV",
   "id": "77a4dc93cb1858d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:32:45.036138Z",
     "start_time": "2025-08-20T22:21:29.997577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify output directory\n",
    "out_dir = globals().get(\"OUT_DIR\", \"outputs\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Change timestamp format to \"dd.mm.yyyy-hh:mm\"\n",
    "timestamp = time.strftime(\"%d.%m.%Y-%H:%M\")\n",
    "csv_name = f\"tabpfn_{DATASET}_{timestamp}.csv\"\n",
    "csv_path = os.path.join(out_dir, csv_name)\n",
    "\n",
    "# Round all numerical results to 4 decimal places before saving\n",
    "if \"score\" in results_df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(results_df[\"score\"]):\n",
    "        results_df[\"score\"] = results_df[\"score\"].round(4)\n",
    "    else:\n",
    "        # Optionally, try to convert to numeric first\n",
    "        results_df[\"score\"] = pd.to_numeric(results_df[\"score\"], errors=\"coerce\").round(4)\n",
    "\n",
    "# Filter out rows with empty score values before saving\n",
    "if \"score\" in results_df.columns:\n",
    "    results_df = results_df[results_df[\"score\"].notnull()]\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved results to: {csv_path}\")\n"
   ],
   "id": "c13a29742326995c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: outputs/tabpfn_rel-f1_21.08.2025-00:21.csv\n"
     ]
    }
   ],
   "execution_count": 117
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
